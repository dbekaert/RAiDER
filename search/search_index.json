{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAiDER","text":"<p>Raytracing Atmospheric Delay Estimation for RADAR</p> <p> </p> <p>RAiDER-tools is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged. </p> <p>Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved.</p> <p>THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK.</p>"},{"location":"#contents","title":"Contents","text":"<ol> <li>Getting Started<ul> <li>Installing With Conda</li> <li>Using the Docker Image</li> <li>Installing from Source</li> </ul> </li> <li>Setup of third party weather model access</li> <li>Running RAiDER and Documentation</li> <li>Citing</li> <li>Development<ul> <li>Contributors </li> </ul> </li> </ol>"},{"location":"#1-getting-started","title":"1. Getting Started","text":"<p>RAiDER has been tested on the following systems: - Ubuntu v.16 and up - Mac OS v.10 and up</p> <p>RAiDER does not currently run on arm64 processors on Mac. We will update this note once the build becomes available. </p>"},{"location":"#installing-with-conda","title":"Installing With Conda","text":"<p>RAiDER is available on conda-forge. Conda is a cross-platform way to use Python that allows you to setup and use \"virtual environments.\" These can help to keep dependencies for different sets of code separate. We recommend using Miniforge, a conda environment manager that uses conda-forge as its default code repo. Alternatively,see here for help installing Anaconda and here for installing Miniconda.</p> <p>Installing RAiDER:</p> <pre><code>conda env create --name RAiDER  -c conda-forge raider\nconda activate RAiDER\n</code></pre>"},{"location":"#using-the-docker-image","title":"Using the Docker image","text":"<p>RAiDER provides a docker container image with all the necessary dependencies pre-installed. To get the latest released version: </p> <pre><code>docker pull ghcr.io/dbekaert/raider:latest\n</code></pre> <p>a specific release version (&gt;=v0.2.0 only):</p> <pre><code>docker pull ghcr.io/dbekaert/raider:0.2.0\n</code></pre> <p>or the current development version:</p> <pre><code>docker pull ghcr.io/dbekaert/raider:test\n</code></pre> <p>To run <code>raider.py</code> inside the container:</p> <pre><code>docker run -it --rm ghcr.io/dbekaert/raider:latest\n</code></pre> <p>To mount your current directory inside the container so that files will be written back to your local machine:</p> <pre><code>docker run -it -v ${PWD}:/home/raider/work --rm ghcr.io/dbekaert/raider:latest\ncd work\n</code></pre> <p>To jump into a <code>bash</code> shell inside the container:</p> <pre><code>docker run -it --rm --entrypoint /bin/bash ghcr.io/dbekaert/raider:latest -l\n</code></pre> <p>For more docker run options, see: https://docs.docker.com/engine/reference/run/.</p>"},{"location":"#2-setup-of-third-party-weather-model-access","title":"2. Setup of third party weather model access","text":"<p>RAiDER has the ability to download weather models from third-parties; some of which require license agreements. See here for details.</p>"},{"location":"#3-running-raider-and-documentation","title":"3. Running RAiDER and Documentation","text":"<p>For detailed documentation, examples, and Jupyter notebooks see the RAiDER-docs repository. We welcome contributions of other examples on how to leverage the RAiDER  (see here for instructions). <code>raiderDelay.py -h</code> provides a help menu and list of example commands to get started. The RAiDER scripts are highly modularized in Python and allows for building your own processing workflow.</p>"},{"location":"#4-citation","title":"4. Citation","text":"<p>TODO</p>"},{"location":"#5-development","title":"5. Development","text":"<p>Contributions are welcome and heartily encourage! See our contributing guide.</p>"},{"location":"#development-install","title":"Development install","text":"<p>For development, we recommend installing directly from source.</p> <pre><code>git clone https://github.com/dbekaert/RAiDER.git\ncd RAiDER\nconda env create -f environment.yml\nconda activate RAiDER\npython -m pip install -e .\n</code></pre> <p>For more details on installing from source see here.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>David Bekaert</li> <li>Jeremy Maurer</li> <li>Raymond Hogenson</li> <li>Brett Buzzanga</li> <li>Piyush Agram (Descartes Labs)</li> <li>Yang Lei</li> <li>Rohan Weeden</li> <li>Simran Sangha</li> <li>other community members</li> </ul> <p>We welcome community contributions! For instructions see here.</p>"},{"location":"Installing_from_source/","title":"Installing from source","text":""},{"location":"Installing_from_source/#common-installation-issues","title":"Common Installation Issues","text":"<ol> <li>This package uses GDAL and g++, both of which can be tricky to set up correctly. GDAL in particular will often break after installing a new program If you receive error messages such as the following:</li> </ol> <pre><code>ImportError: ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/../../../libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ~/anaconda3/envs/RAiDER/lib/python3.7/site-packages/matplotlib/ft2font.cpython-37m-x86_64-linux-gnu.so)\nImportError: libtiledb.so.1.6.0: cannot open shared object file: No such file or directory\n***cmake: ~/anaconda3/envs/RAiDER/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by cmake)***\n</code></pre> <p>try running the following commands within your RAiDER conda environment:</p> <pre><code>conda update --force-reinstall libstdcxx-ng\nconda update --force-reinstall gdal libgdal\n</code></pre> <ol> <li>This package requires both C++ and C headers, and the system headers are used for some C libraries. If running on a Mac computer, and \"python setup.py build\" results in a message stating that some system library header file is missing, try the following steps, and accept the various licenses and step through the installation process. Try re-running the build step after each update:</li> </ol> <p><code>xcode-select --install  open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg</code></p>"},{"location":"Installing_from_source/#testing-your-installation","title":"Testing your installation","text":"<p>To test the installation was successfull you can run the following tests:</p> <pre><code>py.test test/\nraiderDelay.py -h\n</code></pre>"},{"location":"Installing_from_source/#to-enable-automatic-circleci-tests-from-a-pull-requests","title":"To enable automatic CircleCI Tests from a pull requests","text":"<p>You will need to make sure that CircleCI is an authorized OAuth application from Github. Simply sign in here using your github account.</p>"},{"location":"WeatherModels/","title":"Accessing weather model data","text":"<p>RAiDER has built-in support for a number of different weather models.  RAiDER provides all the interfacing to data servers required to access data for the different weather models, although some weather models require a license agreement and accounts to be set-up. Instructions for accessing data, including license-limited data, are provided below. It is the user's responsibility to accept license agreements for whatever model is desired. </p> <p>In addition, RAiDER provides functionality for adding additional weather models. See the RAiDER-docs repository page on how to do this. We would love to expand the suite of supported models, and welcome any contributions. Please see the contributing guidelines or reach out through an issue ticket for help. </p>"},{"location":"WeatherModels/#1-usage","title":"1. Usage","text":""},{"location":"WeatherModels/#RAiDER.processWM.prepareWeatherModel","title":"<code>RAiDER.processWM.prepareWeatherModel(weather_model, time, ll_bounds, download_only=False, makePlots=False, force_download=False)</code>","text":"<p>Parse inputs to download and prepare a weather model grid for interpolation</p> <p>Parameters:</p> Name Type Description Default <code>weather_model</code> <p>WeatherModel   - instantiated weather model object</p> required <code>time</code> <p>datetime                - Python datetime to request. Will be rounded to nearest available time</p> required <code>ll_bounds</code> <p>list/array        - SNWE bounds target area to ensure weather model contains them</p> required <code>download_only</code> <code>bool</code> <p>bool           - False if preprocessing weather model data</p> <code>False</code> <code>makePlots</code> <code>bool</code> <p>bool               - whether to write debug plots</p> <code>False</code> <code>force_download</code> <code>bool</code> <p>bool          - True if you want to download even when the weather model exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>filename of the netcdf file to which the weather model has been written</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/processWM.py</code> <pre><code>def prepareWeatherModel(\n        weather_model,\n        time,\n        ll_bounds,\n        download_only: bool=False,\n        makePlots: bool=False,\n        force_download: bool=False,\n    ) -&gt; str:\n\"\"\"Parse inputs to download and prepare a weather model grid for interpolation\n\n    Args:\n        weather_model: WeatherModel   - instantiated weather model object\n        time: datetime                - Python datetime to request. Will be rounded to nearest available time\n        ll_bounds: list/array        - SNWE bounds target area to ensure weather model contains them\n        download_only: bool           - False if preprocessing weather model data\n        makePlots: bool               - whether to write debug plots\n        force_download: bool          - True if you want to download even when the weather model exists\n\n    Returns:\n        str: filename of the netcdf file to which the weather model has been written\n    \"\"\"\n    ## set the bounding box from the in the case that it hasn't been set\n    if weather_model.get_latlon_bounds() is None:\n        weather_model.set_latlon_bounds(ll_bounds)\n\n    # Ensure the file output location exists\n    wmLoc     = weather_model.get_wmLoc()\n    weather_model.setTime(time)\n\n    # get the path to the less processed weather model file\n    path_wm_raw = make_raw_weather_data_filename(wmLoc, weather_model.Model(), time)\n\n    # get the path to the more processed (cropped) weather model file\n    path_wm_crop = weather_model.out_file(wmLoc)\n\n    # check whether weather model files exists and/or or should be downloaded\n    if os.path.exists(path_wm_crop) and not force_download:\n        logger.warning(\n            'Processed weather model already exists, please remove it (\"%s\") if you want '\n            'to download a new one.', path_wm_crop)\n\n    # check whether the raw weather model covers this area\n    elif os.path.exists(path_wm_raw) and \\\n        checkContainment_raw(path_wm_raw, ll_bounds) and not force_download:\n        logger.warning(\n            'Raw weather model already exists, please remove it (\"%s\") if you want '\n            'to download a new one.', path_wm_raw)\n\n    # if no weather model files supplied, check the standard location\n    else:\n        E = weather_model.fetch(path_wm_raw, time)\n        if E:\n            logger.warning (E)\n            raise RuntimeError\n\n    # If only downloading, exit now\n    if download_only:\n        logger.warning(\n            'download_only flag selected. No further processing will happen.'\n        )\n        return None\n\n    # Otherwise, load the weather model data\n    f = weather_model.load()\n\n    if f is not None:\n        logger.warning(\n            'The processed weather model file already exists,'\n            ' so I will use that.'\n        )\n\n        containment = weather_model.checkContainment(ll_bounds)\n        if not containment and weather_model.Model() in 'GMAO ERA5 ERA5T HRES'.split():\n            msg = 'The weather model passed does not cover all of the input ' \\\n                'points; you may need to download a larger area.'\n            logger.error(msg)\n            raise RuntimeError(msg)\n        return f\n\n    # Logging some basic info\n    logger.debug(\n        'Number of weather model nodes: %s',\n            np.prod(weather_model.getWetRefractivity().shape)\n            )\n    shape = weather_model.getWetRefractivity().shape\n    logger.debug(f'Shape of weather model: {shape}')\n    logger.debug(\n        'Bounds of the weather model: %.2f/%.2f/%.2f/%.2f (SNWE)',\n        np.nanmin(weather_model._ys), np.nanmax(weather_model._ys),\n        np.nanmin(weather_model._xs), np.nanmax(weather_model._xs)\n    )\n    logger.debug('Weather model: %s', weather_model.Model())\n    logger.debug(\n        'Mean value of the wet refractivity: %f',\n        np.nanmean(weather_model.getWetRefractivity())\n    )\n    logger.debug(\n        'Mean value of the hydrostatic refractivity: %f',\n        np.nanmean(weather_model.getHydroRefractivity())\n    )\n    logger.debug(weather_model)\n\n    if makePlots:\n        weather_model.plot('wh', True)\n        weather_model.plot('pqt', True)\n        plt.close('all')\n\n    try:\n        f = weather_model.write()\n        containment = weather_model.checkContainment(ll_bounds)\n\n    except Exception as e:\n        logger.exception(\"Unable to save weathermodel to file\")\n        logger.exception(e)\n        raise RuntimeError(\"Unable to save weathermodel to file\")\n\n    finally:\n        wm = weather_model.Model()\n        del weather_model\n\n    if not containment and wm in 'GMAO ERA5 ERA5T HRES'.split():\n        msg = 'The weather model passed does not cover all of the input ' \\\n            'points; you may need to download a larger area.'\n        logger.error(msg)\n        raise RuntimeError(msg)\n    else:\n        return f\n</code></pre>"},{"location":"WeatherModels/#potential-download-failure","title":"Potential download failure","text":"<p>ERA-5/ERA-I products require access to the ESA Copernicus servers. GMAO and MERRA-2 products require access to the NASA Earthdata servers. If you are unable to download products, ensure that you have registered and have downloaded the public API key, and accepted/added the license/application for type of product you wish to download as detailed below. </p>"},{"location":"WeatherModels/#2-noaa-weather-models-hrrr","title":"2. NOAA weather models (HRRR)","text":"<p>High-resolution rapid refresh (HRRR) weather model data products are generated by NOAA for the coninental US (CONUS) but not archived beyond three days. However, a public archive is available at the University of Utah. This archive does not require a license agreement. This model has the highest spatial resolution available in RAiDER, with a horizontal grid spacing of about 3 km, and is provided in a Lambert conformal conic projection. </p>"},{"location":"WeatherModels/#3-ecmwf-weather-models-era5-era5t-erai-hres","title":"3. ECMWF weather models (ERA5, ERA5T, ERAI, HRES)","text":"<p>The Copernicus Climate Data Store (CDS) provides access to the European Centre for Medium-Range Weather Forecasts (ECMWF)  provides a number of different weather models, including ERA5 and ERA5T reanalysis models.  </p> <p>The ECMWF provides access to both reanalysis and real-time prediction models. You can read more information about their reanalysis models here and real-time model here. ECMWF models are global, with horizontal resolution of about 30 km for ERA-I, ERA-5, and ERA-5T, and 6 km for Hi-RES. All of these models come in a global projection (EPSG 4326, WGS-84). </p>"},{"location":"WeatherModels/#accessing-era5-and-era5t-weather-reanalysis-products-from-copernicus-cds","title":"Accessing ERA5 and ERA5T weather reanalysis products from Copernicus CDS","text":"<ol> <li>Create an account on the Copernicus servers here</li> <li>Confirm your email, etc. </li> <li> <p>Install the public API key and client as instructed here:</p> <p>a. Copy the URL and API key from the webpage into a file in your home directory name <code>~/.cdsapirc</code></p> <pre><code>url: https://cds.climate.copernicus.eu/api/v2\nkey: your_key_here\n</code></pre> <p>Note: the key represents the API key obtained upon the registration of CDS API, and should be replaced with the user's own information.</p> <p>b. Install the CDS API using pip</p> <pre><code>pip install cdsapi\n</code></pre> <p>Note: this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER</p> </li> <li> <p>You must accept the license for each product you wish to download.</p> </li> </ol>"},{"location":"WeatherModels/#accessing-erai-hres","title":"Accessing ERAI, HRES","text":"<p>ECMWF requires a license agreement to be able to access, download, and use their products. Instructions for completing this process is below. </p> <ol> <li>Create an account on the ECMWF servers here. The ERA-I model is open-access, while HRES requires a special licence agreement.</li> <li>Confirm your email, etc. </li> <li> <p>Install the public API key and client as instructed here: </p> <p>a. Copy the URL and API key from the webpage into a file in your home directory name <code>~/.ecmwfapirc</code></p> <pre><code>{\n    \"url\"   : \"https://api.ecmwf.int/v1\",\n    \"key\"   : your key here,\n    \"email\" : your email here\n}\n</code></pre> <p>Note: the email that is used to register the user account, and the key represents the API key obtained upon the registration of ECMWF API, and should be replaced with the user's own information.</p> <p>b. Install the ECMWF API using pip:</p> <pre><code>pip install ecmwf-api-client`\n</code></pre> <p>Note: this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER</p> </li> </ol>"},{"location":"WeatherModels/#4-nasa-weather-models-gmao-merra2","title":"4. NASA weather models (GMAO, MERRA2)","text":"<ol> <li> <p>The Global Modeling and Assimilation Office (GMAO) at NASA generates reanalysis weather models. GMAO products can also be accessed without a license agreement through the pyDAP interface implemented in RAiDER. GMAO has a horizontal grid spacing of approximately 33 km, and its projection is EPSG code 4326 (WGS-84). </p> </li> <li> <p>The Modern-Era Retrospective analysis for Research and Applications, Version 2 (MERRA-2) provides data beginning in 1980. MERRA-2 is also produced by NASA and has a spatial resolution of about 50 km and a global projection (EPSG 4326, WGS-84).  </p> </li> </ol> <p>Reference: The Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2), Ronald Gelaro, et al., 2017, J. Clim., doi: 10.1175/JCLI-D-16-0758.1</p>"},{"location":"WeatherModels/#accessing-nasa-weather-model-data","title":"Accessing NASA weather model data","text":"<ol> <li>Create an account on the NASA's Earthdata website here</li> <li>Confirm your email, etc. </li> <li> <p>Copy the login username and password to a file in your home directory name ~/.netrc </p> <pre><code> machine urs.earthdata.nasa.gov\n         login &lt;USERNAME&gt;\n         password &lt;PASSWORD&gt;\n</code></pre> <p>Note: the username and password represent the user's username and password.</p> </li> <li> <p>Add the application <code>NASA GESDISC DATA ARCHIVE</code> by clicking on the <code>Applications-&gt;Authorized Apps</code> on the menu after logging into your Earthdata profile, and then scrolling down to the application <code>NASA GESDISC DATA ARCHIVE</code> to approve it. This seems not required for GMAO for now, but recommended to do so for all OpenDAP-based weather models.</p> </li> <li> <p>Install the OpenDAP using pip: </p> <pre><code> pip install pydap==3.2.1\n</code></pre> <p>Note: this step has been included in the conda install of RAiDER, thus can be omitted if one uses the recommended conda install of RAiDER</p> <p>Note: PyDAP v3.2.1 is required for now (thus specified in the above pip install command) because the latest v3.2.2 (as of now) has a known bug in accessing and slicing the GMAO data. This bug is expected to be fixed in newer versions of PyDAP.</p> </li> </ol>"},{"location":"reference/","title":"RAiDER v0.4.4 API Reference","text":"<p>Raytracing Atmospheric Delay Estimation for RADAR</p> <p>Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved.</p>"},{"location":"reference/#RAiDER.aria","title":"<code>aria</code>","text":""},{"location":"reference/#RAiDER.aria.calcGUNW","title":"<code>calcGUNW</code>","text":"<p>Calculate the interferometric phase from the 4 delays files of a GUNW Write it to disk</p>"},{"location":"reference/#RAiDER.aria.calcGUNW.compute_delays_slc","title":"<code>compute_delays_slc(cube_filenames, wavelength)</code>","text":"<p>Get delays and convert to radians. Return xr dataset.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/calcGUNW.py</code> <pre><code>def compute_delays_slc(cube_filenames:list, wavelength):\n\"\"\" Get delays and convert to radians. Return xr dataset. \"\"\"\n    # parse date from filename\n    dct_delays = {}\n    for f in cube_filenames:\n        date = datetime.strptime(os.path.basename(f).split('_')[2], '%Y%m%dT%H%M%S')\n        dct_delays[date] = f\n\n    sec, ref = sorted(dct_delays.keys())\n\n    wet_delays  = []\n    hyd_delays  = []\n    phase2range = (-4 * np.pi) / float(wavelength)\n    for dt in [ref, sec]:\n        path = dct_delays[dt]\n        with xr.open_dataset(path) as ds:\n            da_wet   = ds['wet'] * phase2range\n            da_hydro = ds['hydro'] * phase2range\n\n            wet_delays.append(da_wet)\n            hyd_delays.append(da_hydro)\n\n            crs = da_wet.rio.crs\n            gt  = da_wet.rio.transform()\n\n    chunk_sizes = da_wet.shape[0], da_wet.shape[1]/3, da_wet.shape[2]/3\n\n    # open one to copy and store new data\n    ds_slc   = xr.open_dataset(path).copy()\n    encoding = ds_slc['wet'].encoding # chunksizes and fill value\n    encoding['contiguous'] = False\n    encoding['_FillValue'] = 0.\n    encoding['chunksizes'] = tuple([np.floor(cs) for cs in chunk_sizes])\n    del ds_slc['wet'], ds_slc['hydro']\n\n    for i, key in enumerate('reference secondary'.split()):\n        ds_slc[f'{key}_{TROPO_NAMES[0]}'] = wet_delays[i]\n        ds_slc[f'{key}_{TROPO_NAMES[1]}'] = hyd_delays[i]\n\n    model = os.path.basename(path).split('_')[0]\n\n    attrs = {\n             'units': 'radians',\n             'grid_mapping': 'crs',\n             }\n\n    ## no data (fill value?) chunk size?\n    for name in TROPO_NAMES:\n        for key in 'reference secondary'.split():\n            descrip  = f\"Delay due to {name.lstrip('troposphere')} component of troposphere\"\n            da_attrs = {**attrs,  'description':descrip,\n                        'long_name':name, 'standard_name':name,\n                        'RAiDER version': RAiDER.__version__,\n                        }\n            ds_slc[f'{key}_{name}'] = ds_slc[f'{key}_{name}'].assign_attrs(da_attrs)\n            ds_slc[f'{key}_{name}'].encoding = encoding\n\n    ds_slc = ds_slc.assign_attrs(model=model, method='ray tracing')\n\n    ## force these to float32 to prevent stitching errors\n    coords = {coord:ds_slc[coord].astype(np.float32) for coord in ds_slc.coords}\n    ds_slc = ds_slc.assign_coords(coords)\n\n    return ds_slc.rename(z=DIM_NAMES[0], y=DIM_NAMES[1], x=DIM_NAMES[2])\n</code></pre>"},{"location":"reference/#RAiDER.aria.calcGUNW.tropo_gunw_slc","title":"<code>tropo_gunw_slc(cube_filenames, path_gunw, wavelength, out_dir, update_gunw)</code>","text":"<p>Calculate ref/sec phase delay</p> Requires <p>list with filename of delay cube for ref and sec date (netcdf) path to the gunw file wavelength (units: m) output directory (where to store the delays)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/calcGUNW.py</code> <pre><code>def tropo_gunw_slc(cube_filenames:list, path_gunw:str, wavelength, out_dir:str, update_gunw:bool):\n\"\"\" Calculate ref/sec phase delay\n\n    Requires:\n        list with filename of delay cube for ref and sec date (netcdf)\n        path to the gunw file\n        wavelength (units: m)\n        output directory (where to store the delays)\n    \"\"\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    ds_slc = compute_delays_slc(cube_filenames, wavelength)\n    da     = ds_slc[f'reference_{TROPO_NAMES[0]}'] # for metadata\n    model  = ds_slc.model\n\n    # write the interferometric delay to disk\n    ref, sec = os.path.basename(path_gunw).split('-')[6].split('_')\n    mid_time = os.path.basename(path_gunw).split('-')[7]\n    dst      = os.path.join(out_dir, f'{model}_interferometric_{ref}-{sec}_{mid_time}.nc')\n    ds_slc.to_netcdf(dst)\n    logger.info ('Wrote slc delays to: %s', dst)\n\n    ## optionally update netcdf with the slc delay\n    update_gunw_slc(path_gunw, ds_slc)\n\n    ## temp\n    update_gunw_version(path_gunw)\n</code></pre>"},{"location":"reference/#RAiDER.aria.calcGUNW.update_gunw_slc","title":"<code>update_gunw_slc(path_gunw, ds_slc)</code>","text":"<p>Update the path_gunw file using the slc delays in ds_slc</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/calcGUNW.py</code> <pre><code>def update_gunw_slc(path_gunw:str, ds_slc):\n\"\"\" Update the path_gunw file using the slc delays in ds_slc \"\"\"\n    ## first need to delete the variable; only can seem to with h5\n    with h5py.File(path_gunw, 'a') as h5:\n        for k in TROPO_GROUP.split():\n            h5 = h5[k]\n        # in case GUNW has already been updated once before\n        try:\n            del h5[TROPO_NAMES[0]]\n            del h5[TROPO_NAMES[1]]\n        except KeyError:\n            pass\n\n        for k in 'crs'.split():\n            if k in h5.keys():\n                del h5[k]\n\n\n    with Dataset(path_gunw, mode='a') as ds:\n        ds_grp    = ds[TROPO_GROUP]\n        ds_grp.createGroup(ds_slc.attrs['model'].upper())\n        ds_grp_wm = ds_grp[ds_slc.attrs['model'].upper()]\n\n\n        ## create and store new data e.g., corrections/troposphere/GMAO/reference/troposphereWet\n        for rs in 'reference secondary'.split():\n            ds_grp_wm.createGroup(rs)\n            ds_grp_rs = ds_grp_wm[rs]\n\n            ## create the new dimensions e.g., corrections/troposphere/GMAO/reference/latitudeMeta\n            for dim in DIM_NAMES:\n                ## dimension may already exist if updating\n                try:\n                    ds_grp_rs.createDimension(dim, len(ds_slc.coords[dim]))\n                    ## necessary for transform\n                    v  = ds_grp_rs.createVariable(dim, np.float32, dim)\n                    v[:] = ds_slc[dim]\n                    v.setncatts(ds_slc[dim].attrs)\n\n                except RuntimeError:\n                    pass\n\n            ## add the projection if it doesnt exist\n            try:\n                v_proj = ds_grp_rs.createVariable('crs', 'i')\n            except RuntimeError:\n                v_proj = ds_grp_rs['crs']\n            v_proj.setncatts(ds_slc[\"crs\"].attrs)\n\n            ## update the actual tropo data\n            for name in TROPO_NAMES:\n                da        = ds_slc[f'{rs}_{name}']\n                nodata    = da.encoding['_FillValue']\n                chunksize = da.encoding['chunksizes']\n\n                ## in case updating\n                try:\n                    v    = ds_grp_rs.createVariable(name, np.float32, DIM_NAMES,\n                                        chunksizes=chunksize, fill_value=nodata)\n                except RuntimeError:\n                    v    = ds_grp_rs[name]\n\n                v[:] = da.data\n                v.setncatts(da.attrs)\n\n\n    logger.info('Updated %s group in: %s', os.path.basename(TROPO_GROUP), path_gunw)\n    return\n</code></pre>"},{"location":"reference/#RAiDER.aria.calcGUNW.update_gunw_version","title":"<code>update_gunw_version(path_gunw)</code>","text":"<p>temporary hack for updating version to test aria-tools</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/calcGUNW.py</code> <pre><code>def update_gunw_version(path_gunw):\n\"\"\" temporary hack for updating version to test aria-tools \"\"\"\n    with Dataset(path_gunw, mode='a') as ds:\n        ds.version = '1c'\n    return\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW","title":"<code>prepFromGUNW</code>","text":""},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW","title":"<code>GUNW</code>  <code>dataclass</code>","text":"Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>@dataclass\nclass GUNW:\n    path_gunw: str\n    wm: str\n    out_dir: str\n\n    def __post_init__(self):\n        self.SNWE      = self.get_bbox()\n        self.heights   = np.arange(-500, 9500, 500).tolist()\n        # self.heights   = [-500, 0]\n        self.dates, self.mid_time = self.get_datetimes()\n\n        self.look_dir   = self.get_look_dir()\n        self.wavelength = self.get_wavelength()\n        self.name       = self.make_fname()\n        self.OrbitFile  = self.get_orbit_file()\n        self.spacing_m  = int(DCT_POSTING[self.wm] * 1e5)\n\n        ## not implemented\n        # self.spacing_m = self.calc_spacing_UTM() # probably wrong/unnecessary\n        # self.lat_file, self.lon_file = self.makeLatLonGrid_native()\n        # self.path_cube  = self.make_cube() # not needed\n\n\n    def get_bbox(self):\n\"\"\" Get the bounding box (SNWE) from an ARIA GUNW product \"\"\"\n        with xr.open_dataset(self.path_gunw) as ds:\n            poly_str = ds['productBoundingBox'].data[0].decode('utf-8')\n\n        poly     = shapely.wkt.loads(poly_str)\n        W, S, E, N = poly.bounds\n\n        return [S, N, W, E]\n\n\n    def make_fname(self):\n\"\"\" Match the ref/sec filename (SLC dates may be different around edge cases) \"\"\"\n        ref, sec = os.path.basename(self.path_gunw).split('-')[6].split('_')\n        mid_time = os.path.basename(self.path_gunw).split('-')[7]\n        return f'{ref}-{sec}_{mid_time}'\n\n\n    def get_datetimes(self):\n\"\"\" Get the datetimes and set the satellite for orbit \"\"\"\n        ref_sec  = self.get_slc_dt()\n        middates = []\n        for aq in ref_sec:\n            st, en   = aq\n            midpt    = st + (en-st)/2\n            middates.append(int(midpt.date().strftime('%Y%m%d')))\n            midtime = midpt.time().strftime('%H:%M:%S')\n        return middates, midtime\n\n\n    def get_slc_dt(self):\n\"\"\" Grab the SLC start date and time from the GUNW \"\"\"\n        group    = 'science/radarMetaData/inputSLC'\n        lst_sten = []\n        for i, key in enumerate('reference secondary'.split()):\n            ds   = xr.open_dataset(self.path_gunw, group=f'{group}/{key}')\n            slcs = ds['L1InputGranules']\n            nslcs = slcs.count().item()\n            # single slc\n            if nslcs == 1:\n                slc    = slcs.item()\n                assert slc, f'Missing {key} SLC  metadata in GUNW: {self.f}'\n                st = datetime.strptime(slc.split('_')[5], '%Y%m%dT%H%M%S')\n                en = datetime.strptime(slc.split('_')[6], '%Y%m%dT%H%M%S')\n            else:\n                st, en = datetime(1989, 3, 1), datetime(1989, 3, 1)\n                for j in range(nslcs):\n                    slc = slcs.data[j]\n                    if slc:\n                        ## get the maximum range\n                        st_tmp = datetime.strptime(slc.split('_')[5], '%Y%m%dT%H%M%S')\n                        en_tmp = datetime.strptime(slc.split('_')[6], '%Y%m%dT%H%M%S')\n\n                        ## check the second SLC is within one day of the previous\n                        if st &gt; datetime(1989, 3, 1):\n                            stdiff = np.abs((st_tmp - st).days)\n                            endiff = np.abs((en_tmp - en).days)\n                            assert stdiff &lt; 2 and endiff &lt; 2, 'SLCs granules are too far apart in time. Incorrect metadata'\n\n\n                        st = st_tmp if st_tmp &gt; st else st\n                        en = en_tmp if en_tmp &gt; en else en\n\n                assert st&gt;datetime(1989, 3, 1), f'Missing {key} SLC metadata in GUNW: {self.f}'\n\n            lst_sten.append([st, en])\n\n        return lst_sten\n\n\n    def get_look_dir(self):\n        look_dir = os.path.basename(self.path_gunw).split('-')[3].lower()\n        return 'right' if look_dir == 'r' else 'left'\n\n\n    def get_wavelength(self):\n        group ='science/radarMetaData'\n        with xr.open_dataset(self.path_gunw, group=group) as ds:\n            wavelength = ds['wavelength'].item()\n        return wavelength\n\n\n    def get_orbit_file(self):\n\"\"\" Get orbit file for reference (GUNW: first &amp; later date)\"\"\"\n        orbit_dir = os.path.join(self.out_dir, 'orbits')\n        os.makedirs(orbit_dir, exist_ok=True)\n\n        # just to get the correct satellite\n        group    = 'science/radarMetaData/inputSLC/reference'\n\n        ds   = xr.open_dataset(self.path_gunw, group=f'{group}')\n        slcs = ds['L1InputGranules']\n        nslcs = slcs.count().item()\n\n        if nslcs == 1:\n            slc    = slcs.item()\n        else:\n            for j in range(nslcs):\n                slc = slcs.data[j]\n                if slc:\n                    break\n\n        sat = slc.split('_')[0]\n        dt  = datetime.strptime(f'{self.dates[0]}T{self.mid_time}', '%Y%m%dT%H:%M:%S')\n\n        path_orb = eof.download.download_eofs([dt], [sat], save_dir=orbit_dir)\n\n        return path_orb\n\n\n    ## ------ methods below are not used\n    def get_version(self):\n        with xr.open_dataset(self.path_gunw) as ds:\n            version = ds.attrs['version']\n        return version\n\n\n    def getHeights(self):\n\"\"\" Get the 4 height levels within a GUNW \"\"\"\n        group ='science/grids/imagingGeometry'\n        with xr.open_dataset(self.path_gunw, group=group) as ds:\n            hgts = ds.heightsMeta.data.tolist()\n        return hgts\n\n\n    def calc_spacing_UTM(self, posting:float=0.01):\n\"\"\" Convert desired horizontal posting in degrees to meters\n\n        Want to calculate delays close to native model resolution (3 km for HRR)\n        \"\"\"\n        from RAiDER.utilFcns import WGS84_to_UTM\n        group = 'science/grids/data'\n        with xr.open_dataset(self.path_gunw, group=group) as ds0:\n            lats = ds0.latitude.data\n            lons = ds0.longitude.data\n\n\n        lat0, lon0 = lats[0], lons[0]\n        lat1, lon1 = lat0 + posting, lon0 + posting\n        res        = WGS84_to_UTM(np.array([lon0, lon1]), np.array([lat0, lat1]))\n        lon_spacing_m = np.subtract(*res[2][::-1])\n        lat_spacing_m = np.subtract(*res[3][::-1])\n        return np.mean([lon_spacing_m, lat_spacing_m])\n\n\n    def makeLatLonGrid_native(self):\n\"\"\" Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba) \"\"\"\n        group = 'science/grids/data'\n        with xr.open_dataset(self.path_gunw, group=group) as ds0:\n            lats = ds0.latitude.data\n            lons = ds0.longitude.data\n\n        Lat, Lon  = np.meshgrid(lats, lons)\n\n        dims   = 'longitude latitude'.split()\n        da_lon = xr.DataArray(Lon.T, coords=[Lon[0, :], Lat[:, 0]], dims=dims)\n        da_lat = xr.DataArray(Lat.T, coords=[Lon[0, :], Lat[:, 0]], dims=dims)\n\n        dst_lat = os.path.join(self.out_dir, 'latitude.geo')\n        dst_lon = os.path.join(self.out_dir, 'longitude.geo')\n\n        da_lat.to_netcdf(dst_lat)\n        da_lon.to_netcdf(dst_lon)\n\n        logger.debug('Wrote: %s', dst_lat)\n        logger.debug('Wrote: %s', dst_lon)\n        return dst_lat, dst_lon\n\n\n    def make_cube(self):\n\"\"\" Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba) \"\"\"\n        group = 'science/grids/data'\n        with xr.open_dataset(self.path_gunw, group=group) as ds0:\n            lats0 = ds0.latitude.data\n            lons0 = ds0.longitude.data\n\n        lat_st, lat_en = np.floor(lats0.min()), np.ceil(lats0.max())\n        lon_st, lon_en = np.floor(lons0.min()), np.ceil(lons0.max())\n\n        lats = np.arange(lat_st, lat_en, DCT_POSTING[self.wmodel])\n        lons = np.arange(lon_st, lon_en, DCT_POSTING[self.wmodel])\n\n        S, N = lats.min(), lats.max()\n        W, E = lons.min(), lons.max()\n\n        ds = xr.Dataset(coords={'latitude': lats, 'longitude': lons, 'heights': self.heights})\n        dst_cube = os.path.join(self.out_dir, f'GeoCube_{self.name}.nc')\n        ds.to_netcdf(dst_cube)\n\n        logger.info('Wrote cube to: %s', dst_cube)\n        return dst_cube\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.calc_spacing_UTM","title":"<code>calc_spacing_UTM(posting=0.01)</code>","text":"<p>Convert desired horizontal posting in degrees to meters</p> <p>Want to calculate delays close to native model resolution (3 km for HRR)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def calc_spacing_UTM(self, posting:float=0.01):\n\"\"\" Convert desired horizontal posting in degrees to meters\n\n    Want to calculate delays close to native model resolution (3 km for HRR)\n    \"\"\"\n    from RAiDER.utilFcns import WGS84_to_UTM\n    group = 'science/grids/data'\n    with xr.open_dataset(self.path_gunw, group=group) as ds0:\n        lats = ds0.latitude.data\n        lons = ds0.longitude.data\n\n\n    lat0, lon0 = lats[0], lons[0]\n    lat1, lon1 = lat0 + posting, lon0 + posting\n    res        = WGS84_to_UTM(np.array([lon0, lon1]), np.array([lat0, lat1]))\n    lon_spacing_m = np.subtract(*res[2][::-1])\n    lat_spacing_m = np.subtract(*res[3][::-1])\n    return np.mean([lon_spacing_m, lat_spacing_m])\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.getHeights","title":"<code>getHeights()</code>","text":"<p>Get the 4 height levels within a GUNW</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def getHeights(self):\n\"\"\" Get the 4 height levels within a GUNW \"\"\"\n    group ='science/grids/imagingGeometry'\n    with xr.open_dataset(self.path_gunw, group=group) as ds:\n        hgts = ds.heightsMeta.data.tolist()\n    return hgts\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.get_bbox","title":"<code>get_bbox()</code>","text":"<p>Get the bounding box (SNWE) from an ARIA GUNW product</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def get_bbox(self):\n\"\"\" Get the bounding box (SNWE) from an ARIA GUNW product \"\"\"\n    with xr.open_dataset(self.path_gunw) as ds:\n        poly_str = ds['productBoundingBox'].data[0].decode('utf-8')\n\n    poly     = shapely.wkt.loads(poly_str)\n    W, S, E, N = poly.bounds\n\n    return [S, N, W, E]\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.get_datetimes","title":"<code>get_datetimes()</code>","text":"<p>Get the datetimes and set the satellite for orbit</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def get_datetimes(self):\n\"\"\" Get the datetimes and set the satellite for orbit \"\"\"\n    ref_sec  = self.get_slc_dt()\n    middates = []\n    for aq in ref_sec:\n        st, en   = aq\n        midpt    = st + (en-st)/2\n        middates.append(int(midpt.date().strftime('%Y%m%d')))\n        midtime = midpt.time().strftime('%H:%M:%S')\n    return middates, midtime\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.get_orbit_file","title":"<code>get_orbit_file()</code>","text":"<p>Get orbit file for reference (GUNW: first &amp; later date)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def get_orbit_file(self):\n\"\"\" Get orbit file for reference (GUNW: first &amp; later date)\"\"\"\n    orbit_dir = os.path.join(self.out_dir, 'orbits')\n    os.makedirs(orbit_dir, exist_ok=True)\n\n    # just to get the correct satellite\n    group    = 'science/radarMetaData/inputSLC/reference'\n\n    ds   = xr.open_dataset(self.path_gunw, group=f'{group}')\n    slcs = ds['L1InputGranules']\n    nslcs = slcs.count().item()\n\n    if nslcs == 1:\n        slc    = slcs.item()\n    else:\n        for j in range(nslcs):\n            slc = slcs.data[j]\n            if slc:\n                break\n\n    sat = slc.split('_')[0]\n    dt  = datetime.strptime(f'{self.dates[0]}T{self.mid_time}', '%Y%m%dT%H:%M:%S')\n\n    path_orb = eof.download.download_eofs([dt], [sat], save_dir=orbit_dir)\n\n    return path_orb\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.get_slc_dt","title":"<code>get_slc_dt()</code>","text":"<p>Grab the SLC start date and time from the GUNW</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def get_slc_dt(self):\n\"\"\" Grab the SLC start date and time from the GUNW \"\"\"\n    group    = 'science/radarMetaData/inputSLC'\n    lst_sten = []\n    for i, key in enumerate('reference secondary'.split()):\n        ds   = xr.open_dataset(self.path_gunw, group=f'{group}/{key}')\n        slcs = ds['L1InputGranules']\n        nslcs = slcs.count().item()\n        # single slc\n        if nslcs == 1:\n            slc    = slcs.item()\n            assert slc, f'Missing {key} SLC  metadata in GUNW: {self.f}'\n            st = datetime.strptime(slc.split('_')[5], '%Y%m%dT%H%M%S')\n            en = datetime.strptime(slc.split('_')[6], '%Y%m%dT%H%M%S')\n        else:\n            st, en = datetime(1989, 3, 1), datetime(1989, 3, 1)\n            for j in range(nslcs):\n                slc = slcs.data[j]\n                if slc:\n                    ## get the maximum range\n                    st_tmp = datetime.strptime(slc.split('_')[5], '%Y%m%dT%H%M%S')\n                    en_tmp = datetime.strptime(slc.split('_')[6], '%Y%m%dT%H%M%S')\n\n                    ## check the second SLC is within one day of the previous\n                    if st &gt; datetime(1989, 3, 1):\n                        stdiff = np.abs((st_tmp - st).days)\n                        endiff = np.abs((en_tmp - en).days)\n                        assert stdiff &lt; 2 and endiff &lt; 2, 'SLCs granules are too far apart in time. Incorrect metadata'\n\n\n                    st = st_tmp if st_tmp &gt; st else st\n                    en = en_tmp if en_tmp &gt; en else en\n\n            assert st&gt;datetime(1989, 3, 1), f'Missing {key} SLC metadata in GUNW: {self.f}'\n\n        lst_sten.append([st, en])\n\n    return lst_sten\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.makeLatLonGrid_native","title":"<code>makeLatLonGrid_native()</code>","text":"<p>Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def makeLatLonGrid_native(self):\n\"\"\" Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba) \"\"\"\n    group = 'science/grids/data'\n    with xr.open_dataset(self.path_gunw, group=group) as ds0:\n        lats = ds0.latitude.data\n        lons = ds0.longitude.data\n\n    Lat, Lon  = np.meshgrid(lats, lons)\n\n    dims   = 'longitude latitude'.split()\n    da_lon = xr.DataArray(Lon.T, coords=[Lon[0, :], Lat[:, 0]], dims=dims)\n    da_lat = xr.DataArray(Lat.T, coords=[Lon[0, :], Lat[:, 0]], dims=dims)\n\n    dst_lat = os.path.join(self.out_dir, 'latitude.geo')\n    dst_lon = os.path.join(self.out_dir, 'longitude.geo')\n\n    da_lat.to_netcdf(dst_lat)\n    da_lon.to_netcdf(dst_lon)\n\n    logger.debug('Wrote: %s', dst_lat)\n    logger.debug('Wrote: %s', dst_lon)\n    return dst_lat, dst_lon\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.make_cube","title":"<code>make_cube()</code>","text":"<p>Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def make_cube(self):\n\"\"\" Make LatLonGrid at GUNW spacing (90m = 0.00083333\u00ba) \"\"\"\n    group = 'science/grids/data'\n    with xr.open_dataset(self.path_gunw, group=group) as ds0:\n        lats0 = ds0.latitude.data\n        lons0 = ds0.longitude.data\n\n    lat_st, lat_en = np.floor(lats0.min()), np.ceil(lats0.max())\n    lon_st, lon_en = np.floor(lons0.min()), np.ceil(lons0.max())\n\n    lats = np.arange(lat_st, lat_en, DCT_POSTING[self.wmodel])\n    lons = np.arange(lon_st, lon_en, DCT_POSTING[self.wmodel])\n\n    S, N = lats.min(), lats.max()\n    W, E = lons.min(), lons.max()\n\n    ds = xr.Dataset(coords={'latitude': lats, 'longitude': lons, 'heights': self.heights})\n    dst_cube = os.path.join(self.out_dir, f'GeoCube_{self.name}.nc')\n    ds.to_netcdf(dst_cube)\n\n    logger.info('Wrote cube to: %s', dst_cube)\n    return dst_cube\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.GUNW.make_fname","title":"<code>make_fname()</code>","text":"<p>Match the ref/sec filename (SLC dates may be different around edge cases)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def make_fname(self):\n\"\"\" Match the ref/sec filename (SLC dates may be different around edge cases) \"\"\"\n    ref, sec = os.path.basename(self.path_gunw).split('-')[6].split('_')\n    mid_time = os.path.basename(self.path_gunw).split('-')[7]\n    return f'{ref}-{sec}_{mid_time}'\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.check_weather_model_availability","title":"<code>check_weather_model_availability(gunw_path, weather_model_name)</code>","text":"<p>Checks weather reference and secondary dates of GUNW occur within weather model valid range</p>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.check_weather_model_availability--parameters","title":"Parameters","text":"<p>gunw_path : str weather_model_name : str     Should be one of 'HRRR', 'HRES', 'ERA5', 'ERA5T', 'GMAO'. Returns</p> <p>bool:     True if both reference and secondary acquisitions are within the valid range. We assume that     reference_date &gt; secondary_date (i.e. reference scenes are most recent)</p>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.check_weather_model_availability--raises","title":"Raises","text":"<p>ValueError     - If weather model is not correctly referencing the Class from RAiDER.models     - HRRR was requested and it's not in the HRRR CONUS or HRRR AK coverage area</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def check_weather_model_availability(gunw_path: str,\n                                     weather_model_name: str) -&gt; bool:\n\"\"\"Checks weather reference and secondary dates of GUNW occur within\n    weather model valid range\n\n    Parameters\n    ----------\n    gunw_path : str\n    weather_model_name : str\n        Should be one of 'HRRR', 'HRES', 'ERA5', 'ERA5T', 'GMAO'.\n    Returns\n    -------\n    bool:\n        True if both reference and secondary acquisitions are within the valid range. We assume that\n        reference_date &gt; secondary_date (i.e. reference scenes are most recent)\n\n    Raises\n    ------\n    ValueError\n        - If weather model is not correctly referencing the Class from RAiDER.models\n        - HRRR was requested and it's not in the HRRR CONUS or HRRR AK coverage area\n    \"\"\"\n    ref_slc_ids = get_slc_ids_from_gunw(gunw_path, reference_or_secondary='reference')\n    sec_slc_ids = get_slc_ids_from_gunw(gunw_path, reference_or_secondary='secondary')\n\n    ref_ts = get_acq_from_slc_id(ref_slc_ids[0])\n    sec_ts = get_acq_from_slc_id(sec_slc_ids[0])\n\n    if weather_model_name == 'HRRR':\n        group = '/science/grids/data/'\n        variable = 'coherence'\n        with rasterio.open(f'netcdf:{gunw_path}:{group}/{variable}') as ds:\n            gunw_poly = box(*ds.bounds)\n        if HRRR_CONUS_COVERAGE_POLYGON.intersects(gunw_poly):\n            pass\n        elif AK_GEO.intersects(gunw_poly):\n            weather_model_name = 'HRRRAK'\n        else:\n            raise ValueError('HRRR was requested but it is not available in this area')\n\n    # source: https://stackoverflow.com/a/7668273\n    # Allows us to get weather models as strings\n    # getattr(module, 'HRRR') will return HRRR class\n    module = sys.modules['RAiDER.models']\n    weather_model_names = module.__all__\n    if weather_model_name not in weather_model_names:\n        raise ValueError(f'The \"weather_model_name\" must be in {\", \".join(weather_model_names)}')\n\n    weather_model_cls = getattr(module, weather_model_name)\n    weather_model = weather_model_cls()\n\n    wm_start_date, wm_end_date = weather_model._valid_range\n    if isinstance(wm_end_date, str) and wm_end_date == 'Present':\n        wm_end_date = datetime.today() - weather_model._lag_time\n    elif not isinstance(wm_end_date, datetime):\n        raise ValueError(f'the weather model\\'s end date is not valid: {wm_end_date}')\n    ref_cond = ref_ts &lt;= wm_end_date\n    sec_cond = sec_ts &gt;= wm_start_date\n    return ref_cond and sec_cond\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.main","title":"<code>main(args)</code>","text":"<p>Read parameters needed for RAiDER from ARIA Standard Products (GUNW)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def main(args):\n\"\"\" Read parameters needed for RAiDER from ARIA Standard Products (GUNW) \"\"\"\n\n    # Check if WEATHER MODEL API credentials hidden file exists, if not create it or raise ERROR\n    credentials.check_api(args.weather_model, args.api_uid, args.api_key)\n\n    GUNWObj = GUNW(args.file, args.weather_model, args.output_directory)\n\n    raider_cfg  = {\n           'weather_model': args.weather_model,\n           'look_dir':  GUNWObj.look_dir,\n           'cube_spacing_in_m': GUNWObj.spacing_m,\n           'aoi_group' : {'bounding_box': GUNWObj.SNWE},\n           'height_group' : {'height_levels': GUNWObj.heights},\n           'date_group': {'date_list': GUNWObj.dates},\n           'time_group': {'time': GUNWObj.mid_time,\n                          # Options are 'none', 'center_time', and 'azimuth_time_grid'\n                          'interpolate_time': args.interpolate_time},\n           'los_group' : {'ray_trace': True,\n                          'orbit_file': GUNWObj.OrbitFile,\n                          'wavelength': GUNWObj.wavelength,\n                          },\n\n           'runtime_group': {'raster_format': 'nc',\n                             'output_directory': args.output_directory,\n                             }\n    }\n\n    path_cfg = f'GUNW_{GUNWObj.name}.yaml'\n    update_yaml(raider_cfg, path_cfg)\n    return path_cfg, GUNWObj.wavelength\n</code></pre>"},{"location":"reference/#RAiDER.aria.prepFromGUNW.update_yaml","title":"<code>update_yaml(dct_cfg, dst='GUNW.yaml')</code>","text":"<p>Write a new yaml file from a dictionary.</p> <p>Updates parameters in the default 'raider.yaml' file. Each key:value pair will in 'dct_cfg' will overwrite that in the default</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/aria/prepFromGUNW.py</code> <pre><code>def update_yaml(dct_cfg:dict, dst:str='GUNW.yaml'):\n\"\"\" Write a new yaml file from a dictionary.\n\n    Updates parameters in the default 'raider.yaml' file.\n    Each key:value pair will in 'dct_cfg' will overwrite that in the default\n    \"\"\"\n\n    template_file = os.path.join(\n                    os.path.dirname(RAiDER.__file__), 'cli', 'raider.yaml')\n\n    with open(template_file, 'r') as f:\n        try:\n            params = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            print(exc)\n            raise ValueError(f'Something is wrong with the yaml file {example_yaml}')\n\n    params = {**params, **dct_cfg}\n\n    with open(dst, 'w') as fh:\n        yaml.safe_dump(params, fh,  default_flow_style=False)\n\n    logger.info (f'Wrote new cfg file: %s', dst)\n    return dst\n</code></pre>"},{"location":"reference/#RAiDER.checkArgs","title":"<code>checkArgs</code>","text":""},{"location":"reference/#RAiDER.checkArgs.checkArgs","title":"<code>checkArgs(args)</code>","text":"<p>Helper fcn for checking argument compatibility and returns the correct variables</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/checkArgs.py</code> <pre><code>def checkArgs(args):\n'''\n    Helper fcn for checking argument compatibility and returns the\n    correct variables\n    '''\n\n    #########################################################################################################################\n    # Directories\n    if args.weather_model_directory is None:\n        args.weather_model_directory = os.path.join(args.output_directory, 'weather_files')\n\n    os.makedirs(args.output_directory, exist_ok=True)\n    os.makedirs(args.weather_model_directory, exist_ok=True)\n    args['weather_model'].set_wmLoc(args.weather_model_directory)\n\n    #########################################################################################################################\n    # Date and Time parsing\n    args.date_list = [datetime.combine(d, args.time) for d in args.date_list]\n    if (len(args.date_list) &gt; 1) &amp; (args.orbit_file is not None):\n        logger.warning('Only one orbit file is being used to get the '\n                        'look vectors for all requested times, if you '\n                        'want to use separate orbit files you will '\n                        'need to run raider separately for each time.')\n\n    args.los.setTime(args.date_list[0])\n\n    #########################################################################################################################\n    # filenames\n    wetNames, hydroNames = [], []\n    for d in args.date_list:\n        if (args.aoi.type() != 'bounding_box'):\n\n            # Handle the GNSS station file\n            if (args.aoi.type()=='station_file'):\n                wetFilename = os.path.join(\n                    args.output_directory,\n                    f'{args.weather_model._dataset.upper()}_Delay'\\\n                    f'_{d.strftime(\"%Y%m%dT%H%M%S\")}_ztd.csv'\n                )\n\n                hydroFilename = '' # only the 'wetFilename' is used for the station_file\n\n                # copy the input station file to the output location for editing\n                indf = pd.read_csv(args.aoi._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n                indf.to_csv(wetFilename, index=False)\n\n            else:\n                # This implies rasters\n                fmt = get_raster_ext(args.file_format)\n                wetFilename, hydroFilename = makeDelayFileNames(\n                    d,\n                    args.los,\n                    fmt,\n                    args.weather_model._dataset.upper(),\n                    args.output_directory,\n                )\n\n\n        else:\n            # In this case a cube file format is needed\n            if args.file_format not in '.nc .h5 h5 hdf5 .hdf5 nc'.split():\n                fmt = 'nc'\n                logger.debug('Invalid extension %s for cube. Defaulting to .nc', args.file_format)\n            else:\n                fmt = args.file_format.strip('.').replace('df', '')\n\n            wetFilename, hydroFilename = makeDelayFileNames(\n                d,\n                args.los,\n                fmt,\n                args.weather_model._dataset.upper(),\n                args.output_directory,\n            )\n\n        wetNames.append(wetFilename)\n        hydroNames.append(hydroFilename)\n\n    args.wetFilenames = wetNames\n    args.hydroFilenames = hydroNames\n\n    return args\n</code></pre>"},{"location":"reference/#RAiDER.checkArgs.makeDelayFileNames","title":"<code>makeDelayFileNames(time, los, outformat, weather_model_name, out)</code>","text":"<p>return names for the wet and hydrostatic delays.</p>"},{"location":"reference/#RAiDER.checkArgs.makeDelayFileNames--examples","title":"Examples:","text":"<p>makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5') makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\") ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5')</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/checkArgs.py</code> <pre><code>def makeDelayFileNames(time, los, outformat, weather_model_name, out):\n'''\n    return names for the wet and hydrostatic delays.\n\n    # Examples:\n    &gt;&gt;&gt; makeDelayFileNames(datetime(2020, 1, 1, 0, 0, 0), None, \"h5\", \"model_name\", \"some_dir\")\n    ('some_dir/model_name_wet_00_00_00_ztd.h5', 'some_dir/model_name_hydro_00_00_00_ztd.h5')\n    &gt;&gt;&gt; makeDelayFileNames(None, None, \"h5\", \"model_name\", \"some_dir\")\n    ('some_dir/model_name_wet_ztd.h5', 'some_dir/model_name_hydro_ztd.h5')\n    '''\n    format_string = \"{model_name}_{{}}_{time}{los}.{ext}\".format(\n        model_name=weather_model_name,\n        time=time.strftime(\"%Y%m%dT%H%M%S_\") if time is not None else \"\",\n        los=\"ztd\" if (isinstance(los, Zenith) or los is None) else \"std\",\n        ext=outformat\n    )\n    hydroname, wetname = (\n        format_string.format(dtyp) for dtyp in ('hydro', 'wet')\n    )\n\n    hydro_file_name = os.path.join(out, hydroname)\n    wet_file_name = os.path.join(out, wetname)\n    return wet_file_name, hydro_file_name\n</code></pre>"},{"location":"reference/#RAiDER.cli","title":"<code>cli</code>","text":""},{"location":"reference/#RAiDER.cli.__main__","title":"<code>__main__</code>","text":""},{"location":"reference/#RAiDER.cli.parser","title":"<code>parser</code>","text":""},{"location":"reference/#RAiDER.cli.raider","title":"<code>raider</code>","text":""},{"location":"reference/#RAiDER.cli.raider.calcDelays","title":"<code>calcDelays(iargs=None)</code>","text":"<p>Parse command line arguments using argparse.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/raider.py</code> <pre><code>def calcDelays(iargs=None):\n\"\"\" Parse command line arguments using argparse. \"\"\"\n    import RAiDER\n    import RAiDER.processWM\n    from RAiDER.delay import tropo_delay\n    from RAiDER.checkArgs import checkArgs\n    from RAiDER.utilFcns import writeDelays, get_nearest_wmtimes\n    examples = 'Examples of use:' \\\n        '\\n\\t raider.py customTemplatefile.cfg' \\\n        '\\n\\t raider.py -g'\n\n    p = argparse.ArgumentParser(\n        description =\n            'Command line interface for RAiDER processing with a configure file.'\n            'Default options can be found by running: raider.py --generate_config',\n        epilog=examples, formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    p.add_argument(\n        'customTemplateFile', nargs='?',\n        help='custom template with option settings.\\n' +\n        \"ignored if the default smallbaselineApp.cfg is input.\"\n    )\n\n    p.add_argument(\n        '-g', '--generate_template', action='store_true',\n        help='generate default template (if it does not exist) and exit.'\n    )\n\n    p.add_argument(\n        '--download_only', action='store_true',\n        help='only download a weather model.'\n    )\n\n    ## if not None, will replace first argument (customTemplateFile)\n    args = p.parse_args(args=iargs)\n\n    # default input file\n    template_file = os.path.join(os.path.dirname(RAiDER.__file__),\n                                                        'cli', 'raider.yaml')\n\n    if args.generate_template:\n        dst = os.path.join(os.getcwd(), 'raider.yaml')\n        shutil.copyfile(template_file, dst)\n        logger.info('Wrote: %s', dst)\n        sys.exit()\n\n\n    # check: existence of input template files\n    if (not args.customTemplateFile\n            and not os.path.isfile(os.path.basename(template_file))\n            and not args.generate_template):\n        msg = \"No template file found! It requires that either:\"\n        msg += \"\\n  a custom template file, OR the default template \"\n        msg += \"\\n  file 'raider.yaml' exists in current directory.\"\n\n        p.print_usage()\n        print(examples)\n        raise SystemExit(f'ERROR: {msg}')\n\n    if  args.customTemplateFile:\n        # check the existence\n        if not os.path.isfile(args.customTemplateFile):\n            raise FileNotFoundError(args.customTemplateFile)\n\n        args.customTemplateFile = os.path.abspath(args.customTemplateFile)\n    else:\n        args.customTemplateFile = template_file\n\n    # Read the template file\n    params = read_template_file(args.customTemplateFile)\n\n    # Argument checking\n    params  = checkArgs(params)\n    dl_only = True if params['download_only'] or args.download_only else False\n\n    if not params.verbose:\n        logger.setLevel(logging.INFO)\n\n    # Extract and buffer the AOI\n    los = params['los']\n    aoi = params['aoi']\n    model = params['weather_model']\n\n    # adjust user requested AOI by grid size and buffer slightly\n    aoi.add_buffer(model.getLLRes())\n\n    # define the xy grid within the buffered bounding box\n    aoi.set_output_xygrid(params['output_projection'])\n\n    # add a buffer determined by latitude for ray tracing\n    if los.ray_trace():\n        wm_bounds = aoi.calc_buffer_ray(los.getSensorDirection(),\n                                lookDir=los.getLookDirection(), incAngle=30)\n    else:\n        wm_bounds = aoi.bounds()\n\n    model.set_latlon_bounds(wm_bounds, output_spacing=aoi.get_output_spacing())\n\n    wet_filenames = []\n    for t, w, f in zip(\n        params['date_list'],\n        params['wetFilenames'],\n        params['hydroFilenames']\n    ):\n\n        ###########################################################\n        # weather model calculation\n        logger.debug('Starting to run the weather model calculation')\n        logger.debug(f'Requested date,time: {t.strftime(\"%Y%m%d, %H:%M\")}')\n        logger.debug('Beginning weather model pre-processing')\n\n        interp_method = params.get('interpolate_time')\n        if interp_method is None:\n            interp_method = 'none'\n            logger.warning('interp_method is not specified, defaulting to \\'none\\', i.e. nearest datetime for delay '\n                           'calculation')\n\n        # Grab the closest two times unless the user specifies 'nearest' via 'none' or None.\n        # If the model time_delta is not specified then use 6\n        # The two datetimes will be combined to a single file and processed\n        # TODO: make more transparent control flow for GUNW and non-GUNW workflow\n        if (interp_method in ['none', 'center_time']):\n            times = get_nearest_wmtimes(t, [model.dtime() if \\\n                                        model.dtime() is not None else 6][0]) if interp_method == 'center_time' else [t]\n        elif interp_method == 'azimuth_time_grid':\n            n_target_dates = 3\n            step = model.dtime()\n            time_step_hours = step if step is not None else 6\n            # Will yield 3 dates\n            times = get_n_closest_datetimes(t,\n                                            n_target_dates,\n                                            time_step_hours)\n        else:\n            raise NotImplementedError('Only none, center_time, and azimuth_time_grid are accepted values for '\n                                      'interp_method.')\n        wfiles = []\n        for tt in times:\n            try:\n                wfile = RAiDER.processWM.prepareWeatherModel(model,\n                                                             tt,\n                                                             aoi.bounds(),\n                                                             makePlots=params['verbose'])\n                wfiles.append(wfile)\n\n            # catch when requested datetime fails\n            except RuntimeError as re:\n                continue\n\n            # catch when something else within weather model class fails\n            except Exception as e:\n                S, N, W, E = wm_bounds\n                logger.info(f'Weather model point bounds are {S:.2f}/{N:.2f}/{W:.2f}/{E:.2f}')\n                logger.info(f'Query datetime: {tt}')\n                msg = f'Downloading and/or preparation of {model._Name} failed.'\n                logger.error(e)\n                logger.error(msg)\n\n        # dont process the delays for download only\n        if dl_only:\n            continue\n\n        if len(wfiles) == 0:\n            logger.error('No weather model data was successfully processed.')\n            if len(params['date_list']) == 1:\n                raise RuntimeError\n            # skip date and continue processing if multiple dates are requested\n            else:\n                continue\n\n        # nearest weather model time via 'none' is specified\n        # When interp_method is 'none' only 1 weather model file and one relevant time\n        elif (interp_method == 'none') and (len(wfiles) == 1) and (len(times) == 1):\n            weather_model_file = wfiles[0]\n\n        # only one time in temporal interpolation worked\n        # TODO: this seems problematic - unexpected behavior possibly for 'center_time'\n        elif len(wfiles)==1 and len(times)==2:\n            logger.warning('Time interpolation did not succeed, defaulting to nearest available date')\n            weather_model_file = wfiles[0]\n\n        # TODO: ensure this additional conditional is appropriate; assuming wfiles == 2 ONLY for 'center_time'\n        #  value of 'interp_method' parameter\n        elif (interp_method == 'center_time') and (len(wfiles) == 2):\n            ds1 = xr.open_dataset(wfiles[0])\n            ds2 = xr.open_dataset(wfiles[1])\n\n            # calculate relative weights of each dataset\n            date1 = datetime.datetime.strptime(ds1.attrs['datetime'], '%Y_%m_%dT%H_%M_%S')\n            date2 = datetime.datetime.strptime(ds2.attrs['datetime'], '%Y_%m_%dT%H_%M_%S')\n            wgts  = [ 1 - get_dt(t, date1) / get_dt(date2, date1), 1 - get_dt(date2, t) / get_dt(date2, date1)]\n            try:\n                assert np.isclose(np.sum(wgts), 1)\n            except AssertionError:\n                logger.error('Time interpolation weights do not sum to one; something is off with query datetime: %s', t)\n                continue\n\n            # combine datasets\n            ds = ds1\n            for var in ['wet', 'hydro', 'wet_total', 'hydro_total']:\n                ds[var] = (wgts[0] * ds1[var]) + (wgts[1] * ds2[var])\n            ds.attrs['Date1'] = 0\n            ds.attrs['Date2'] = 0\n            weather_model_file = os.path.join(\n                os.path.dirname(wfiles[0]),\n                os.path.basename(wfiles[0]).split('_')[0] + '_' + t.strftime('%Y_%m_%dT%H_%M_%S') + '_timeInterp_' + '_'.join(wfiles[0].split('_')[-4:]),\n            )\n            ds.to_netcdf(weather_model_file)\n        elif (interp_method == 'azimuth_time_grid') and (len(wfiles) == 3):\n            datasets = [xr.open_dataset(f) for f in wfiles]\n\n            # Each model will require some inspection here\n            # the subsequent s1 azimuth time grid requires dimension\n            # inputs to all have same dimensions and either be\n            # 1d or 3d.\n            if model._dataset in ['hrrr', 'hrrrak']:\n                lat_2d = datasets[0].latitude.data\n                lon_2d = datasets[0].longitude.data\n                z_1d = datasets[0].z.data\n                m, n, p = z_1d.shape[0], lat_2d.shape[0], lat_2d.shape[1]\n\n                lat = np.broadcast_to(lat_2d, (m, n, p))\n                lon = np.broadcast_to(lon_2d, (m, n, p))\n                hgt = np.broadcast_to(z_1d[:, None, None], (m, n, p))\n\n            else:\n                raise NotImplementedError('Azimuth Time is currently only implemented for HRRR')\n\n            time_grid = get_s1_azimuth_time_grid(lon,\n                                                 lat,\n                                                 hgt,\n                                                 # This is the acq time from loop\n                                                 t)\n\n            if np.any(np.isnan(time_grid)):\n                raise ValueError('The Time Grid return nans meaning no orbit was downloaded.')\n            wgts = get_inverse_weights_for_dates(time_grid, times)\n            # combine datasets\n            ds_out = datasets[0]\n            for var in ['wet', 'hydro', 'wet_total', 'hydro_total']:\n                ds_out[var] = sum([wgt * ds[var] for (wgt, ds) in zip(wgts, datasets)])\n            ds_out.attrs['Date1'] = 0\n            ds_out.attrs['Date2'] = 0\n            weather_model_file = os.path.join(\n                os.path.dirname(wfiles[0]),\n                # TODO: clean up\n                os.path.basename(wfiles[0]).split('_')[0] + '_' + t.strftime('%Y_%m_%dT%H_%M_%S') + '_timeInterpAziGrid_' + '_'.join(wfiles[0].split('_')[-4:]),\n            )\n            ds_out.to_netcdf(weather_model_file)\n        # TODO: test to ensure this error is caught\n        else:\n            n = len(wfiles)\n            raise NotImplementedError(f'The {interp_method} with {n} retrieved weather model files was not well posed '\n                                      'for the dela current workflow.')\n\n        # Now process the delays\n        try:\n            wet_delay, hydro_delay = tropo_delay(\n                t, weather_model_file, aoi, los,\n                height_levels = params['height_levels'],\n                out_proj = params['output_projection'],\n                zref=params['zref']\n            )\n        except RuntimeError:\n            logger.exception(\"Datetime %s failed\", t)\n            continue\n\n        # Different options depending on the inputs\n        if los.is_Projected():\n            out_filename = w.replace(\"_ztd\", \"_std\")\n            f = f.replace(\"_ztd\", \"_std\")\n        elif los.ray_trace():\n            out_filename = w.replace(\"_std\", \"_ray\")\n            f = f.replace(\"_std\", \"_ray\")\n        else:\n            out_filename = w\n\n        if hydro_delay is None:\n            # means that a dataset was returned (cubes and station files)\n            ds = wet_delay\n            ext = os.path.splitext(out_filename)[1]\n            out_filename = out_filename.replace('wet', 'tropo')\n\n            if ext not in ['.nc', '.h5']:\n                out_filename = f'{os.path.splitext(out_filename)[0]}.nc'\n\n            if out_filename.endswith(\".nc\"):\n                ds.to_netcdf(out_filename, mode=\"w\")\n            elif out_filename.endswith(\".h5\"):\n                ds.to_netcdf(out_filename, engine=\"h5netcdf\", invalid_netcdf=True)\n\n            logger.info('\\nSuccessfully wrote delay cube to: %s\\n', out_filename)\n\n        else:\n            if aoi.type() == 'station_file':\n                out_filename = f'{os.path.splitext(out_filename)[0]}.csv'\n\n            if aoi.type() in ['station_file', 'radar_rasters', 'geocoded_file']:\n                writeDelays(aoi, wet_delay, hydro_delay, out_filename, f, outformat=params['raster_format'])\n\n        wet_filenames.append(out_filename)\n\n    return wet_filenames\n</code></pre>"},{"location":"reference/#RAiDER.cli.raider.combineZTDFiles","title":"<code>combineZTDFiles()</code>","text":"<p>Command-line program to process delay files from RAiDER and GNSS into a single file.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/raider.py</code> <pre><code>def combineZTDFiles():\n'''\n    Command-line program to process delay files from RAiDER and GNSS into a single file.\n    '''\n    from RAiDER.gnss.processDelayFiles import main, combineDelayFiles, create_parser\n\n    p = create_parser()\n    args = p.parse_args()\n\n    if not os.path.exists(args.raider_file):\n        combineDelayFiles(args.raider_file, loc=args.raider_folder)\n\n    if not os.path.exists(args.gnss_file):\n        combineDelayFiles(args.gnss_file, loc=args.gnss_folder, source='GNSS',\n                          ref=args.raider_file, col_name=args.column_name)\n\n    if args.gnss_file is not None:\n        main(\n            args.raider_file,\n            args.gnss_file,\n            col_name=args.column_name,\n            raider_delay=args.raider_column_name,\n            outName=args.out_name,\n            localTime=args.local_time\n        )\n</code></pre>"},{"location":"reference/#RAiDER.cli.raider.downloadGNSS","title":"<code>downloadGNSS()</code>","text":"<p>Parse command line arguments using argparse.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/raider.py</code> <pre><code>def downloadGNSS():\n\"\"\"Parse command line arguments using argparse.\"\"\"\n    from RAiDER.gnss.downloadGNSSDelays import main as dlGNSS\n    p = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\" \\\n    Check for and download tropospheric zenith delays for a set of GNSS stations from UNR\n\n    Example call to virtually access and append zenith delay information to a CSV table in specified output\n    directory, across specified range of time (in YYMMDD YYMMDD) and all available times of day, and confined to specified\n    geographic bounding box :\n    downloadGNSSdelay.py --out products -y 20100101 20141231 -b '39 40 -79 -78'\n\n    Example call to virtually access and append zenith delay information to a CSV table in specified output\n    directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally :\n    downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00'\n\n\n    Example call to virtually access and append zenith delay information to a CSV table in specified output\n    directory, across specified range of time in 12 day steps (in YYMMDD YYMMDD days) and specified time of day, and distributed globally :\n    downloadGNSSdelay.py --out products -y 20100101 20141231 12 --returntime '00:00:00'\n\n    Example call to virtually access and append zenith delay information to a CSV table in specified output\n    directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and distributed globally but restricted\n    to list of stations specified in input textfile :\n    downloadGNSSdelay.py --out products -y 20100101 20141231 --returntime '00:00:00' -f station_list.txt\n\n    NOTE, following example call to physically download zenith delay information not recommended as it is not\n    necessary for most applications.\n    Example call to physically download and append zenith delay information to a CSV table in specified output\n    directory, across specified range of time (in YYMMDD YYMMDD) and specified time of day, and confined to specified\n    geographic bounding box :\n    downloadGNSSdelay.py --download --out products -y 20100101 20141231 --returntime '00:00:00' -b '39 40 -79 -78'\n    \"\"\")\n\n    # Stations to check/download\n    area = p.add_argument_group(\n        'Stations to check/download. Can be a lat/lon bounding box or file, or will run the whole world if not specified')\n    area.add_argument(\n        '--station_file', '-f', default=None, dest='station_file',\n        help=('Text file containing a list of 4-char station IDs separated by newlines'))\n    area.add_argument(\n        '-b', '--bounding_box', dest='bounding_box', type=str, default=None,\n        help=\"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\")\n    area.add_argument(\n        '--gpsrepo', '-gr', default='UNR', dest='gps_repo',\n        help=('Specify GPS repository you wish to query. Currently supported archives: UNR.'))\n\n    misc = p.add_argument_group(\"Run parameters\")\n    add_out(misc)\n\n    misc.add_argument(\n        '--date', dest='dateList',\n        help=dedent(\"\"\"\\\n            Date to calculate delay.\n            Can be a single date, a list of two dates (earlier, later) with 1-day interval, or a list of two dates and interval in days (earlier, later, interval).\n            Example accepted formats:\n               YYYYMMDD or\n               YYYYMMDD YYYYMMDD\n               YYYYMMDD YYYYMMDD N\n            \"\"\"),\n        nargs=\"+\",\n        action=DateListAction,\n        type=date_type,\n        required=True\n    )\n\n    misc.add_argument(\n        '--returntime', dest='returnTime',\n        help=\"Return delays closest to this specified time. If not specified, the GPS delays for all times will be returned. Input in 'HH:MM:SS', e.g. '16:00:00'\",\n        default=None)\n\n    misc.add_argument(\n        '--download',\n        help='Physically download data. Note this option is not necessary to proceed with statistical analyses, as data can be handled virtually in the program.',\n        action='store_true', dest='download', default=False)\n\n    add_cpus(misc)\n    add_verbose(misc)\n\n    args =  p.parse_args()\n\n    dlGNSS(args)\n    return\n</code></pre>"},{"location":"reference/#RAiDER.cli.raider.read_template_file","title":"<code>read_template_file(fname)</code>","text":"<p>Read the template file into a dictionary structure. Args:     fname (str): full path to the template file Returns:     dict: arguments to pass to RAiDER functions</p> <p>Examples:</p> <p>template = read_template_file('raider.yaml')</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/raider.py</code> <pre><code>def read_template_file(fname):\n\"\"\"\n    Read the template file into a dictionary structure.\n    Args:\n        fname (str): full path to the template file\n    Returns:\n        dict: arguments to pass to RAiDER functions\n\n    Examples:\n    &gt;&gt;&gt; template = read_template_file('raider.yaml')\n\n    \"\"\"\n    from RAiDER.cli.validators import (\n        enforce_time, parse_dates, get_query_region, get_heights, get_los, enforce_wm\n    )\n    with open(fname, 'r') as f:\n        try:\n            params = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            print(exc)\n            raise ValueError('Something is wrong with the yaml file {}'.format(fname))\n\n    # Drop any values not specified\n    params = drop_nans(params)\n\n    # Need to ensure that all the groups exist, even if they are not specified by the user\n    group_keys = ['date_group', 'time_group', 'aoi_group', 'height_group', 'los_group', 'runtime_group']\n    for key in group_keys:\n        if not key in params.keys():\n            params[key] = {}\n\n    # Parse the user-provided arguments\n    template = DEFAULT_DICT\n    for key, value in params.items():\n        if key == 'runtime_group':\n            for k, v in value.items():\n                if v is not None:\n                    template[k] = v\n        if key == 'time_group':\n            template.update(enforce_time(AttributeDict(value)))\n        if key == 'date_group':\n            template['date_list'] = parse_dates(AttributeDict(value))\n        if key == 'aoi_group':\n            ## in case a DEM is passed and should be used\n            dct_temp = {**AttributeDict(value),\n                        **AttributeDict(params['height_group'])}\n            template['aoi'] = get_query_region(AttributeDict(dct_temp))\n\n        if key == 'los_group':\n            template['los']  = get_los(AttributeDict(value))\n            template['zref'] = AttributeDict(value).get('zref')\n        if key == 'look_dir':\n            if value.lower() not in ['right', 'left']:\n                raise ValueError(f\"Unknown look direction {value}\")\n            template['look_dir'] = value.lower()\n        if key == 'cube_spacing_in_m':\n            template[key] = float(value) if isinstance(value, str) else value\n        if key == 'download_only':\n            template[key] = bool(value)\n\n    # Have to guarantee that certain variables exist prior to looking at heights\n    for key, value in params.items():\n        if key == 'height_group':\n            template.update(\n                get_heights(\n                    AttributeDict(value),\n                    template['output_directory'],\n                    template['station_file'],\n                    template['bounding_box'],\n                )\n            )\n\n        if key == 'weather_model':\n            template[key]= enforce_wm(value, template['aoi'])\n\n    template['aoi']._cube_spacing_m = template['cube_spacing_in_m']\n    return AttributeDict(template)\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot","title":"<code>statsPlot</code>","text":""},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats","title":"<code>RaiderStats</code>","text":"<p>             Bases: <code>object</code></p> <p>Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>class RaiderStats(object):\n'''\n        Class which loads standard weather model/GPS delay files and generates a series of user-requested statistics and graphics.\n    '''\n\n    # import dependencies\n    import glob\n\n    def __init__(self, filearg, col_name, unit='m', workdir='./', bbox=None, spacing=1, timeinterval=None, seasonalinterval=None,\n                 obs_errlimit='inf', time_lines=False, stationsongrids=False, station_seasonal_phase=False, cbounds=None, colorpercentile=[25, 95],\n                 usr_colormap='hot_r', grid_heatmap=False, grid_delay_mean=False, grid_delay_median=False, grid_delay_stdev=False,\n                 grid_seasonal_phase=False, grid_delay_absolute_mean=False, grid_delay_absolute_median=False,\n                 grid_delay_absolute_stdev=False, grid_seasonal_absolute_phase=False, grid_to_raster=False, min_span=[2, 0.6],\n                 period_limit=0., numCPUs=8, phaseamp_per_station=False):\n        self.fname = filearg\n        self.col_name = col_name\n        self.unit = unit\n        self.workdir = workdir\n        self.bbox = bbox\n        self.spacing = spacing\n        self.timeinterval = timeinterval\n        self.seasonalinterval = seasonalinterval\n        self.obs_errlimit = float(obs_errlimit)\n        self.time_lines = time_lines\n        self.stationsongrids = stationsongrids\n        self.station_seasonal_phase = station_seasonal_phase\n        self.cbounds = cbounds\n        self.colorpercentile = colorpercentile\n        self.usr_colormap = usr_colormap\n        self.grid_heatmap = grid_heatmap\n        self.grid_delay_mean = grid_delay_mean\n        self.grid_delay_median = grid_delay_median\n        self.grid_delay_stdev = grid_delay_stdev\n        self.grid_seasonal_phase = grid_seasonal_phase\n        self.grid_seasonal_amplitude = False\n        self.grid_seasonal_period = False\n        self.grid_seasonal_phase_stdev = False\n        self.grid_seasonal_amplitude_stdev = False\n        self.grid_seasonal_period_stdev = False\n        self.grid_seasonal_fit_rmse = False\n        self.grid_delay_absolute_mean = grid_delay_absolute_mean\n        self.grid_delay_absolute_median = grid_delay_absolute_median\n        self.grid_delay_absolute_stdev = grid_delay_absolute_stdev\n        self.grid_seasonal_absolute_phase = grid_seasonal_absolute_phase\n        self.grid_seasonal_absolute_amplitude = False\n        self.grid_seasonal_absolute_period = False\n        self.grid_seasonal_absolute_phase_stdev = False\n        self.grid_seasonal_absolute_amplitude_stdev = False\n        self.grid_seasonal_absolute_period_stdev = False\n        self.grid_seasonal_absolute_fit_rmse = False\n        self.grid_to_raster = grid_to_raster\n        self.min_span = min_span\n        self.period_limit = period_limit\n        self.numCPUs = numCPUs\n        self.phaseamp_per_station = phaseamp_per_station\n        self.grid_range = False\n        self.grid_variance = False\n        self.grid_variogram_rmse = False\n\n        # create workdir if it doesn't exist\n        if not os.path.exists(self.workdir):\n            os.mkdir(self.workdir)\n\n        # get colorbounds\n        if self.cbounds:\n            self.cbounds = [float(val) for val in self.cbounds.split()]\n\n        # Pass color percentile and check for input error\n        if self.colorpercentile is None:\n            self.colorpercentile = [25, 95]\n        if self.colorpercentile[0] &gt; self.colorpercentile[1]:\n            raise Exception('Input colorpercentile lower threshold {} higher than upper threshold {}'.format(\n                self.colorpercentile[0], self.colorpercentile[1]))\n\n        # load dataframe directly if previously generated TIF grid-file\n        if self.fname.endswith('.tif'):\n            if 'grid_heatmap' in self.fname:\n                self.grid_heatmap, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_heatmap')[0]\n            if 'grid_delay_mean' in self.fname:\n                self.grid_delay_mean, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_mean')[0]\n            if 'grid_delay_median' in self.fname:\n                self.grid_delay_median, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_median')[0]\n            if 'grid_delay_stdev' in self.fname:\n                self.grid_delay_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_stdev')[0]\n            if 'grid_seasonal_phase' in self.fname:\n                self.grid_seasonal_phase, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_phase')[0]\n            if 'grid_seasonal_period' in self.fname:\n                self.grid_seasonal_period, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_period')[0]\n            if 'grid_seasonal_amplitude' in self.fname:\n                self.grid_seasonal_amplitude, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_amplitude')[0]\n            if 'grid_seasonal_phase_stdev' in self.fname:\n                self.grid_seasonal_phase_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_phase_stdev')[0]\n            if 'grid_seasonal_amplitude_stdev' in self.fname:\n                self.grid_seasonal_amplitude_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_amplitude_stdev')[0]\n            if 'grid_seasonal_period_stdev' in self.fname:\n                self.grid_seasonal_period_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_period_stdev')[0]\n            if 'grid_seasonal_fit_rmse' in self.fname:\n                self.grid_seasonal_fit_rmse, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_fit_rmse')[0]\n            if 'grid_delay_absolute_mean' in self.fname:\n                self.grid_delay_absolute_mean, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_absolute_mean')[0]\n            if 'grid_delay_absolute_median' in self.fname:\n                self.grid_delay_absolute_median, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_absolute_median')[0]\n            if 'grid_delay_absolute_stdev' in self.fname:\n                self.grid_delay_absolute_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_delay_absolute_stdev')[0]\n            if 'grid_seasonal_absolute_phase' in self.fname:\n                self.grid_seasonal_absolute_phase, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_phase')[0]\n            if 'grid_seasonal_absolute_period' in self.fname:\n                self.grid_seasonal_absolute_period, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_period')[0]\n            if 'grid_seasonal_absolute_amplitude' in self.fname:\n                self.grid_seasonal_absolute_amplitude, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_amplitude')[0]\n            if 'grid_seasonal_absolute_phase_stdev' in self.fname:\n                self.grid_seasonal_absolute_phase_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_phase_stdev')[0]\n            if 'grid_seasonal_absolute_amplitude_stdev' in self.fname:\n                self.grid_seasonal_absolute_amplitude_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_amplitude_stdev')[0]\n            if 'grid_seasonal_absolute_period_stdev' in self.fname:\n                self.grid_seasonal_absolute_period_stdev, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_period_stdev')[0]\n            if 'grid_seasonal_absolute_fit_rmse' in self.fname:\n                self.grid_seasonal_absolute_fit_rmse, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_seasonal_absolute_fit_rmse')[0]\n            if 'grid_range' in self.fname:\n                self.grid_range, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_range')[0]\n            if 'grid_variance' in self.fname:\n                self.grid_variance, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_variance')[0]\n            if 'grid_variogram_rmse' in self.fname:\n                self.grid_variogram_rmse, self.plotbbox, self.spacing, self.colorbarfmt, self.stationsongrids, self.time_lines = load_gridfile(self.fname, self.unit)\n                self.col_name = os.path.basename(self.fname).split('_' + 'grid_variogram_rmse')[0]\n        # setup dataframe for statistical analyses (if CSV)\n        if self.fname.endswith('.csv'):\n            self.create_DF()\n\n    def _get_extent(self):  # dataset, spacing=1, userbbox=None\n\"\"\" Get the bbox, spacing in deg (by default 1deg), optionally pass user-specified bbox. Output array in WESN degrees \"\"\"\n        extent = [np.floor(min(self.df['Lon'])), np.ceil(max(self.df['Lon'])),\n                  np.floor(min(self.df['Lat'])), np.ceil(max(self.df['Lat']))]\n        if self.bbox is not None:\n            dfextents_poly = Polygon(np.column_stack((np.array([extent[0], extent[0], extent[1], extent[1], extent[0]]),\n                                                      np.array([extent[2], extent[3], extent[3], extent[2], extent[2]]))))\n            userbbox_poly = Polygon(np.column_stack((np.array([self.bbox[2], self.bbox[3], self.bbox[3], self.bbox[2], self.bbox[2]]),\n                                                     np.array([self.bbox[0], self.bbox[0], self.bbox[1], self.bbox[1], self.bbox[0]]))))\n            if userbbox_poly.intersects(dfextents_poly):\n                extent = [np.floor(self.bbox[2]), np.ceil(self.bbox[-1]), np.floor(self.bbox[0]), np.ceil(self.bbox[1])]\n            else:\n                raise Exception(\"User-specified bounds do not overlap with dataset bounds, adjust bounds and re-run program.\")\n            if extent[0] &lt; -180. or extent[1] &gt; 180. or extent[2] &lt; -90. or extent[3] &gt; 90.:\n                raise Exception(\"Specified bounds exceed -180/180 lon and/or -90/90 lat, adjust bounds and re-run program.\")\n            del dfextents_poly, userbbox_poly\n\n        # ensure that extents do not exceed -180/180 lon and -90/90 lat\n        if extent[0] &lt; -180.:\n            extent[0] = -180.\n        if extent[1] &gt; 180.:\n            extent[1] = 180.\n        if extent[2] &lt; -90.:\n            extent[2] = -90.\n        if extent[3] &gt; 90.:\n            extent[3] = 90.\n\n        # ensure even spacing, set spacing to 1 if specified spacing is not even multiple of bounds\n        if (extent[1] - extent[0]) % self.spacing != 0 or (extent[-1] - extent[-2]) % self.spacing:\n            logger.warning(\"User-specified spacing %s is not even multiple of bounds, resetting spacing to 1\\N{DEGREE SIGN}\", self.spacing)\n            self.spacing = 1\n\n        # Create corners of rectangle to be transformed to a grid\n        nw = [extent[0] + (self.spacing / 2), extent[-1] - (self.spacing / 2)]\n        se = [extent[1] - (self.spacing / 2), extent[2] + (self.spacing / 2)]\n\n        # Store grid dimension [y,x]\n        grid_dim = [int((extent[1] - extent[0]) / self.spacing),\n                    int((extent[-1] - extent[-2]) / self.spacing)]\n\n        # Iterate over 2D area\n        gridpoints = []\n        y_shape = []\n        x_shape = []\n        x = se[0]\n        while x &gt;= nw[0]:\n            y = se[1]\n            while y &lt;= nw[1]:\n                y_shape.append(y)\n                gridpoints.append([x, y])\n                y += self.spacing\n            x_shape.append(x)\n            x -= self.spacing\n        gridpoints.reverse()\n\n        return extent, grid_dim, gridpoints\n\n    def _check_stationgrid_intersection(self, stat_ID):\n'''\n        Return index of grid cell which intersects with station\n        Note: Fast, but assumes station locations don't change\n        '''\n        coord = Point((self.unique_points[1][self.unique_points[0].index(\n            stat_ID)], self.unique_points[2][self.unique_points[0].index(stat_ID)]))\n        # Get grid cell polygon which intersect with station coordinate\n        grid_int = self.polygon_tree.query(coord)\n        # Pass corresponding grid cell index\n        if len(grid_int) != 0:\n            return grid_int[0]\n        else:\n            return 'NaN'\n\n    def _reader(self):\n'''\n        Read a input file\n        '''\n        try:\n            data = pd.read_csv(self.fname, parse_dates=['Datetime'])\n            data['Date'] = data['Datetime'].apply(lambda x: x.date())\n            data['Date'] = data['Date'].apply(lambda x: dt.datetime.strptime(x.strftime(\"%Y-%m-%d\"), \"%Y-%m-%d\"))\n        except BaseException:\n            data = pd.read_csv(self.fname, parse_dates=['Date'])\n\n        # check if user-specified key is valid\n        if self.col_name not in data.keys():\n            raise Exception(\n                'User-specified key {} not found in input file {}. Must specify valid key.' .format(self.col_name, self.fname))\n\n        # if user-specified key is the same as the 'Date' field, rename\n        if self.col_name == 'Date':\n            logger.warning('Input key {} same as \"Date\" field name, rename the former'.format(self.col_name))\n            self.col_name += '_plot'\n            data[self.col_name] = data['Date']\n\n        # convert to specified output unit\n        inputunit = 'm'\n        data[self.col_name] = convert_SI(data[self.col_name], inputunit, self.unit)\n        # filter out obs by error\n        if 'sigZTD' in data.keys():\n            data['sigZTD'] = convert_SI(data['sigZTD'], inputunit, self.unit)\n            self.obs_errlimit = convert_SI(self.obs_errlimit, inputunit, self.unit)\n            data = data[data['sigZTD'] &lt;= self.obs_errlimit]\n        else:\n            logger.warning('Key \"sigZTD\" not found in dataset, cannot filter out obs by error')\n\n        return data\n\n    def create_DF(self):\n'''\n            Create dataframe.\n        '''\n        # Open file\n        self.df = self._reader()\n\n        # Filter dataframe\n        # drop all nans\n        self.df.dropna(how='any', inplace=True)\n        self.df.reset_index(drop=True, inplace=True)\n        # convert to datetime object\n\n        # time-interval filter\n        if self.timeinterval:\n            self.timeinterval = [dt.datetime.strptime(\n                val, '%Y-%m-%d') for val in self.timeinterval.split()]\n            self.df = self.df[(self.df['Date'] &gt;= self.timeinterval[0]) &amp; (\n                self.df['Date'] &lt;= self.timeinterval[-1])]\n\n        # seasonal filter\n        if self.seasonalinterval:\n            self.seasonalinterval = self.seasonalinterval.split()\n            # get day of year\n            self.seasonalinterval = [dt.datetime.strptime('2001-' + self.seasonalinterval[0], '%Y-%m-%d').timetuple(\n            ).tm_yday, dt.datetime.strptime('2001-' + self.seasonalinterval[-1], '%Y-%m-%d').timetuple().tm_yday]\n            # track input order and wrap around year if necessary\n            # e.g. month/day: 03/01 to 06/01\n            if self.seasonalinterval[0] &lt; self.seasonalinterval[1]:\n                # non leap-year\n                filtered_self = self.df[(self.df['Date'].dt.is_leap_year == False) &amp; (\n                    self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[0]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[-1])]\n                # leap-year\n                self.seasonalinterval = [i + 1 if i &gt;\n                                         59 else i for i in self.seasonalinterval]\n                filtered_self_ly = self.df[(self.df['Date'].dt.is_leap_year == True) &amp; (\n                    self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[0]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[-1])]\n                self.df = pd.concat([filtered_self, filtered_self_ly], ignore_index=True)\n                del filtered_self\n            # e.g. month/day: 12/01 to 03/01\n            if self.seasonalinterval[0] &gt; self.seasonalinterval[1]:\n                # non leap-year\n                filtered_self = self.df[(self.df['Date'].dt.is_leap_year == False) &amp; (\n                    self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[-1]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[0])]\n                # leap-year\n                self.seasonalinterval = [i + 1 if i &gt;\n                                         59 else i for i in self.seasonalinterval]\n                filtered_self_ly = self.df[(self.df['Date'].dt.is_leap_year == True) &amp; (\n                    self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[-1]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[0])]\n                self.df = pd.concat([filtered_self, filtered_self_ly], ignore_index=True)\n                del filtered_self\n\n        # estimate central longitude lines if '--time_lines' specified\n        if self.time_lines and 'Datetime' in self.df.keys():\n            self.df['Date_hr'] = self.df['Datetime'].dt.hour.astype(float).astype(\"Int32\")\n            # get list of unique times\n            all_hrs = sorted(set(self.df['Date_hr']))\n\n            # get central longitude bands associated with each time\n            central_points = []\n            # if single time, avoid loop\n            if len(all_hrs) == 1:\n                central_points.append(([0, max(self.df['Lon'])],\n                                       [0, min(self.df['Lon'])]))\n            else:\n                for i in enumerate(all_hrs):\n                    # last entry\n                    if i[0] == len(all_hrs) - 1:\n                        lons = self.df[self.df['Date_hr'] &gt; all_hrs[i[0] - 1]]\n                    # first entry\n                    elif i[0] == 0:\n                        lons = self.df[self.df['Date_hr'] &lt; all_hrs[i[0] + 1]]\n                    else:\n                        lons = self.df[(self.df['Date_hr'] &gt; all_hrs[i[0] - 1])\n                                       &amp; (self.df['Date_hr'] &lt; all_hrs[i[0] + 1])]\n                    central_points.append(([0, max(lons['Lon'])],\n                                           [0, min(lons['Lon'])]))\n            # get central longitudes\n            self.time_lines = [midpoint(i[0], i[1]) for i in central_points]\n\n        # Get bbox, buffered by grid spacing.\n        # Check if bbox input is valid list.\n        if self.bbox is not None:\n            try:\n                self.bbox = [float(val) for val in self.bbox.split()]\n            except BaseException:\n                raise Exception(\n                    'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.')\n        self.plotbbox, self.grid_dim, self.gridpoints = self._get_extent()\n\n        # generate list of grid-polygons\n        append_poly = []\n        for i in self.gridpoints:\n            bbox = [i[1] - (self.spacing / 2), i[1] + (self.spacing / 2),\n                    i[0] - (self.spacing / 2), i[0] + (self.spacing / 2)]\n            append_poly.append(Polygon(np.column_stack((np.array([bbox[2], bbox[3], bbox[3], bbox[2], bbox[2]]),\n                                                        np.array([bbox[0], bbox[0], bbox[1], bbox[1], bbox[0]])))))  # Pass lons/lats to create polygon\n\n        # Check for grid cell intersection with each station\n        idtogrid_dict = {}\n        self.unique_points = self.df.groupby(['ID', 'Lon', 'Lat']).size()\n        self.unique_points = [self.unique_points.index.get_level_values('ID').tolist(), self.unique_points.index.get_level_values(\n            'Lon').tolist(), self.unique_points.index.get_level_values('Lat').tolist()]\n        # Initiate R-tree of gridded array domain\n        self.polygon_tree = STRtree(append_poly)\n        for stat_ID in self.unique_points[0]:\n            grd_index = self._check_stationgrid_intersection(stat_ID)\n            idtogrid_dict[stat_ID] = grd_index\n\n        # map gridnode dictionary to dataframe\n        self.df['gridnode'] = self.df['ID'].map(idtogrid_dict)\n        self.df = self.df[self.df['gridnode'].astype(str) != 'NaN']\n        del self.unique_points, self.polygon_tree, idtogrid_dict, append_poly\n        # sort by grid and date\n        self.df.sort_values(['gridnode', 'Date'])\n\n        # If specified, pass station locations to superimpose on gridplots\n        if self.stationsongrids:\n            unique_points = self.df.groupby(['Lon', 'Lat']).size()\n            self.stationsongrids = [unique_points.index.get_level_values(\n                'Lon').tolist(), unique_points.index.get_level_values('Lat').tolist()]\n\n        # If specified, setup gridded array(s)\n        if self.grid_heatmap:\n            self.grid_heatmap = np.array([np.nan if i[0] not in self.df['gridnode'].values[:] else int(len(np.unique(\n                self.df['ID'][self.df['gridnode'] == i[0]]))) for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_heatmap' + '.tif')\n                save_gridfile(self.grid_heatmap, 'grid_heatmap', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%1i',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='int16',\n                              noData=0)\n\n        if self.grid_delay_mean:\n            # Take mean of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].mean()\n            unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_mean = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_mean' + '.tif')\n                save_gridfile(self.grid_delay_mean, 'grid_delay_mean', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        if self.grid_delay_median:\n            # Take mean of station-wise medians per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].median()\n            unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_median = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_median' + '.tif')\n                save_gridfile(self.grid_delay_median, 'grid_delay_median', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        if self.grid_delay_stdev:\n            # Take mean of station-wise stdev per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].std()\n            unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_stdev' + '.tif')\n                save_gridfile(self.grid_delay_stdev, 'grid_delay_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        if self.grid_delay_absolute_mean:\n            # Take mean of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])[self.col_name].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_absolute_mean = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_mean' + '.tif')\n                save_gridfile(self.grid_delay_absolute_mean, 'grid_delay_absolute_mean', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        if self.grid_delay_absolute_median:\n            # Take median of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])[self.col_name].median()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_absolute_median = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_median' + '.tif')\n                save_gridfile(self.grid_delay_absolute_median, 'grid_delay_absolute_median', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        if self.grid_delay_absolute_stdev:\n            # Take stdev of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])[self.col_name].std()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_delay_absolute_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_stdev' + '.tif')\n                save_gridfile(self.grid_delay_absolute_stdev, 'grid_delay_absolute_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n        # If specified, compute phase/amplitude fits\n        if self.station_seasonal_phase or self.grid_seasonal_phase or self.grid_seasonal_absolute_phase:\n            # Sort by coordinates\n            unique_points = self.df.sort_values(['ID', 'Date'])\n            unique_points['Date'] = [i.timestamp() for i in unique_points['Date']]\n            # Setup variables\n            self.ampfit = []\n            self.phsfit = []\n            self.periodfit = []\n            self.ampfit_c = []\n            self.phsfit_c = []\n            self.periodfit_c = []\n            self.seasonalfit_rmse = []\n            args = []\n            for i in sorted(list(set(unique_points['ID']))):\n                # pass all values corresponding to station (ID, data = y, time = x)\n                args.append((i, unique_points[unique_points['ID'] == i]['Date'].to_list(), unique_points[unique_points['ID'] == i][self.col_name].to_list(), self.min_span[0], self.min_span[1], self.period_limit))\n            # Parallelize iteration through all grid-cells and time slices\n            with multiprocessing.Pool(self.numCPUs) as multipool:\n                for i, j, k, l, m, n, o in multipool.starmap(self._amplitude_and_phase, args):\n                    self.ampfit.extend(i)\n                    self.phsfit.extend(j)\n                    self.periodfit.extend(k)\n                    self.ampfit_c.extend(l)\n                    self.phsfit_c.extend(m)\n                    self.periodfit_c.extend(n)\n                    self.seasonalfit_rmse.extend(o)\n            # map phase/amplitude fits dictionary to dataframe\n            self.phsfit = {k: v for d in self.phsfit for k, v in d.items()}\n            self.ampfit = {k: v for d in self.ampfit for k, v in d.items()}\n            self.periodfit = {k: v for d in self.periodfit for k, v in d.items()}\n            self.df['phsfit'] = self.df['ID'].map(self.phsfit)\n            # check if there are any valid data values\n            if self.df['phsfit'].isnull().values.all(axis=0):\n                raise Exception(\"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {}\".\n                                format(self.min_span[0], self.min_span[1]))\n            self.df['ampfit'] = self.df['ID'].map(self.ampfit)\n            self.df['periodfit'] = self.df['ID'].map(self.periodfit)\n            self.phsfit_c = {k: v for d in self.phsfit_c for k, v in d.items()}\n            self.ampfit_c = {k: v for d in self.ampfit_c for k, v in d.items()}\n            self.periodfit_c = {k: v for d in self.periodfit_c for k, v in d.items()}\n            self.seasonalfit_rmse = {k: v for d in self.seasonalfit_rmse for k, v in d.items()}\n            self.df['phsfit_c'] = self.df['ID'].map(self.phsfit_c)\n            self.df['ampfit_c'] = self.df['ID'].map(self.ampfit_c)\n            self.df['periodfit_c'] = self.df['ID'].map(self.periodfit_c)\n            self.df['seasonalfit_rmse'] = self.df['ID'].map(self.seasonalfit_rmse)\n            # drop nan\n            self.df.dropna(how='any', inplace=True)\n            # If grid plots specified\n            if self.grid_seasonal_phase:\n                # Pass mean phase of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['phsfit'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['phsfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_phase = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_phase' + '.tif')\n                    save_gridfile(self.grid_seasonal_phase, 'grid_seasonal_phase', gridfile_name, self.plotbbox, self.spacing,\n                                  'days', colorbarfmt='%.1i',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass mean amplitude of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['ampfit'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['ampfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_amplitude = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_amplitude' + '.tif')\n                    save_gridfile(self.grid_seasonal_amplitude, 'grid_seasonal_amplitude', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.3f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass mean period of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['periodfit'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['periodfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_period = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_period' + '.tif')\n                    save_gridfile(self.grid_seasonal_period, 'grid_seasonal_period', gridfile_name, self.plotbbox, self.spacing,\n                                  'years', colorbarfmt='%.2f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                ########################################################################################################################\n                # Pass mean phase stdev of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['phsfit_c'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['phsfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_phase_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_phase_stdev, 'grid_seasonal_phase_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  'days', colorbarfmt='%.1i',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass mean amplitude stdev of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['ampfit_c'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['ampfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_amplitude_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_amplitude_stdev, 'grid_seasonal_amplitude_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.3f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass mean period stdev of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['periodfit_c'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['periodfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_period_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_period_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_period_stdev, 'grid_seasonal_period_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  'years', colorbarfmt='%.2e',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass mean seasonal fit RMSE of station-wise means per gridcell\n                unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['seasonalfit_rmse'].mean()\n                unique_points = unique_points.groupby(['gridnode'])['seasonalfit_rmse'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_fit_rmse = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif')\n                    save_gridfile(self.grid_seasonal_fit_rmse, 'grid_seasonal_fit_rmse', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.3f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n            ########################################################################################################################\n            if self.grid_seasonal_absolute_phase:\n                # Pass absolute mean phase of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['phsfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_phase = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_phase, 'grid_seasonal_absolute_phase', gridfile_name, self.plotbbox, self.spacing,\n                                  'days', colorbarfmt='%.1i',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass absolute mean amplitude of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['ampfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_amplitude = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_amplitude, 'grid_seasonal_absolute_amplitude', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.3f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass absolute mean period of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['periodfit'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_period = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_period' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_period, 'grid_seasonal_absolute_period', gridfile_name, self.plotbbox, self.spacing,\n                                  'years', colorbarfmt='%.2f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                ########################################################################################################################\n                # Pass absolute mean phase stdev of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['phsfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_phase_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_phase_stdev, 'grid_seasonal_absolute_phase_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  'days', colorbarfmt='%.1i',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass absolute mean amplitude stdev of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['ampfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_amplitude_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_amplitude_stdev, 'grid_seasonal_absolute_amplitude_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.3f',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n                # Pass absolute mean period stdev of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['periodfit_c'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_period_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_period_stdev, 'grid_seasonal_absolute_period_stdev', gridfile_name, self.plotbbox, self.spacing,\n                                  'years', colorbarfmt='%.2e',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n\n                # Pass absolute mean seasonal fit RMSE of all data per gridcell\n                unique_points = self.df.groupby(['gridnode'])['seasonalfit_rmse'].mean()\n                unique_points.dropna(how='any', inplace=True)\n                self.grid_seasonal_absolute_fit_rmse = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n                ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n                # If specified, save gridded array(s)\n                if self.grid_to_raster:\n                    gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif')\n                    save_gridfile(self.grid_seasonal_absolute_fit_rmse, 'grid_seasonal_absolute_fit_rmse', gridfile_name, self.plotbbox, self.spacing,\n                                  self.unit, colorbarfmt='%.2e',\n                                  stationsongrids=self.stationsongrids,\n                                  time_lines=self.time_lines, dtype='float32')\n\n    def _amplitude_and_phase(self, station, tt, yy, min_span=2, min_frac=0.6, period_limit=0.):\n'''\n        Fit sin to the input time sequence, and return fitting parameters:\n            \"amp\", \"omega\", \"phase\", \"offset\", \"freq\", \"period\" and \"fitfunc\".\n        Minimum time span in years (min_span), minimum fractional observations in span (min_frac),\n            and period limit (period_limit) enforced for statistical analysis.\n        Source: https://stackoverflow.com/questions/16716302/how-do-i-fit-a-sine-curve-to-my-data-with-pylab-and-numpy\n        '''\n        ampfit = {}\n        phsfit = {}\n        periodfit = {}\n        ampfit_c = {}\n        phsfit_c = {}\n        periodfit_c = {}\n        seasonalfit_rmse = {}\n        ampfit[station] = np.nan\n        phsfit[station] = np.nan\n        periodfit[station] = np.nan\n        ampfit_c[station] = np.nan\n        phsfit_c[station] = np.nan\n        periodfit_c[station] = np.nan\n        seasonalfit_rmse[station] = np.nan\n        # Fit with custom fit function with fixed period, if specified\n        if period_limit != 0.:\n            # convert from years to radians/seconds\n            w = (1 / period_limit) * (1 / 31556952) * (2. * np.pi)\n\n            def custom_sine_function_base(t, A, p, c):\n                return self._sine_function_base(t, A, w, p, c)\n        else:\n            def custom_sine_function_base(t, A, w, p, c):\n                return self._sine_function_base(t, A, w, p, c)\n        # If station TS does not span specified time period, pass NaNs\n        time_span_yrs = (max(tt) - min(tt)) / 31556952\n        if time_span_yrs &gt;= min_span and len(list(set(tt))) / (time_span_yrs * 365.25) &gt;= min_frac:\n            tt = np.array(tt)\n            yy = np.array(yy)\n            ff = np.fft.fftfreq(len(tt), (tt[1] - tt[0]))  # assume uniform spacing\n            Fyy = abs(np.fft.fft(yy))\n            guess_freq = abs(ff[np.argmax(Fyy[1:]) + 1])  # excluding the zero period \"peak\", which is related to offset\n            guess_amp = np.std(yy) * 2.**0.5\n            guess_offset = np.mean(yy)\n            guess = np.array([guess_amp, 2. * np.pi * guess_freq, 0., guess_offset])\n            # Adjust frequency guess to reflect fixed period, if specified\n            if period_limit != 0.:\n                guess = np.array([guess_amp, 0., guess_offset])\n            # Catch warning where covariance cannot be estimated\n            # I.e. OptimizeWarning: Covariance of the parameters could not be estimated\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"error\", OptimizeWarning)\n                try:\n                    optimize_warning = False\n                    try:\n                        # Note, may have to adjust max number of iterations (maxfev) higher to avoid crashes\n                        popt, pcov = optimize.curve_fit(custom_sine_function_base, tt, yy, p0=guess, maxfev=int(1e6))\n                    # If sparse input such that fittitng is not possible, pass NaNs\n                    except TypeError:\n                        self.ampfit.append(np.nan), self.phsfit.append(np.nan), self.periodfit.append(np.nan), \\\n                            self.ampfit_c.append(np.nan), self.phsfit_c.append(np.nan), \\\n                            self.periodfit_c.append(np.nan), self.seasonalfit_rmse.append(np.nan)\n                        return self.ampfit, self.phsfit, self.periodfit, self.ampfit_c, \\\n                            self.phsfit_c, self.periodfit_c, self.seasonalfit_rmse\n                except OptimizeWarning:\n                    optimize_warning = True\n                    warnings.simplefilter(\"ignore\", OptimizeWarning)\n                    popt, pcov = optimize.curve_fit(custom_sine_function_base, tt, yy, p0=guess, maxfev=int(1e6))\n                    print('OptimizeWarning: Covariance for station {} could not be estimated. Refer to debug figure here {} \\\n                          '.format(station, os.path.join(self.workdir, 'phaseamp_per_station', 'station{}.png'.format(station))))\n                    pass\n            # Adjust expected output to reflect fixed period, if specified\n            if period_limit != 0.:\n                A, p, c = popt\n            else:\n                A, w, p, c = popt\n            # convert from radians/seconds to years\n            f = (w / (2. * np.pi)) * (31556952)\n            f = 1 / f\n\n            def fitfunc(t):\n                return A * np.sin(w * t + p) + c\n            # Outputs = \"amp\": A, \"angular frequency\": w, \"phase\": p, \"offset\": c, \"freq\": f, \"period\": 1./f,\n            #        \"fitfunc\": fitfunc, \"maxcov\": np.max(pcov), \"rawres\": (guess,popt,pcov)\n            # Pass amplitude (specified units) and phase (days) and stdev\n            ampfit[station] = abs(A)\n            # Convert phase from rad to days, apply half wavelength shift if Amp is negative\n            if A &lt; 0:\n                p += 3.14159\n            phsfit[station] = (365.25 / 2) * np.sin(p)\n            periodfit[station] = f\n            # Catch warning where output is so small that it gets rounded to 0\n            # I.e. RuntimeWarning: invalid value encountered in double_scalars\n            with np.errstate(invalid='raise'):\n                try:\n                    # pass covariance for each parameter\n                    ampfit_c[station] = pcov[0, 0]**0.5\n                    periodfit_c[station] = pcov[1, 1]**0.5\n                    phsfit_c[station] = pcov[2, 2]**0.5\n                    # pass RMSE of fit\n                    seasonalfit_rmse[station] = yy - custom_sine_function_base(tt, *popt)\n                    seasonalfit_rmse[station] = (scipy_sum(seasonalfit_rmse[station]**2) /\n                                                 (seasonalfit_rmse[station].size - 2))**0.5\n                except FloatingPointError:\n                    pass\n            if self.phaseamp_per_station or optimize_warning:\n                # Debug plotting for each station\n                # convert time (datetime seconds) to absolute years for plotting\n                tt_plot = copy.deepcopy(tt)\n                tt_plot -= min(tt_plot)\n                tt_plot /= 31556952\n                plt.plot(tt_plot, yy, \"ok\", label=\"input\")\n                plt.xlabel(\"time (years)\")\n                plt.ylabel(\"data ({})\".format(self.unit))\n                num_testpoints = len(tt) * 10\n                if num_testpoints &gt; 1000:\n                    num_testpoints = 1000\n                tt2 = np.linspace(min(tt), max(tt), num_testpoints)\n                # convert time to years for plotting\n                tt2_plot = copy.deepcopy(tt2)\n                tt2_plot -= min(tt2_plot)\n                tt2_plot /= 31556952\n                plt.plot(tt2_plot, fitfunc(tt2), \"r-\", label=\"fit\", linewidth=2)\n                plt.legend(loc=\"best\")\n                if not os.path.exists(os.path.join(self.workdir, 'phaseamp_per_station')):\n                    os.mkdir(os.path.join(self.workdir, 'phaseamp_per_station'))\n                plt.savefig(os.path.join(self.workdir, 'phaseamp_per_station', 'station{}.png'.format(station)),\n                            format='png', bbox_inches='tight')\n                plt.close()\n                optimize_warning = False\n\n        self.ampfit.append(ampfit)\n        self.phsfit.append(phsfit)\n        self.periodfit.append(periodfit)\n        self.ampfit_c.append(ampfit_c)\n        self.phsfit_c.append(phsfit_c)\n        self.periodfit_c.append(periodfit_c)\n        self.seasonalfit_rmse.append(seasonalfit_rmse)\n\n        return self.ampfit, self.phsfit, self.periodfit, self.ampfit_c, \\\n            self.phsfit_c, self.periodfit_c, self.seasonalfit_rmse\n\n    def _sine_function_base(self, t, A, w, p, c):\n'''\n        Base function for modeling sinusoidal amplitude/phase fits.\n        '''\n        return A * np.sin(w * t + p) + c\n\n    def __call__(self, gridarr, plottype, workdir='./', drawgridlines=False, colorbarfmt='%.2f', stationsongrids=None, resValue=5, plotFormat='pdf', userTitle=None):\n'''\n            Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument.\n        '''\n        from cartopy import crs as ccrs\n        from cartopy import feature as cfeature\n        from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n        from matplotlib import ticker as mticker\n        from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n        # If specified workdir doesn't exist, create it\n        if not os.path.exists(workdir):\n            os.mkdir(workdir)\n\n        # Pass cbounds\n        cbounds = self.cbounds\n        # Initiate no-data array to mask data\n        nodat_arr = [0, np.nan, np.inf]\n        if self.unit in ['minute', 'hour', 'day', 'year']:\n            colorbarfmt = '%.1i'\n            nodat_arr = [np.nan, np.inf]\n\n        fig, axes = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n        # by default set background to white\n        axes.add_feature(cfeature.NaturalEarthFeature(\n            'physical', 'land', '50m', facecolor='white'), zorder=0)\n        axes.set_extent(self.plotbbox, ccrs.PlateCarree())\n        # add coastlines\n        axes.coastlines(linewidth=0.2, color=\"gray\", zorder=4)\n        cmap = copy.copy(mpl.cm.get_cmap(self.usr_colormap))\n        # cmap.set_bad('black', 0.)\n        # extract all colors from the hot map\n        cmaplist = [cmap(i) for i in range(cmap.N)]\n        # create the new map\n        cmap = mpl.colors.LinearSegmentedColormap.from_list(\n            'Custom cmap', cmaplist)\n        axes.set_xlabel('Longitude', weight='bold', zorder=2)\n        axes.set_ylabel('Latitude', weight='bold', zorder=2)\n\n        # set ticks\n        axes.set_xticks(np.linspace(\n            self.plotbbox[0], self.plotbbox[1], 5), crs=ccrs.PlateCarree())\n        axes.set_yticks(np.linspace(\n            self.plotbbox[2], self.plotbbox[3], 5), crs=ccrs.PlateCarree())\n        lon_formatter = LongitudeFormatter(\n            number_format='.0f', degree_symbol='')\n        lat_formatter = LatitudeFormatter(\n            number_format='.0f', degree_symbol='')\n        axes.xaxis.set_major_formatter(lon_formatter)\n        axes.yaxis.set_major_formatter(lat_formatter)\n\n        # draw central longitude lines corresponding to respective datetimes\n        if self.time_lines:\n            tl = axes.grid(axis='x', linewidth=1.5,\n                                color='blue', alpha=0.5, linestyle='-',\n                                zorder=3)\n\n        # If individual stations passed\n        if isinstance(gridarr, list):\n            # spatial distribution of stations\n            if plottype == \"station_distribution\":\n                im = axes.scatter(gridarr[0], gridarr[1], zorder=1, s=0.5,\n                                  marker='.', color='b', transform=ccrs.PlateCarree())\n\n            # passing 3rd column as z-value\n            if len(gridarr) &gt; 2:\n                # set land/water background to light gray/blue respectively so station point data can be seen\n                axes.add_feature(cfeature.NaturalEarthFeature(\n                    'physical', 'land', '50m', facecolor='#A9A9A9'), zorder=0)\n                axes.add_feature(cfeature.NaturalEarthFeature(\n                    'physical', 'ocean', '50m', facecolor='#ADD8E6'), zorder=0)\n                # set masked values as nans\n                zvalues = gridarr[2]\n                for i in nodat_arr:\n                    zvalues = np.ma.masked_where(zvalues == i, zvalues)\n                zvalues = np.ma.filled(zvalues, np.nan)\n                # define the bins and normalize\n                if cbounds is None:\n                    # avoid \"ufunc 'isnan'\" error by casting array as float\n                    cbounds = [np.nanpercentile(zvalues.astype('float'), self.colorpercentile[0]), np.nanpercentile(\n                        zvalues.astype('float'), self.colorpercentile[1])]\n                    # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError\n                    if cbounds[0] == cbounds[1]:\n                        cbounds[0] *= 0.75\n                        cbounds.sort()\n                    # adjust precision for colorbar if necessary\n                    if (abs(np.nanmax(zvalues) - np.nanmin(zvalues)) &lt; 1 and (np.nanmean(zvalues)) &lt; 1) \\\n                            or abs(np.nanmax(zvalues) - np.nanmin(zvalues)) &gt; 500:\n                        colorbarfmt = '%.2e'\n\n                colorbounds = np.linspace(cbounds[0], cbounds[1], 256)\n                colorbounds = np.unique(colorbounds)\n                norm = mpl.colors.BoundaryNorm(colorbounds, cmap.N)\n                colorbounds_ticks = np.linspace(cbounds[0], cbounds[1], 10)\n\n                # plot data and initiate colorbar\n                im = axes.scatter(gridarr[0], gridarr[1], c=zvalues, cmap=cmap, norm=norm,\n                                  zorder=1, s=0.5, marker='.', transform=ccrs.PlateCarree())\n                # initiate colorbar and control height of colorbar\n                divider = make_axes_locatable(axes)\n                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n                cbar_ax = fig.colorbar(im, spacing='proportional',\n                                       ticks=colorbounds_ticks, boundaries=colorbounds, format=colorbarfmt, pad=0.1, cax=cax)\n                cbar_ax.ax.minorticks_off()\n\n        # If gridded area passed\n        else:\n            # set masked values as nans\n            for i in nodat_arr:\n                gridarr = np.ma.masked_where(gridarr == i, gridarr)\n            gridarr = np.ma.filled(gridarr, np.nan)\n            # set land/water background to light gray/blue respectively so grid cells can be seen\n            axes.add_feature(cfeature.NaturalEarthFeature(\n                'physical', 'land', '50m', facecolor='#A9A9A9'), zorder=0)\n            axes.add_feature(cfeature.NaturalEarthFeature(\n                'physical', 'ocean', '50m', facecolor='#ADD8E6'), zorder=0)\n            # define the bins and normalize\n            if cbounds is None:\n                cbounds = [np.nanpercentile(gridarr, self.colorpercentile[0]), np.nanpercentile(\n                    gridarr, self.colorpercentile[1])]\n                # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError\n                if cbounds[0] == cbounds[1]:\n                    cbounds[0] *= 0.75\n                    cbounds.sort()\n                # plot data and initiate colorbar\n                if (abs(np.nanmax(gridarr) - np.nanmin(gridarr)) &lt; 1 and abs(np.nanmean(gridarr)) &lt; 1) \\\n                        or abs(np.nanmax(gridarr) - np.nanmin(gridarr)) &gt; 500:\n                    colorbarfmt = '%.2e'\n\n            colorbounds = np.linspace(cbounds[0], cbounds[1], 256)\n            colorbounds = np.unique(colorbounds)\n            norm = mpl.colors.BoundaryNorm(colorbounds, cmap.N)\n            colorbounds_ticks = np.linspace(cbounds[0], cbounds[1], 10)\n\n            # plot data\n            im = axes.imshow(gridarr, cmap=cmap, norm=norm, extent=self.plotbbox,\n                             zorder=1, origin='upper', transform=ccrs.PlateCarree())\n            # initiate colorbar and control height of colorbar\n            divider = make_axes_locatable(axes)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n            cbar_ax = fig.colorbar(im, spacing='proportional', ticks=colorbounds_ticks,\n                                   boundaries=colorbounds, format=colorbarfmt, pad=0.1, cax=cax)\n            cbar_ax.ax.minorticks_off()\n\n            # superimpose your gridded array with a supplementary list of point, if specified\n            if self.stationsongrids:\n                axes.scatter(self.stationsongrids[0], self.stationsongrids[1], zorder=2,\n                             s=0.5, marker='.', color='b', transform=ccrs.PlateCarree())\n\n            # draw gridlines, if specified\n            if drawgridlines:\n                gl = axes.gridlines(crs=ccrs.PlateCarree(\n                ), linewidth=0.5, color='black', alpha=0.5, linestyle='-', zorder=3)\n                gl.xlocator = mticker.FixedLocator(np.arange(\n                    self.plotbbox[0], self.plotbbox[1] + self.spacing, self.spacing).tolist())\n                gl.ylocator = mticker.FixedLocator(np.arange(\n                    self.plotbbox[2], self.plotbbox[3] + self.spacing, self.spacing).tolist())\n\n        # Add labels to colorbar, if necessary\n        if 'cbar_ax' in locals():\n            # experimental variogram fit sill heatmap\n            if plottype == \"grid_variance\":\n                cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({}\\u00b2)'.format(self.unit),\n                                  rotation=-90, labelpad=10)\n            # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap\n            elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or  \\\n                    plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\\n                    plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\\n                    plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\\n                    plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\\n                    plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\\n                    plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\\n                    plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\":\n                # update label if sigZTD\n                if 'sig' in self.col_name:\n                    cbar_ax.set_label(\"sig ZTD \" + \" \".join(plottype.replace('grid_',\n                                      '').replace('delay_', '').split('_')).title() + ' ({})'.format(self.unit),\n                                      rotation=-90, labelpad=10)\n                else:\n                    cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format(self.unit),\n                                      rotation=-90, labelpad=10)\n            # specify appropriate units for phase heatmap (days)\n            elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\\n                    plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\":\n                cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format('days'),\n                                  rotation=-90, labelpad=10)\n            # specify appropriate units for period heatmap (years)\n            elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\\n                    plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\":\n                cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format('years'),\n                                  rotation=-90, labelpad=10)\n            # gridmap of station density has no units\n            else:\n                cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title(), rotation=-90, labelpad=10)\n\n        # Add title to plots, if specified\n        if userTitle:\n            axes.set_title(userTitle, zorder=2)\n\n        # save/close figure\n        # cbar_ax.ax.locator_params(nbins=10)\n        # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]:\n            # label.set_visible(False)\n        plt.savefig(os.path.join(workdir, self.col_name + '_' + plottype + '.' + plotFormat),\n                    format=plotFormat, bbox_inches='tight')\n        plt.close()\n\n        return\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats.__call__","title":"<code>__call__(gridarr, plottype, workdir='./', drawgridlines=False, colorbarfmt='%.2f', stationsongrids=None, resValue=5, plotFormat='pdf', userTitle=None)</code>","text":"<p>Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def __call__(self, gridarr, plottype, workdir='./', drawgridlines=False, colorbarfmt='%.2f', stationsongrids=None, resValue=5, plotFormat='pdf', userTitle=None):\n'''\n        Visualize a suite of statistics w.r.t. stations. Pass either a list of points or a gridded array as the first argument. Alternatively, you may superimpose your gridded array with a supplementary list of points by passing the latter through the stationsongrids argument.\n    '''\n    from cartopy import crs as ccrs\n    from cartopy import feature as cfeature\n    from cartopy.mpl.ticker import LatitudeFormatter, LongitudeFormatter\n    from matplotlib import ticker as mticker\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    # If specified workdir doesn't exist, create it\n    if not os.path.exists(workdir):\n        os.mkdir(workdir)\n\n    # Pass cbounds\n    cbounds = self.cbounds\n    # Initiate no-data array to mask data\n    nodat_arr = [0, np.nan, np.inf]\n    if self.unit in ['minute', 'hour', 'day', 'year']:\n        colorbarfmt = '%.1i'\n        nodat_arr = [np.nan, np.inf]\n\n    fig, axes = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n    # by default set background to white\n    axes.add_feature(cfeature.NaturalEarthFeature(\n        'physical', 'land', '50m', facecolor='white'), zorder=0)\n    axes.set_extent(self.plotbbox, ccrs.PlateCarree())\n    # add coastlines\n    axes.coastlines(linewidth=0.2, color=\"gray\", zorder=4)\n    cmap = copy.copy(mpl.cm.get_cmap(self.usr_colormap))\n    # cmap.set_bad('black', 0.)\n    # extract all colors from the hot map\n    cmaplist = [cmap(i) for i in range(cmap.N)]\n    # create the new map\n    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n        'Custom cmap', cmaplist)\n    axes.set_xlabel('Longitude', weight='bold', zorder=2)\n    axes.set_ylabel('Latitude', weight='bold', zorder=2)\n\n    # set ticks\n    axes.set_xticks(np.linspace(\n        self.plotbbox[0], self.plotbbox[1], 5), crs=ccrs.PlateCarree())\n    axes.set_yticks(np.linspace(\n        self.plotbbox[2], self.plotbbox[3], 5), crs=ccrs.PlateCarree())\n    lon_formatter = LongitudeFormatter(\n        number_format='.0f', degree_symbol='')\n    lat_formatter = LatitudeFormatter(\n        number_format='.0f', degree_symbol='')\n    axes.xaxis.set_major_formatter(lon_formatter)\n    axes.yaxis.set_major_formatter(lat_formatter)\n\n    # draw central longitude lines corresponding to respective datetimes\n    if self.time_lines:\n        tl = axes.grid(axis='x', linewidth=1.5,\n                            color='blue', alpha=0.5, linestyle='-',\n                            zorder=3)\n\n    # If individual stations passed\n    if isinstance(gridarr, list):\n        # spatial distribution of stations\n        if plottype == \"station_distribution\":\n            im = axes.scatter(gridarr[0], gridarr[1], zorder=1, s=0.5,\n                              marker='.', color='b', transform=ccrs.PlateCarree())\n\n        # passing 3rd column as z-value\n        if len(gridarr) &gt; 2:\n            # set land/water background to light gray/blue respectively so station point data can be seen\n            axes.add_feature(cfeature.NaturalEarthFeature(\n                'physical', 'land', '50m', facecolor='#A9A9A9'), zorder=0)\n            axes.add_feature(cfeature.NaturalEarthFeature(\n                'physical', 'ocean', '50m', facecolor='#ADD8E6'), zorder=0)\n            # set masked values as nans\n            zvalues = gridarr[2]\n            for i in nodat_arr:\n                zvalues = np.ma.masked_where(zvalues == i, zvalues)\n            zvalues = np.ma.filled(zvalues, np.nan)\n            # define the bins and normalize\n            if cbounds is None:\n                # avoid \"ufunc 'isnan'\" error by casting array as float\n                cbounds = [np.nanpercentile(zvalues.astype('float'), self.colorpercentile[0]), np.nanpercentile(\n                    zvalues.astype('float'), self.colorpercentile[1])]\n                # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError\n                if cbounds[0] == cbounds[1]:\n                    cbounds[0] *= 0.75\n                    cbounds.sort()\n                # adjust precision for colorbar if necessary\n                if (abs(np.nanmax(zvalues) - np.nanmin(zvalues)) &lt; 1 and (np.nanmean(zvalues)) &lt; 1) \\\n                        or abs(np.nanmax(zvalues) - np.nanmin(zvalues)) &gt; 500:\n                    colorbarfmt = '%.2e'\n\n            colorbounds = np.linspace(cbounds[0], cbounds[1], 256)\n            colorbounds = np.unique(colorbounds)\n            norm = mpl.colors.BoundaryNorm(colorbounds, cmap.N)\n            colorbounds_ticks = np.linspace(cbounds[0], cbounds[1], 10)\n\n            # plot data and initiate colorbar\n            im = axes.scatter(gridarr[0], gridarr[1], c=zvalues, cmap=cmap, norm=norm,\n                              zorder=1, s=0.5, marker='.', transform=ccrs.PlateCarree())\n            # initiate colorbar and control height of colorbar\n            divider = make_axes_locatable(axes)\n            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n            cbar_ax = fig.colorbar(im, spacing='proportional',\n                                   ticks=colorbounds_ticks, boundaries=colorbounds, format=colorbarfmt, pad=0.1, cax=cax)\n            cbar_ax.ax.minorticks_off()\n\n    # If gridded area passed\n    else:\n        # set masked values as nans\n        for i in nodat_arr:\n            gridarr = np.ma.masked_where(gridarr == i, gridarr)\n        gridarr = np.ma.filled(gridarr, np.nan)\n        # set land/water background to light gray/blue respectively so grid cells can be seen\n        axes.add_feature(cfeature.NaturalEarthFeature(\n            'physical', 'land', '50m', facecolor='#A9A9A9'), zorder=0)\n        axes.add_feature(cfeature.NaturalEarthFeature(\n            'physical', 'ocean', '50m', facecolor='#ADD8E6'), zorder=0)\n        # define the bins and normalize\n        if cbounds is None:\n            cbounds = [np.nanpercentile(gridarr, self.colorpercentile[0]), np.nanpercentile(\n                gridarr, self.colorpercentile[1])]\n            # if upper/lower bounds identical, overwrite lower bound as 75% of upper bound to avoid plotting ValueError\n            if cbounds[0] == cbounds[1]:\n                cbounds[0] *= 0.75\n                cbounds.sort()\n            # plot data and initiate colorbar\n            if (abs(np.nanmax(gridarr) - np.nanmin(gridarr)) &lt; 1 and abs(np.nanmean(gridarr)) &lt; 1) \\\n                    or abs(np.nanmax(gridarr) - np.nanmin(gridarr)) &gt; 500:\n                colorbarfmt = '%.2e'\n\n        colorbounds = np.linspace(cbounds[0], cbounds[1], 256)\n        colorbounds = np.unique(colorbounds)\n        norm = mpl.colors.BoundaryNorm(colorbounds, cmap.N)\n        colorbounds_ticks = np.linspace(cbounds[0], cbounds[1], 10)\n\n        # plot data\n        im = axes.imshow(gridarr, cmap=cmap, norm=norm, extent=self.plotbbox,\n                         zorder=1, origin='upper', transform=ccrs.PlateCarree())\n        # initiate colorbar and control height of colorbar\n        divider = make_axes_locatable(axes)\n        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n        cbar_ax = fig.colorbar(im, spacing='proportional', ticks=colorbounds_ticks,\n                               boundaries=colorbounds, format=colorbarfmt, pad=0.1, cax=cax)\n        cbar_ax.ax.minorticks_off()\n\n        # superimpose your gridded array with a supplementary list of point, if specified\n        if self.stationsongrids:\n            axes.scatter(self.stationsongrids[0], self.stationsongrids[1], zorder=2,\n                         s=0.5, marker='.', color='b', transform=ccrs.PlateCarree())\n\n        # draw gridlines, if specified\n        if drawgridlines:\n            gl = axes.gridlines(crs=ccrs.PlateCarree(\n            ), linewidth=0.5, color='black', alpha=0.5, linestyle='-', zorder=3)\n            gl.xlocator = mticker.FixedLocator(np.arange(\n                self.plotbbox[0], self.plotbbox[1] + self.spacing, self.spacing).tolist())\n            gl.ylocator = mticker.FixedLocator(np.arange(\n                self.plotbbox[2], self.plotbbox[3] + self.spacing, self.spacing).tolist())\n\n    # Add labels to colorbar, if necessary\n    if 'cbar_ax' in locals():\n        # experimental variogram fit sill heatmap\n        if plottype == \"grid_variance\":\n            cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({}\\u00b2)'.format(self.unit),\n                              rotation=-90, labelpad=10)\n        # specify appropriate units for mean/median/std/amplitude/experimental variogram fit heatmap\n        elif plottype == \"grid_delay_mean\" or plottype == \"grid_delay_median\" or plottype == \"grid_delay_stdev\" or  \\\n                plottype == \"grid_seasonal_amplitude\" or plottype == \"grid_range\" or plottype == \"station_delay_mean\" or \\\n                plottype == \"station_delay_median\" or plottype == \"station_delay_stdev\" or \\\n                plottype == \"station_seasonal_amplitude\" or plottype == \"grid_delay_absolute_mean\" or \\\n                plottype == \"grid_delay_absolute_median\" or plottype == \"grid_delay_absolute_stdev\" or \\\n                plottype == \"grid_seasonal_absolute_amplitude\" or plottype == \"grid_seasonal_amplitude_stdev\" or \\\n                plottype == \"grid_seasonal_absolute_amplitude_stdev\" or plottype == \"grid_seasonal_fit_rmse\" or \\\n                plottype == \"grid_seasonal_absolute_fit_rmse\" or plottype == \"grid_variogram_rmse\":\n            # update label if sigZTD\n            if 'sig' in self.col_name:\n                cbar_ax.set_label(\"sig ZTD \" + \" \".join(plottype.replace('grid_',\n                                  '').replace('delay_', '').split('_')).title() + ' ({})'.format(self.unit),\n                                  rotation=-90, labelpad=10)\n            else:\n                cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format(self.unit),\n                                  rotation=-90, labelpad=10)\n        # specify appropriate units for phase heatmap (days)\n        elif plottype == \"station_seasonal_phase\" or plottype == \"grid_seasonal_phase\" or plottype == \"grid_seasonal_absolute_phase\" or \\\n                plottype == \"grid_seasonal_absolute_phase_stdev\" or plottype == \"grid_seasonal_phase_stdev\":\n            cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format('days'),\n                              rotation=-90, labelpad=10)\n        # specify appropriate units for period heatmap (years)\n        elif plottype == \"station_delay_period\" or plottype == \"grid_seasonal_period\" or plottype == \"grid_seasonal_absolute_period\" or \\\n                plottype == \"grid_seasonal_absolute_period_stdev\" or plottype == \"grid_seasonal_period_stdev\":\n            cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title() + ' ({})'.format('years'),\n                              rotation=-90, labelpad=10)\n        # gridmap of station density has no units\n        else:\n            cbar_ax.set_label(\" \".join(plottype.replace('grid_', '').split('_')).title(), rotation=-90, labelpad=10)\n\n    # Add title to plots, if specified\n    if userTitle:\n        axes.set_title(userTitle, zorder=2)\n\n    # save/close figure\n    # cbar_ax.ax.locator_params(nbins=10)\n    # for label in cbar_ax.ax.xaxis.get_ticklabels()[::25]:\n        # label.set_visible(False)\n    plt.savefig(os.path.join(workdir, self.col_name + '_' + plottype + '.' + plotFormat),\n                format=plotFormat, bbox_inches='tight')\n    plt.close()\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.RaiderStats.create_DF","title":"<code>create_DF()</code>","text":"<p>Create dataframe.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def create_DF(self):\n'''\n        Create dataframe.\n    '''\n    # Open file\n    self.df = self._reader()\n\n    # Filter dataframe\n    # drop all nans\n    self.df.dropna(how='any', inplace=True)\n    self.df.reset_index(drop=True, inplace=True)\n    # convert to datetime object\n\n    # time-interval filter\n    if self.timeinterval:\n        self.timeinterval = [dt.datetime.strptime(\n            val, '%Y-%m-%d') for val in self.timeinterval.split()]\n        self.df = self.df[(self.df['Date'] &gt;= self.timeinterval[0]) &amp; (\n            self.df['Date'] &lt;= self.timeinterval[-1])]\n\n    # seasonal filter\n    if self.seasonalinterval:\n        self.seasonalinterval = self.seasonalinterval.split()\n        # get day of year\n        self.seasonalinterval = [dt.datetime.strptime('2001-' + self.seasonalinterval[0], '%Y-%m-%d').timetuple(\n        ).tm_yday, dt.datetime.strptime('2001-' + self.seasonalinterval[-1], '%Y-%m-%d').timetuple().tm_yday]\n        # track input order and wrap around year if necessary\n        # e.g. month/day: 03/01 to 06/01\n        if self.seasonalinterval[0] &lt; self.seasonalinterval[1]:\n            # non leap-year\n            filtered_self = self.df[(self.df['Date'].dt.is_leap_year == False) &amp; (\n                self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[0]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[-1])]\n            # leap-year\n            self.seasonalinterval = [i + 1 if i &gt;\n                                     59 else i for i in self.seasonalinterval]\n            filtered_self_ly = self.df[(self.df['Date'].dt.is_leap_year == True) &amp; (\n                self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[0]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[-1])]\n            self.df = pd.concat([filtered_self, filtered_self_ly], ignore_index=True)\n            del filtered_self\n        # e.g. month/day: 12/01 to 03/01\n        if self.seasonalinterval[0] &gt; self.seasonalinterval[1]:\n            # non leap-year\n            filtered_self = self.df[(self.df['Date'].dt.is_leap_year == False) &amp; (\n                self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[-1]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[0])]\n            # leap-year\n            self.seasonalinterval = [i + 1 if i &gt;\n                                     59 else i for i in self.seasonalinterval]\n            filtered_self_ly = self.df[(self.df['Date'].dt.is_leap_year == True) &amp; (\n                self.df['Date'].dt.dayofyear &gt;= self.seasonalinterval[-1]) &amp; (self.df['Date'].dt.dayofyear &lt;= self.seasonalinterval[0])]\n            self.df = pd.concat([filtered_self, filtered_self_ly], ignore_index=True)\n            del filtered_self\n\n    # estimate central longitude lines if '--time_lines' specified\n    if self.time_lines and 'Datetime' in self.df.keys():\n        self.df['Date_hr'] = self.df['Datetime'].dt.hour.astype(float).astype(\"Int32\")\n        # get list of unique times\n        all_hrs = sorted(set(self.df['Date_hr']))\n\n        # get central longitude bands associated with each time\n        central_points = []\n        # if single time, avoid loop\n        if len(all_hrs) == 1:\n            central_points.append(([0, max(self.df['Lon'])],\n                                   [0, min(self.df['Lon'])]))\n        else:\n            for i in enumerate(all_hrs):\n                # last entry\n                if i[0] == len(all_hrs) - 1:\n                    lons = self.df[self.df['Date_hr'] &gt; all_hrs[i[0] - 1]]\n                # first entry\n                elif i[0] == 0:\n                    lons = self.df[self.df['Date_hr'] &lt; all_hrs[i[0] + 1]]\n                else:\n                    lons = self.df[(self.df['Date_hr'] &gt; all_hrs[i[0] - 1])\n                                   &amp; (self.df['Date_hr'] &lt; all_hrs[i[0] + 1])]\n                central_points.append(([0, max(lons['Lon'])],\n                                       [0, min(lons['Lon'])]))\n        # get central longitudes\n        self.time_lines = [midpoint(i[0], i[1]) for i in central_points]\n\n    # Get bbox, buffered by grid spacing.\n    # Check if bbox input is valid list.\n    if self.bbox is not None:\n        try:\n            self.bbox = [float(val) for val in self.bbox.split()]\n        except BaseException:\n            raise Exception(\n                'Cannot understand the --bounding_box argument. String input is incorrect or path does not exist.')\n    self.plotbbox, self.grid_dim, self.gridpoints = self._get_extent()\n\n    # generate list of grid-polygons\n    append_poly = []\n    for i in self.gridpoints:\n        bbox = [i[1] - (self.spacing / 2), i[1] + (self.spacing / 2),\n                i[0] - (self.spacing / 2), i[0] + (self.spacing / 2)]\n        append_poly.append(Polygon(np.column_stack((np.array([bbox[2], bbox[3], bbox[3], bbox[2], bbox[2]]),\n                                                    np.array([bbox[0], bbox[0], bbox[1], bbox[1], bbox[0]])))))  # Pass lons/lats to create polygon\n\n    # Check for grid cell intersection with each station\n    idtogrid_dict = {}\n    self.unique_points = self.df.groupby(['ID', 'Lon', 'Lat']).size()\n    self.unique_points = [self.unique_points.index.get_level_values('ID').tolist(), self.unique_points.index.get_level_values(\n        'Lon').tolist(), self.unique_points.index.get_level_values('Lat').tolist()]\n    # Initiate R-tree of gridded array domain\n    self.polygon_tree = STRtree(append_poly)\n    for stat_ID in self.unique_points[0]:\n        grd_index = self._check_stationgrid_intersection(stat_ID)\n        idtogrid_dict[stat_ID] = grd_index\n\n    # map gridnode dictionary to dataframe\n    self.df['gridnode'] = self.df['ID'].map(idtogrid_dict)\n    self.df = self.df[self.df['gridnode'].astype(str) != 'NaN']\n    del self.unique_points, self.polygon_tree, idtogrid_dict, append_poly\n    # sort by grid and date\n    self.df.sort_values(['gridnode', 'Date'])\n\n    # If specified, pass station locations to superimpose on gridplots\n    if self.stationsongrids:\n        unique_points = self.df.groupby(['Lon', 'Lat']).size()\n        self.stationsongrids = [unique_points.index.get_level_values(\n            'Lon').tolist(), unique_points.index.get_level_values('Lat').tolist()]\n\n    # If specified, setup gridded array(s)\n    if self.grid_heatmap:\n        self.grid_heatmap = np.array([np.nan if i[0] not in self.df['gridnode'].values[:] else int(len(np.unique(\n            self.df['ID'][self.df['gridnode'] == i[0]]))) for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_heatmap' + '.tif')\n            save_gridfile(self.grid_heatmap, 'grid_heatmap', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%1i',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='int16',\n                          noData=0)\n\n    if self.grid_delay_mean:\n        # Take mean of station-wise means per gridcell\n        unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].mean()\n        unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_mean = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_mean' + '.tif')\n            save_gridfile(self.grid_delay_mean, 'grid_delay_mean', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    if self.grid_delay_median:\n        # Take mean of station-wise medians per gridcell\n        unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].median()\n        unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_median = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_median' + '.tif')\n            save_gridfile(self.grid_delay_median, 'grid_delay_median', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    if self.grid_delay_stdev:\n        # Take mean of station-wise stdev per gridcell\n        unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)[self.col_name].std()\n        unique_points = unique_points.groupby(['gridnode'])[self.col_name].mean()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_stdev' + '.tif')\n            save_gridfile(self.grid_delay_stdev, 'grid_delay_stdev', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    if self.grid_delay_absolute_mean:\n        # Take mean of all data per gridcell\n        unique_points = self.df.groupby(['gridnode'])[self.col_name].mean()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_absolute_mean = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_mean' + '.tif')\n            save_gridfile(self.grid_delay_absolute_mean, 'grid_delay_absolute_mean', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    if self.grid_delay_absolute_median:\n        # Take median of all data per gridcell\n        unique_points = self.df.groupby(['gridnode'])[self.col_name].median()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_absolute_median = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_median' + '.tif')\n            save_gridfile(self.grid_delay_absolute_median, 'grid_delay_absolute_median', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    if self.grid_delay_absolute_stdev:\n        # Take stdev of all data per gridcell\n        unique_points = self.df.groupby(['gridnode'])[self.col_name].std()\n        unique_points.dropna(how='any', inplace=True)\n        self.grid_delay_absolute_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n        ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n        # If specified, save gridded array(s)\n        if self.grid_to_raster:\n            gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_delay_absolute_stdev' + '.tif')\n            save_gridfile(self.grid_delay_absolute_stdev, 'grid_delay_absolute_stdev', gridfile_name, self.plotbbox, self.spacing,\n                          self.unit, colorbarfmt='%.2f',\n                          stationsongrids=self.stationsongrids,\n                          time_lines=self.time_lines, dtype='float32')\n\n    # If specified, compute phase/amplitude fits\n    if self.station_seasonal_phase or self.grid_seasonal_phase or self.grid_seasonal_absolute_phase:\n        # Sort by coordinates\n        unique_points = self.df.sort_values(['ID', 'Date'])\n        unique_points['Date'] = [i.timestamp() for i in unique_points['Date']]\n        # Setup variables\n        self.ampfit = []\n        self.phsfit = []\n        self.periodfit = []\n        self.ampfit_c = []\n        self.phsfit_c = []\n        self.periodfit_c = []\n        self.seasonalfit_rmse = []\n        args = []\n        for i in sorted(list(set(unique_points['ID']))):\n            # pass all values corresponding to station (ID, data = y, time = x)\n            args.append((i, unique_points[unique_points['ID'] == i]['Date'].to_list(), unique_points[unique_points['ID'] == i][self.col_name].to_list(), self.min_span[0], self.min_span[1], self.period_limit))\n        # Parallelize iteration through all grid-cells and time slices\n        with multiprocessing.Pool(self.numCPUs) as multipool:\n            for i, j, k, l, m, n, o in multipool.starmap(self._amplitude_and_phase, args):\n                self.ampfit.extend(i)\n                self.phsfit.extend(j)\n                self.periodfit.extend(k)\n                self.ampfit_c.extend(l)\n                self.phsfit_c.extend(m)\n                self.periodfit_c.extend(n)\n                self.seasonalfit_rmse.extend(o)\n        # map phase/amplitude fits dictionary to dataframe\n        self.phsfit = {k: v for d in self.phsfit for k, v in d.items()}\n        self.ampfit = {k: v for d in self.ampfit for k, v in d.items()}\n        self.periodfit = {k: v for d in self.periodfit for k, v in d.items()}\n        self.df['phsfit'] = self.df['ID'].map(self.phsfit)\n        # check if there are any valid data values\n        if self.df['phsfit'].isnull().values.all(axis=0):\n            raise Exception(\"No valid data values, adjust --min_span inputs for time span in years {} and/or fractional obs. {}\".\n                            format(self.min_span[0], self.min_span[1]))\n        self.df['ampfit'] = self.df['ID'].map(self.ampfit)\n        self.df['periodfit'] = self.df['ID'].map(self.periodfit)\n        self.phsfit_c = {k: v for d in self.phsfit_c for k, v in d.items()}\n        self.ampfit_c = {k: v for d in self.ampfit_c for k, v in d.items()}\n        self.periodfit_c = {k: v for d in self.periodfit_c for k, v in d.items()}\n        self.seasonalfit_rmse = {k: v for d in self.seasonalfit_rmse for k, v in d.items()}\n        self.df['phsfit_c'] = self.df['ID'].map(self.phsfit_c)\n        self.df['ampfit_c'] = self.df['ID'].map(self.ampfit_c)\n        self.df['periodfit_c'] = self.df['ID'].map(self.periodfit_c)\n        self.df['seasonalfit_rmse'] = self.df['ID'].map(self.seasonalfit_rmse)\n        # drop nan\n        self.df.dropna(how='any', inplace=True)\n        # If grid plots specified\n        if self.grid_seasonal_phase:\n            # Pass mean phase of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['phsfit'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['phsfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_phase = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_phase' + '.tif')\n                save_gridfile(self.grid_seasonal_phase, 'grid_seasonal_phase', gridfile_name, self.plotbbox, self.spacing,\n                              'days', colorbarfmt='%.1i',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass mean amplitude of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['ampfit'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['ampfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_amplitude = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_amplitude' + '.tif')\n                save_gridfile(self.grid_seasonal_amplitude, 'grid_seasonal_amplitude', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.3f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass mean period of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['periodfit'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['periodfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_period = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_period' + '.tif')\n                save_gridfile(self.grid_seasonal_period, 'grid_seasonal_period', gridfile_name, self.plotbbox, self.spacing,\n                              'years', colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            ########################################################################################################################\n            # Pass mean phase stdev of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['phsfit_c'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['phsfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_phase_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_phase_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_phase_stdev, 'grid_seasonal_phase_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              'days', colorbarfmt='%.1i',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass mean amplitude stdev of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['ampfit_c'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['ampfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_amplitude_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_amplitude_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_amplitude_stdev, 'grid_seasonal_amplitude_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.3f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass mean period stdev of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['periodfit_c'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['periodfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_period_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_period_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_period_stdev, 'grid_seasonal_period_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              'years', colorbarfmt='%.2e',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass mean seasonal fit RMSE of station-wise means per gridcell\n            unique_points = self.df.groupby(['ID', 'Lon', 'Lat', 'gridnode'], as_index=False)['seasonalfit_rmse'].mean()\n            unique_points = unique_points.groupby(['gridnode'])['seasonalfit_rmse'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_fit_rmse = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_fit_rmse' + '.tif')\n                save_gridfile(self.grid_seasonal_fit_rmse, 'grid_seasonal_fit_rmse', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.3f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n        ########################################################################################################################\n        if self.grid_seasonal_absolute_phase:\n            # Pass absolute mean phase of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['phsfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_phase = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_phase' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_phase, 'grid_seasonal_absolute_phase', gridfile_name, self.plotbbox, self.spacing,\n                              'days', colorbarfmt='%.1i',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass absolute mean amplitude of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['ampfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_amplitude = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_amplitude' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_amplitude, 'grid_seasonal_absolute_amplitude', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.3f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass absolute mean period of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['periodfit'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_period = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_period' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_period, 'grid_seasonal_absolute_period', gridfile_name, self.plotbbox, self.spacing,\n                              'years', colorbarfmt='%.2f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            ########################################################################################################################\n            # Pass absolute mean phase stdev of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['phsfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_phase_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_phase_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_phase_stdev, 'grid_seasonal_absolute_phase_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              'days', colorbarfmt='%.1i',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass absolute mean amplitude stdev of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['ampfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_amplitude_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_amplitude_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_amplitude_stdev, 'grid_seasonal_absolute_amplitude_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.3f',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n            # Pass absolute mean period stdev of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['periodfit_c'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_period_stdev = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_period_stdev' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_period_stdev, 'grid_seasonal_absolute_period_stdev', gridfile_name, self.plotbbox, self.spacing,\n                              'years', colorbarfmt='%.2e',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n\n            # Pass absolute mean seasonal fit RMSE of all data per gridcell\n            unique_points = self.df.groupby(['gridnode'])['seasonalfit_rmse'].mean()\n            unique_points.dropna(how='any', inplace=True)\n            self.grid_seasonal_absolute_fit_rmse = np.array([np.nan if i[0] not in unique_points.index.get_level_values('gridnode').tolist(\n            ) else unique_points[i[0]] for i in enumerate(self.gridpoints)]).reshape(self.grid_dim).T\n            # If specified, save gridded array(s)\n            if self.grid_to_raster:\n                gridfile_name = os.path.join(self.workdir, self.col_name + '_' + 'grid_seasonal_absolute_fit_rmse' + '.tif')\n                save_gridfile(self.grid_seasonal_absolute_fit_rmse, 'grid_seasonal_absolute_fit_rmse', gridfile_name, self.plotbbox, self.spacing,\n                              self.unit, colorbarfmt='%.2e',\n                              stationsongrids=self.stationsongrids,\n                              time_lines=self.time_lines, dtype='float32')\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis","title":"<code>VariogramAnalysis</code>","text":"<p>Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>class VariogramAnalysis():\n'''\n        Class which ingests dataframe output from 'RaiderStats' class and performs variogram analysis.\n    '''\n\n    def __init__(self, filearg, gridpoints, col_name, unit='m', workdir='./', seasonalinterval=None, densitythreshold=10, binnedvariogram=False, numCPUs=8, variogram_per_timeslice=False, variogram_errlimit='inf'):\n        self.df = filearg\n        self.col_name = col_name\n        self.unit = unit\n        self.gridpoints = gridpoints\n        self.workdir = workdir\n        self.seasonalinterval = seasonalinterval\n        self.densitythreshold = densitythreshold\n        self.binnedvariogram = binnedvariogram\n        self.numCPUs = numCPUs\n        self.variogram_per_timeslice = variogram_per_timeslice\n        self.variogram_errlimit = float(variogram_errlimit)\n\n    def _get_samples(self, data, Nsamp=1000):\n'''\n        pull samples from a 2D image for variogram analysis\n        '''\n        import random\n        if len(data) &lt; self.densitythreshold:\n            logger.warning('Less than {} points for this gridcell', self.densitythreshold)\n            logger.info('Will pass empty list')\n            d = []\n            indpars = []\n        else:\n            indpars = list(itertools.combinations(range(len(data)), 2))\n            random.shuffle(indpars)\n            # subsample\n            Nvalidsamp = int(len(data) * (len(data) - 1) / 2)\n            # Only downsample if Nsamps&gt;specified value\n            if Nvalidsamp &gt; Nsamp:\n                indpars = indpars[:Nsamp]\n            d = np.array([[data[r[0]], data[r[1]]] for r in indpars])\n\n        return d, indpars\n\n    def _get_XY(self, x2d, y2d, indpars):\n'''\n        Given a list of indices, return the x,y locations\n        from two matrices\n        '''\n        x = np.array([[x2d[r[0]], x2d[r[1]]] for r in indpars])\n        y = np.array([[y2d[r[0]], y2d[r[1]]] for r in indpars])\n\n        return x, y\n\n    def _get_distances(self, XY):\n'''\n        Return the distances between each point in a list of points\n        '''\n        from scipy.spatial.distance import cdist\n        return np.diag(cdist(XY[:, :, 0], XY[:, :, 1], metric='euclidean'))\n\n    def _get_variogram(self, XY, xy=None):\n'''\n        Return variograms\n        '''\n        return 0.5 * np.square(XY - xy)  # XY = 1st col xy= 2nd col\n\n    def _emp_vario(self, x, y, data, Nsamp=1000):\n'''\n        Compute empirical semivariance\n        '''\n        # remove NaNs if possible\n        mask = ~np.isnan(data)\n        if False in mask:\n            data = data[mask]\n            x = x[mask]\n            y = y[mask]\n\n        # deramp\n        temp1, temp2, x, y = WGS84_to_UTM(x, y, common_center=True)\n        A = np.array([x, y, np.ones(len(x))]).T\n        ramp = np.linalg.lstsq(A, data.T, rcond=None)[0]\n        data = data - (np.matmul(A, ramp))\n\n        samples, indpars = self._get_samples(data, Nsamp)\n        x, y = self._get_XY(x, y, indpars)\n        dists = self._get_distances(\n            np.array([[x[:, 0], y[:, 0]], [x[:, 1], y[:, 1]]]).T)\n        vario = self._get_variogram(samples[:, 0], samples[:, 1])\n\n        return dists, vario\n\n    def _binned_vario(self, hEff, rawVario, xBin=None):\n'''\n        return a binned empirical variogram\n        '''\n        if xBin is None:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n                xBin = np.linspace(0, np.nanmax(hEff) * .67, 20)\n\n        nBins = len(xBin) - 1\n        hExp, expVario = [], []\n\n        for iBin in range(nBins):\n            iBinMask = np.logical_and(xBin[iBin] &lt; hEff, hEff &lt;= xBin[iBin + 1])\n            # circumvent indexing\n            try:\n                with warnings.catch_warnings():\n                    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n                    hExp.append(np.nanmean(hEff[iBinMask]))\n                    expVario.append(np.nanmean(rawVario[iBinMask]))\n            except BaseException:  # TODO: Which error(s)?\n                pass\n\n        if False in ~np.isnan(hExp):\n            # NaNs present in binned histogram\n            hExp = [x for x in hExp if str(x) != 'nan']\n            expVario = [x for x in expVario if str(x) != 'nan']\n\n        return np.array(hExp), np.array(expVario)\n\n    def _fit_vario(self, dists, vario, model=None, x0=None, Nparm=None, ub=None):\n'''\n        Fit a variogram model to data\n        '''\n        from scipy.optimize import least_squares\n\n        def resid(x, d, v, m):\n            return (m(x, d) - v)\n\n        if ub is None:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n                ub = np.array([np.nanmax(dists) * 0.8, np.nanmax(vario)\n                               * 0.8, np.nanmax(vario) * 0.8])\n\n        if x0 is None and Nparm is None:\n            raise RuntimeError(\n                'Must specify either x0 or the number of model parameters')\n        if x0 is not None:\n            lb = np.zeros(len(x0))\n        if Nparm is not None:\n            lb = np.zeros(Nparm)\n            x0 = (ub - lb) / 2\n        bounds = (lb, ub)\n\n        mask = np.isnan(dists) | np.isnan(vario)\n        d = dists[~mask].copy()\n        v = vario[~mask].copy()\n\n        res_robust = least_squares(resid, x0, bounds=bounds,\n                                   loss='soft_l1', f_scale=0.1,\n                                   args=(d, v, model))\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n            d_test = np.linspace(0, np.nanmax(dists), 100)\n        # v_test is my y., # res_robust.x =a, b, c, where a = range, b = sill, and c = nugget model, d_test=x\n        v_test = model(res_robust.x, d_test)\n\n        return res_robust, d_test, v_test\n\n    # this would be expontential plus nugget\n    def __exponential__(self, parms, h, nugget=False):\n'''\n        returns a variogram model given a set of arguments and\n        key-word arguments\n        '''\n        # a = range, b = sill, c = nugget model\n        a, b, c = parms\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"overflow encountered in true_divide\")\n            if nugget:\n                return b * (1 - np.exp(-h / a)) + c\n            else:\n                return b * (1 - np.exp(-h / a))\n\n    # this would be gaussian plus nugget\n    def __gaussian__(self, parms, h):\n'''\n        returns a Gaussian variogram model\n        '''\n        a, b, c = parms\n        return b * (1 - np.exp(-np.square(h) / (a**2))) + c\n\n    def _append_variogram(self, grid_ind, grid_subset):\n'''\n        For a given grid-cell, iterate through time slices to generate/append empirical variogram(s)\n        '''\n        # Comprehensive arrays recording data across all time epochs for given station\n        dists_arr = []\n        vario_arr = []\n        dists_binned_arr = []\n        vario_binned_arr = []\n        res_robust_arr = []\n        d_test_arr = []\n        v_test_arr = []\n        for j in sorted(list(set(grid_subset['Date']))):\n            # If insufficient sample size, skip slice and record occurence\n            if len(np.array(grid_subset[grid_subset['Date'] == j][self.col_name])) &lt; self.densitythreshold:\n                # Record skipped [gridnode, timeslice]\n                self.skipped_slices.append([grid_ind, j.strftime(\"%Y-%m-%d\")])\n            else:\n                self.gridcenterlist.append(['grid{} '.format(\n                    grid_ind) + 'Lat:{} Lon:{}'.format(\n                    str(self.gridpoints[grid_ind][1]), str(self.gridpoints[grid_ind][0]))])\n                lonarr = np.array(\n                    grid_subset[grid_subset['Date'] == j]['Lon'])\n                latarr = np.array(\n                    grid_subset[grid_subset['Date'] == j]['Lat'])\n                delayarray = np.array(\n                    grid_subset[grid_subset['Date'] == j][self.col_name])\n                # fit empirical variogram for each time AND grid\n                dists, vario = self._emp_vario(lonarr, latarr, delayarray)\n                dists_binned, vario_binned = self._binned_vario(\n                    dists, vario)\n                # fit experimental variogram for each time AND grid, model default is exponential\n                res_robust, d_test, v_test = self._fit_vario(\n                    dists_binned, vario_binned, model=self.__exponential__, x0=None, Nparm=3)\n                # Plot empirical + experimental variogram for this gridnode and timeslice\n                if not os.path.exists(os.path.join(self.workdir, 'variograms/grid{}'.format(grid_ind))):\n                    os.makedirs(os.path.join(\n                        self.workdir, 'variograms/grid{}'.format(grid_ind)))\n                # Make variogram plots for each time-slice\n                if self.variogram_per_timeslice:\n                    # Plot empirical variogram for this gridnode and timeslice\n                    self.plot_variogram(grid_ind, j.strftime(\"%Y%m%d\"), [self.gridpoints[grid_ind][1], self.gridpoints[grid_ind][0]],\n                                        workdir=os.path.join(self.workdir, 'variograms/grid{}'.format(grid_ind)), dists=dists, vario=vario,\n                                        dists_binned=dists_binned, vario_binned=vario_binned)\n                    # Plot experimental variogram for this gridnode and timeslice\n                    self.plot_variogram(grid_ind, j.strftime(\"%Y%m%d\"), [self.gridpoints[grid_ind][1], self.gridpoints[grid_ind][0]],\n                                        workdir=os.path.join(self.workdir, 'variograms/grid{}'.format(grid_ind)), d_test=d_test, v_test=v_test,\n                                        res_robust=res_robust.x, dists_binned=dists_binned, vario_binned=vario_binned)\n                # append for plotting\n                self.good_slices.append([grid_ind, j.strftime(\"%Y%m%d\")])\n                dists_arr.append(dists)\n                vario_arr.append(vario)\n                dists_binned_arr.append(dists_binned)\n                vario_binned_arr.append(vario_binned)\n                res_robust_arr.append(res_robust.x)\n                d_test_arr.append(d_test)\n                v_test_arr.append(v_test)\n        # fit experimental variogram for each grid\n        if dists_binned_arr != []:\n            # TODO: need to change this from accumulating binned data to raw data\n            dists_arr = np.concatenate(dists_arr).ravel()\n            vario_arr = np.concatenate(vario_arr).ravel()\n            # if specified, passed binned empirical variograms\n            if self.binnedvariogram:\n                dists_binned_arr = np.concatenate(dists_binned_arr).ravel()\n                vario_binned_arr = np.concatenate(vario_binned_arr).ravel()\n            else:\n                # dists_binned_arr = dists_arr ; vario_binned_arr = vario_arr\n                dists_binned_arr, vario_binned_arr = self._binned_vario(\n                    dists_arr, vario_arr)\n            TOT_res_robust, TOT_d_test, TOT_v_test = self._fit_vario(\n                dists_binned_arr, vario_binned_arr, model=self.__exponential__, x0=None, Nparm=3)\n            tot_timetag = self.good_slices[0][1] + '\u2013' + self.good_slices[-1][1]\n            # Append TOT arrays\n            self.TOT_good_slices.append([grid_ind, tot_timetag])\n            self.TOT_res_robust_arr.append(TOT_res_robust.x)\n            self.TOT_tot_timetag.append(tot_timetag)\n            var_rmse = np.sqrt(np.nanmean((TOT_res_robust.fun)**2))\n            if var_rmse &lt;= self.variogram_errlimit:\n                self.TOT_res_robust_rmse.append(var_rmse)\n            else:\n                self.TOT_res_robust_rmse.append(np.array(np.nan))\n            # Plot empirical variogram for this gridnode\n            self.plot_variogram(grid_ind, tot_timetag, [self.gridpoints[grid_ind][1], self.gridpoints[grid_ind][0]],\n                                workdir=os.path.join(self.workdir, 'variograms/grid{}'.format(grid_ind)), dists=dists_arr, vario=vario_arr,\n                                dists_binned=dists_binned_arr, vario_binned=vario_binned_arr, seasonalinterval=self.seasonalinterval)\n            # Plot experimental variogram for this gridnode\n            self.plot_variogram(grid_ind, tot_timetag, [self.gridpoints[grid_ind][1], self.gridpoints[grid_ind][0]],\n                                workdir=os.path.join(self.workdir, 'variograms/grid{}'.format(grid_ind)), d_test=TOT_d_test, v_test=TOT_v_test,\n                                res_robust=TOT_res_robust.x, seasonalinterval=self.seasonalinterval, dists_binned=dists_binned_arr, vario_binned=vario_binned_arr)\n        # Record sparse grids which didn't have sufficient sample size of data through any of the timeslices\n        else:\n            self.sparse_grids.append(grid_ind)\n\n        return self.TOT_good_slices, self.TOT_res_robust_arr, self.TOT_res_robust_rmse, self.gridcenterlist\n\n    def create_variograms(self):\n'''\n        Iterate through grid-cells and time slices to generate empirical variogram(s)\n        '''\n        # track data for plotting\n        self.TOT_good_slices = []\n        self.TOT_res_robust_arr = []\n        self.TOT_res_robust_rmse = []\n        self.TOT_tot_timetag = []\n        # track pass/rejected grids\n        self.sparse_grids = []\n        self.good_slices = []\n        self.skipped_slices = []\n        # record grid-centers for lookup-table\n        self.gridcenterlist = []\n        args = []\n        for i in sorted(list(set(self.df['gridnode']))):\n            # pass subset of all stations corresponding to given grid-cell\n            grid_subset = self.df[self.df['gridnode'] == i]\n            args.append((i, grid_subset))\n        # Parallelize iteration through all grid-cells and time slices\n        with multiprocessing.Pool(self.numCPUs) as multipool:\n            for i, j, k, l in multipool.starmap(self._append_variogram, args):\n                self.TOT_good_slices.extend(i)\n                self.TOT_res_robust_arr.extend(j)\n                self.TOT_res_robust_rmse.extend(k)\n                self.gridcenterlist.extend(l)\n\n        # save grid-center lookup table\n        self.gridcenterlist = [list(i) for i in set(tuple(j)\n                                                    for j in self.gridcenterlist)]\n        self.gridcenterlist.sort(key=lambda x: int(x[0][4:6]))\n        gridcenter = open(\n            (os.path.join(self.workdir, 'variograms/gridlocation_lookup.txt')), \"w\")\n        for element in self.gridcenterlist:\n            gridcenter.writelines(\"\\n\".join(element))\n            gridcenter.write(\"\\n\")\n        gridcenter.close()\n\n        TOT_grids = [i[0] for i in self.TOT_good_slices]\n\n        return TOT_grids, self.TOT_res_robust_arr, self.TOT_res_robust_rmse\n\n    def plot_variogram(self, gridID, timeslice, coords, workdir='./', d_test=None, v_test=None, res_robust=None, dists=None, vario=None, dists_binned=None, vario_binned=None, seasonalinterval=None):\n'''\n        Make empirical and/or experimental variogram fit plots\n        '''\n        # If specified workdir doesn't exist, create it\n        if not os.path.exists(workdir):\n            os.mkdir(workdir)\n\n        # make plot title\n        title_str = ' \\nLat:{:.2f} Lon:{:.2f}\\nTime:{}'.format(\n            coords[1], coords[0], str(timeslice))\n        if seasonalinterval:\n            title_str += ' Season(mm/dd): {}/{} \u2013 {}/{}'.format(int(timeslice[4:6]), int(\n                timeslice[6:8]), int(timeslice[-4:-2]), int(timeslice[-2:]))\n\n        if dists is not None and vario is not None:\n            # scale from m to user-defined units\n            dists = [convert_SI(i, 'm', self.unit) for i in dists]\n            plt.scatter(dists, vario, s=1, facecolor='0.5', label='raw')\n        if dists_binned is not None and vario_binned is not None:\n            # scale from m to user-defined units\n            dists_binned = [convert_SI(i, 'm', self.unit) for i in dists_binned]\n            plt.plot(dists_binned, vario_binned, 'bo', label='binned')\n        if res_robust is not None:\n            plt.axhline(y=res_robust[1], color='g',\n                        linestyle='--', label='\u0263\\u0332\\u00b2({}\\u00b2)'.format(self.unit))\n            # scale from m to user-defined units\n            res_robust[0] = convert_SI(res_robust[0], 'm', self.unit)\n            plt.axvline(x=res_robust[0], color='c',\n                        linestyle='--', label='h ({})'.format(self.unit))\n        if d_test is not None and v_test is not None:\n            # scale from m to user-defined units\n            d_test = [convert_SI(i, 'm', self.unit) for i in d_test]\n            plt.plot(d_test, v_test, 'r-', label='experimental fit')\n        plt.xlabel('Distance ({})'.format(self.unit))\n        plt.ylabel('Dissimilarity ({}\\u00b2)'.format(self.unit))\n        plt.legend(bbox_to_anchor=(1.02, 1),\n                   loc='upper left', borderaxespad=0., framealpha=1.)\n        # Plot empirical variogram\n        if d_test is None and v_test is None:\n            plt.title('Empirical variogram' + title_str)\n            plt.tight_layout()\n            plt.savefig(os.path.join(\n                workdir, 'grid{}_timeslice{}_justEMPvariogram.eps'.format(gridID, timeslice)))\n        # Plot just experimental variogram\n        else:\n            plt.title('Experimental variogram' + title_str)\n            plt.tight_layout()\n            plt.savefig(os.path.join(\n                workdir, 'grid{}_timeslice{}_justEXPvariogram.eps'.format(gridID, timeslice)))\n        plt.close()\n\n        return\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.__exponential__","title":"<code>__exponential__(parms, h, nugget=False)</code>","text":"<p>returns a variogram model given a set of arguments and key-word arguments</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def __exponential__(self, parms, h, nugget=False):\n'''\n    returns a variogram model given a set of arguments and\n    key-word arguments\n    '''\n    # a = range, b = sill, c = nugget model\n    a, b, c = parms\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"overflow encountered in true_divide\")\n        if nugget:\n            return b * (1 - np.exp(-h / a)) + c\n        else:\n            return b * (1 - np.exp(-h / a))\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.__gaussian__","title":"<code>__gaussian__(parms, h)</code>","text":"<p>returns a Gaussian variogram model</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def __gaussian__(self, parms, h):\n'''\n    returns a Gaussian variogram model\n    '''\n    a, b, c = parms\n    return b * (1 - np.exp(-np.square(h) / (a**2))) + c\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.create_variograms","title":"<code>create_variograms()</code>","text":"<p>Iterate through grid-cells and time slices to generate empirical variogram(s)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def create_variograms(self):\n'''\n    Iterate through grid-cells and time slices to generate empirical variogram(s)\n    '''\n    # track data for plotting\n    self.TOT_good_slices = []\n    self.TOT_res_robust_arr = []\n    self.TOT_res_robust_rmse = []\n    self.TOT_tot_timetag = []\n    # track pass/rejected grids\n    self.sparse_grids = []\n    self.good_slices = []\n    self.skipped_slices = []\n    # record grid-centers for lookup-table\n    self.gridcenterlist = []\n    args = []\n    for i in sorted(list(set(self.df['gridnode']))):\n        # pass subset of all stations corresponding to given grid-cell\n        grid_subset = self.df[self.df['gridnode'] == i]\n        args.append((i, grid_subset))\n    # Parallelize iteration through all grid-cells and time slices\n    with multiprocessing.Pool(self.numCPUs) as multipool:\n        for i, j, k, l in multipool.starmap(self._append_variogram, args):\n            self.TOT_good_slices.extend(i)\n            self.TOT_res_robust_arr.extend(j)\n            self.TOT_res_robust_rmse.extend(k)\n            self.gridcenterlist.extend(l)\n\n    # save grid-center lookup table\n    self.gridcenterlist = [list(i) for i in set(tuple(j)\n                                                for j in self.gridcenterlist)]\n    self.gridcenterlist.sort(key=lambda x: int(x[0][4:6]))\n    gridcenter = open(\n        (os.path.join(self.workdir, 'variograms/gridlocation_lookup.txt')), \"w\")\n    for element in self.gridcenterlist:\n        gridcenter.writelines(\"\\n\".join(element))\n        gridcenter.write(\"\\n\")\n    gridcenter.close()\n\n    TOT_grids = [i[0] for i in self.TOT_good_slices]\n\n    return TOT_grids, self.TOT_res_robust_arr, self.TOT_res_robust_rmse\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.VariogramAnalysis.plot_variogram","title":"<code>plot_variogram(gridID, timeslice, coords, workdir='./', d_test=None, v_test=None, res_robust=None, dists=None, vario=None, dists_binned=None, vario_binned=None, seasonalinterval=None)</code>","text":"<p>Make empirical and/or experimental variogram fit plots</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def plot_variogram(self, gridID, timeslice, coords, workdir='./', d_test=None, v_test=None, res_robust=None, dists=None, vario=None, dists_binned=None, vario_binned=None, seasonalinterval=None):\n'''\n    Make empirical and/or experimental variogram fit plots\n    '''\n    # If specified workdir doesn't exist, create it\n    if not os.path.exists(workdir):\n        os.mkdir(workdir)\n\n    # make plot title\n    title_str = ' \\nLat:{:.2f} Lon:{:.2f}\\nTime:{}'.format(\n        coords[1], coords[0], str(timeslice))\n    if seasonalinterval:\n        title_str += ' Season(mm/dd): {}/{} \u2013 {}/{}'.format(int(timeslice[4:6]), int(\n            timeslice[6:8]), int(timeslice[-4:-2]), int(timeslice[-2:]))\n\n    if dists is not None and vario is not None:\n        # scale from m to user-defined units\n        dists = [convert_SI(i, 'm', self.unit) for i in dists]\n        plt.scatter(dists, vario, s=1, facecolor='0.5', label='raw')\n    if dists_binned is not None and vario_binned is not None:\n        # scale from m to user-defined units\n        dists_binned = [convert_SI(i, 'm', self.unit) for i in dists_binned]\n        plt.plot(dists_binned, vario_binned, 'bo', label='binned')\n    if res_robust is not None:\n        plt.axhline(y=res_robust[1], color='g',\n                    linestyle='--', label='\u0263\\u0332\\u00b2({}\\u00b2)'.format(self.unit))\n        # scale from m to user-defined units\n        res_robust[0] = convert_SI(res_robust[0], 'm', self.unit)\n        plt.axvline(x=res_robust[0], color='c',\n                    linestyle='--', label='h ({})'.format(self.unit))\n    if d_test is not None and v_test is not None:\n        # scale from m to user-defined units\n        d_test = [convert_SI(i, 'm', self.unit) for i in d_test]\n        plt.plot(d_test, v_test, 'r-', label='experimental fit')\n    plt.xlabel('Distance ({})'.format(self.unit))\n    plt.ylabel('Dissimilarity ({}\\u00b2)'.format(self.unit))\n    plt.legend(bbox_to_anchor=(1.02, 1),\n               loc='upper left', borderaxespad=0., framealpha=1.)\n    # Plot empirical variogram\n    if d_test is None and v_test is None:\n        plt.title('Empirical variogram' + title_str)\n        plt.tight_layout()\n        plt.savefig(os.path.join(\n            workdir, 'grid{}_timeslice{}_justEMPvariogram.eps'.format(gridID, timeslice)))\n    # Plot just experimental variogram\n    else:\n        plt.title('Experimental variogram' + title_str)\n        plt.tight_layout()\n        plt.savefig(os.path.join(\n            workdir, 'grid{}_timeslice{}_justEXPvariogram.eps'.format(gridID, timeslice)))\n    plt.close()\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.convert_SI","title":"<code>convert_SI(val, unit_in, unit_out)</code>","text":"<p>Convert input to desired units</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def convert_SI(val, unit_in, unit_out):\n'''\n        Convert input to desired units\n    '''\n\n    SI = {'mm': 0.001, 'cm': 0.01, 'm': 1.0, 'km': 1000.,\n          'mm^2': 1e-6, 'cm^2': 1e-4, 'm^2': 1.0, 'km^2': 1e+6}\n\n    # avoid conversion if output unit in time\n    if unit_out in ['minute', 'hour', 'day', 'year']:\n        # adjust if input isn't datetime, and assume it to be part of workflow\n        # e.g. sigZTD filter, already extracted datetime object\n        try:\n            return eval('val.apply(pd.to_datetime).dt.{}.astype(float).astype(\"Int32\")'.format(unit_out))\n        except BaseException:  # TODO: Which error(s)?\n            return val\n\n    # check if output spatial unit is supported\n    if unit_out not in SI:\n        raise Exception(\"User-specified output unit {} not recognized.\".format(unit_out))\n\n    return val * SI[unit_in] / SI[unit_out]\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.create_parser","title":"<code>create_parser()</code>","text":"<p>Parse command line arguments using argparse.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def create_parser():\n\"\"\"Parse command line arguments using argparse.\"\"\"\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=\"\"\"\nPerform basic statistical analyses concerning the spatiotemporal distribution of zenith delays.\n\nSpecifically, make any of the following specified plot(s):\nscatterplot of station locations, total empirical and experimental variogram fits for data in each grid cell\n(and for each valid time-slice if -variogram_per_timeslice specified), and gridded heatmaps of data, station distribution,\nrange and sill values associated with experimental variogram fits. The default is to generate all of these.\n\nExample call to plot gridded station mean delay in a specific time interval :\nraiderStats.py -f &lt;filename&gt; -grid_delay_mean -ti '2016-01-01 2018-01-01'\n\nExample call to plot gridded station mean delay in a specific time interval with superimposed gridlines and station scatterplots :\nraiderStats.py -f &lt;filename&gt; -grid_delay_mean -ti '2016-01-01 2018-01-01' --drawgridlines --stationsongrids\n\nExample call to plot gridded station variogram in a specific time interval and through explicitly the summer seasons:\nraiderStats.py -f &lt;filename&gt; -grid_delay_mean -ti '2016-01-01 2018-01-01' --seasonalinterval '06-21 09-21' -variogramplot\n\"\"\")\n\n    # User inputs\n    userinps = parser.add_argument_group(\n        'User inputs/options for which especially careful review is recommended')\n    userinps.add_argument('-f', '--file', dest='fname',\n                          type=str, required=True, help='Final output file generated from downloadGNSSDelays.py which contains GPS zenith delays for a specified time period and spatial footprint. ')\n    userinps.add_argument('-c', '--column_name', dest='col_name', type=str, default='ZTD',\n                          help='Name of the input column to plot. Input assumed to be in units of meters')\n    userinps.add_argument('-u', '--unit', dest='unit', type=str, default='m',\n                          help='Specified output unit (as distance or time), by default m. Input unit assumed to be m following convention in downloadGNSSDelays.py. Refer to \"convert_SI\" for supported units. Note if you specify time unit here, you must specify input for \"--obs_errlimit\" to be in units of m')\n    userinps.add_argument('-w', '--workdir', dest='workdir', default='./',\n                          help='Specify directory to deposit all outputs. Default is local directory where script is launched.')\n    add_cpus(userinps)\n    userinps.add_argument('-verbose', '--verbose', action='store_true', dest='verbose',\n                          help=\"Run in verbose (debug) mode. Default False\")\n\n    # Spatiotemporal subset options\n    dtsubsets = parser.add_argument_group(\n        'Controls for spatiotemporal subsetting.')\n    dtsubsets.add_argument('-b', '--bounding_box', dest='bounding_box', type=str, default=None,\n                           help=\"Provide either valid shapefile or Lat/Lon Bounding SNWE. -- Example : '19 20 -99.5 -98.5'\")\n    dtsubsets.add_argument('-sp', '--spacing', dest='spacing', type=float, default='1',\n                           help='Specify spacing of grid-cells for statistical analyses. By default 1 deg.')\n    dtsubsets.add_argument('-ti', '--timeinterval', dest='timeinterval', type=str, default=None,\n                           help=\"Subset in time by specifying earliest YYYY-MM-DD date followed by latest date YYYY-MM-DD. -- Example : '2016-01-01 2019-01-01'.\")\n    dtsubsets.add_argument('-si', '--seasonalinterval', dest='seasonalinterval', type=str, default=None,\n                           help=\"Subset in by an specific interval for each year by specifying earliest MM-DD time followed by latest MM-DD time. -- Example : '03-21 06-21'.\")\n    dtsubsets.add_argument('-oe', '--obs_errlimit', dest='obs_errlimit', type=float, default='inf',\n                           help=\"Observation error threshold to discard observations with large uncertainties.\")\n\n    # Plot formatting/options\n    pltformat = parser.add_argument_group(\n        'Optional controls for plot formatting/options.')\n    pltformat.add_argument('-figdpi', '--figdpi', dest='figdpi', type=int,\n                           default=100, help='DPI to use for saving figures')\n    pltformat.add_argument('-title', '--user_title', dest='user_title', type=str,\n                           default=None, help='Specify custom title for plots.')\n    pltformat.add_argument('-fmt', '--plot_format', dest='plot_fmt', type=str,\n                           default='png', help='Plot format to use for saving figures')\n    pltformat.add_argument('-cb', '--color_bounds', dest='cbounds', type=str,\n                           default=None, help='List of two floats to use as color axis bounds')\n    pltformat.add_argument('-cp', '--colorpercentile', dest='colorpercentile', type=float, default=None, nargs=2,\n                           help='Set low and upper percentile for plot colorbars. By default 25%% and 95%%, respectively.')\n    pltformat.add_argument('-cm', '--colormap', dest='usr_colormap', type=str, default='hot_r',\n                           help='Specify matplotlib colorbar.')\n    pltformat.add_argument('-dt', '--densitythreshold', dest='densitythreshold', type=int, default='10',\n                           help='For variogram plots, given grid-cell is only valid if it contains this specified threshold of stations. By default 10 stations.')\n    pltformat.add_argument('-sg', '--stationsongrids', dest='stationsongrids', action='store_true',\n                           help='In gridded plots, superimpose your gridded array with a scatterplot of station locations.')\n    pltformat.add_argument('-dg', '--drawgridlines', dest='drawgridlines',\n                           action='store_true', help='Draw gridlines on gridded plots.')\n    pltformat.add_argument('-tl', '--time_lines', dest='time_lines',\n                           action='store_true', help='Draw central longitudinal lines with respect to datetime. Most useful for local-time analyses.')\n    pltformat.add_argument('-plotall', '--plotall', action='store_true', dest='plotall',\n                           help=\"Generate all supported plots, including variogram plots.\")\n    pltformat.add_argument('-min_span', '--min_span', dest='min_span', type=float,\n                           default=[2, 0.6], nargs=2, help=\"Minimum TS span (years) and minimum fractional observations in span (fraction) imposed for seasonal amplitude/phase analyses to be performed for a given station.\")\n    pltformat.add_argument('-period_limit', '--period_limit', dest='period_limit', type=float,\n                           default=0., help=\"period limit (years) imposed for seasonal amplitude/phase analyses to be performed for a given station.\")\n\n    # All plot types\n    # Station scatter-plots\n    pltscatter = parser.add_argument_group(\n        'Supported types of individual station scatter-plots.')\n    pltscatter.add_argument('-station_distribution', '--station_distribution',\n                            action='store_true', dest='station_distribution', help=\"Plot station distribution.\")\n    pltscatter.add_argument('-station_delay_mean', '--station_delay_mean',\n                            action='store_true', dest='station_delay_mean', help=\"Plot station mean delay.\")\n    pltscatter.add_argument('-station_delay_median', '--station_delay_median',\n                            action='store_true', dest='station_delay_median', help=\"Plot station median delay.\")\n    pltscatter.add_argument('-station_delay_stdev', '--station_delay_stdev',\n                            action='store_true', dest='station_delay_stdev', help=\"Plot station delay stdev.\")\n    pltscatter.add_argument('-station_seasonal_phase', '--station_seasonal_phase',\n                            action='store_true', dest='station_seasonal_phase', help=\"Plot station delay phase/amplitude.\")\n    pltscatter.add_argument('-phaseamp_per_station', '--phaseamp_per_station',\n                            action='store_true', dest='phaseamp_per_station', help=\"Save debug figures of curve-fit vs data per station.\")\n\n    # Gridded plots\n    pltgrids = parser.add_argument_group('Supported types of gridded plots.')\n    pltgrids.add_argument('-grid_heatmap', '--grid_heatmap', action='store_true',\n                          dest='grid_heatmap', help=\"Plot gridded station heatmap.\")\n    pltgrids.add_argument('-grid_delay_mean', '--grid_delay_mean', action='store_true',\n                          dest='grid_delay_mean', help=\"Plot gridded station-wise mean delay.\")\n    pltgrids.add_argument('-grid_delay_median', '--grid_delay_median', action='store_true',\n                          dest='grid_delay_median', help=\"Plot gridded station-wise median delay.\")\n    pltgrids.add_argument('-grid_delay_stdev', '--grid_delay_stdev', action='store_true',\n                          dest='grid_delay_stdev', help=\"Plot gridded station-wise delay stdev.\")\n    pltgrids.add_argument('-grid_seasonal_phase', '--grid_seasonal_phase', action='store_true',\n                          dest='grid_seasonal_phase', help=\"Plot gridded station-wise delay phase/amplitude.\")\n    pltgrids.add_argument('-grid_delay_absolute_mean', '--grid_delay_absolute_mean', action='store_true',\n                          dest='grid_delay_absolute_mean', help=\"Plot absolute gridded station mean delay.\")\n    pltgrids.add_argument('-grid_delay_absolute_median', '--grid_delay_absolute_median', action='store_true',\n                          dest='grid_delay_absolute_median', help=\"Plot absolute gridded station median delay.\")\n    pltgrids.add_argument('-grid_delay_absolute_stdev', '--grid_delay_absolute_stdev', action='store_true',\n                          dest='grid_delay_absolute_stdev', help=\"Plot absolute gridded station delay stdev.\")\n    pltgrids.add_argument('-grid_seasonal_absolute_phase', '--grid_seasonal_absolute_phase', action='store_true',\n                          dest='grid_seasonal_absolute_phase', help=\"Plot absolute gridded station delay phase/amplitude.\")\n    pltgrids.add_argument('-grid_to_raster', '--grid_to_raster', action='store_true',\n                          dest='grid_to_raster', help=\"Save gridded array as raster. May directly load/plot in successive script call.\")\n\n    # Variogram plots\n    pltvario = parser.add_argument_group('Supported types of variogram plots.')\n    pltvario.add_argument('-variogramplot', '--variogramplot', action='store_true',\n                          dest='variogramplot', help=\"Plot gridded station variogram.\")\n    pltvario.add_argument('-binnedvariogram', '--binnedvariogram', action='store_true', dest='binnedvariogram',\n                          help=\"Apply experimental variogram fit to total binned empirical variograms for each time slice. Default is to pass total unbinned empiricial variogram.\")\n    pltvario.add_argument('-variogram_per_timeslice', '--variogram_per_timeslice', action='store_true', dest='variogram_per_timeslice',\n                          help=\"Generate variogram plots per gridded station AND time-slice.\")\n    pltvario.add_argument('-variogram_errlimit', '--variogram_errlimit', dest='variogram_errlimit', type=float, default='inf',\n                          help=\"Variogram RMSE threshold to discard grid-cells with large uncertainties.\")\n\n    return parser\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.load_gridfile","title":"<code>load_gridfile(fname, unit)</code>","text":"<p>Function to load gridded-arrays saved from previous runs.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def load_gridfile(fname, unit):\n'''\n        Function to load gridded-arrays saved from previous runs.\n    '''\n\n    with rasterio.open(fname) as src:\n        grid_array = src.read(1).astype(float)\n\n        # Read metadata variables needed for plotting\n        metadata_dict = src.tags()\n\n    # Initiate no-data array to mask data\n    nodat_arr = [0, np.nan, np.inf]\n    if unit in ['minute', 'hour', 'day', 'year']:\n        nodat_arr = [np.nan, np.inf]\n    # set masked values as nans\n    for i in nodat_arr:\n        grid_array = np.ma.masked_where(grid_array == i, grid_array)\n    grid_array = np.ma.filled(grid_array, np.nan)\n\n    # Make plotting command a global variable\n    print('metadata_dict', metadata_dict)\n    gridfile_type = metadata_dict['gridfile_type']\n    globals()[gridfile_type] = True\n\n    plotbbox = [float(i) for i in metadata_dict['plotbbox'].split()]\n    spacing = float(metadata_dict['spacing'])\n    colorbarfmt = metadata_dict['colorbarfmt']\n    inputunit = metadata_dict['unit']\n    # adjust conversion if native units are squared\n    if '^2' in inputunit:\n        unit = unit.split('^2')[0] + '^2'\n    # convert to specified output unit\n    grid_array = convert_SI(grid_array, inputunit, unit)\n\n    # Backwards compatible for cases where this key doesn't exist\n    try:\n        time_lines = metadata_dict['time_lines']\n    except KeyError:\n        time_lines = False\n\n    if metadata_dict['stationsongrids'] == 'False':\n        stationsongrids = False\n    else:\n        stationsongrids = [float(i) for i in metadata_dict['stationsongrids'.split()]]\n\n    if metadata_dict['time_lines'] == 'False':\n        time_lines = False\n    else:\n        time_lines = [float(i) for i in metadata_dict['time_lines'].split()]\n\n    return grid_array, plotbbox, spacing, colorbarfmt, stationsongrids, time_lines\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.midpoint","title":"<code>midpoint(p1, p2)</code>","text":"<p>Calculate central longitude for '--time_lines' option</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def midpoint(p1, p2):\n'''\n        Calculate central longitude for '--time_lines' option\n    '''\n    import math\n\n    lat1, lon1, lat2, lon2 = map(math.radians, (p1[0], p1[1], p2[0], p2[1]))\n    dlon = lon2 - lon1\n    dx = math.cos(lat2) * math.cos(dlon)\n    dy = math.cos(lat2) * math.sin(dlon)\n    lon3 = lon1 + math.atan2(dy, math.cos(lat1) + dx)\n\n    return (int(math.degrees(lon3)))\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.save_gridfile","title":"<code>save_gridfile(df, gridfile_type, fname, plotbbox, spacing, unit, colorbarfmt='%.2f', stationsongrids=False, time_lines=False, dtype='float32', noData=np.nan)</code>","text":"<p>Function to save gridded-arrays as GDAL-readable file.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def save_gridfile(df, gridfile_type, fname, plotbbox, spacing, unit,\n                  colorbarfmt='%.2f', stationsongrids=False, time_lines=False,\n                  dtype=\"float32\", noData=np.nan):\n'''\n        Function to save gridded-arrays as GDAL-readable file.\n    '''\n    # Pass metadata\n    metadata_dict = {}\n    metadata_dict['gridfile_type'] = gridfile_type\n    metadata_dict['plotbbox'] = ' '.join([str(i) for i in plotbbox])\n    metadata_dict['spacing'] = str(spacing)\n    metadata_dict['unit'] = unit\n    if unit in ['minute', 'hour', 'day', 'year']:\n        colorbarfmt = '%1i'\n    metadata_dict['colorbarfmt'] = colorbarfmt\n\n    if stationsongrids:\n        metadata_dict['stationsongrids'] = ' '.join([str(i) for i in stationsongrids])\n    else:\n        metadata_dict['stationsongrids'] = 'False'\n\n    if time_lines:\n        metadata_dict['time_lines'] = ' '.join([str(i) for i in time_lines])\n    else:\n        metadata_dict['time_lines'] = 'False'\n\n    # Write data to file\n    transform = Affine(spacing, 0., plotbbox[0], 0., -1*spacing, plotbbox[-1])\n    with rasterio.open(fname, mode=\"w\", count=1,\n                       width=df.shape[1], height=df.shape[0],\n                       dtype=dtype, nodata=noData,\n                       crs='+proj=latlong', transform=transform) as dst:\n        dst.update_tags(0, **metadata_dict)\n        dst.write(df, 1)\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.cli.statsPlot.stats_analyses","title":"<code>stats_analyses(fname, col_name, unit, workdir, numCPUs, verbose, bbox, spacing, timeinterval, seasonalinterval, obs_errlimit, figdpi, user_title, plot_fmt, cbounds, colorpercentile, usr_colormap, densitythreshold, stationsongrids, drawgridlines, time_lines, plotall, station_distribution, station_delay_mean, station_delay_median, station_delay_stdev, station_seasonal_phase, phaseamp_per_station, grid_heatmap, grid_delay_mean, grid_delay_median, grid_delay_stdev, grid_seasonal_phase, grid_delay_absolute_mean, grid_delay_absolute_median, grid_delay_absolute_stdev, grid_seasonal_absolute_phase, grid_to_raster, min_span, period_limit, variogramplot, binnedvariogram, variogram_per_timeslice, variogram_errlimit)</code>","text":"<p>Main workflow for generating a suite of plots to illustrate spatiotemporal distribution and/or character of zenith delays</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/statsPlot.py</code> <pre><code>def stats_analyses(\n    fname,\n    col_name,\n    unit,\n    workdir,\n    numCPUs,\n    verbose,\n    bbox,\n    spacing,\n    timeinterval,\n    seasonalinterval,\n    obs_errlimit,\n    figdpi,\n    user_title,\n    plot_fmt,\n    cbounds,\n    colorpercentile,\n    usr_colormap,\n    densitythreshold,\n    stationsongrids,\n    drawgridlines,\n    time_lines,\n    plotall,\n    station_distribution,\n    station_delay_mean,\n    station_delay_median,\n    station_delay_stdev,\n    station_seasonal_phase,\n    phaseamp_per_station,\n    grid_heatmap,\n    grid_delay_mean,\n    grid_delay_median,\n    grid_delay_stdev,\n    grid_seasonal_phase,\n    grid_delay_absolute_mean,\n    grid_delay_absolute_median,\n    grid_delay_absolute_stdev,\n    grid_seasonal_absolute_phase,\n    grid_to_raster,\n    min_span,\n    period_limit,\n    variogramplot,\n    binnedvariogram,\n    variogram_per_timeslice,\n    variogram_errlimit\n):\n'''\n    Main workflow for generating a suite of plots to illustrate spatiotemporal distribution\n    and/or character of zenith delays\n    '''\n    if verbose:\n        logger.setLevel(logging.DEBUG)\n\n    # Control DPI for output figures\n    mpl.rcParams['savefig.dpi'] = figdpi\n\n    # If user requests to generate all plots.\n    if plotall:\n        logger.info('\"-plotall\" == True. All plots will be made.')\n        station_distribution = True\n        station_delay_mean = True\n        station_delay_median = True\n        station_delay_stdev = True\n        station_seasonal_phase = True\n        grid_heatmap = True\n        grid_delay_mean = True\n        grid_delay_median = True\n        grid_delay_stdev = True\n        grid_seasonal_phase = True\n        grid_delay_absolute_mean = True\n        grid_delay_absolute_median = True\n        grid_delay_absolute_stdev = True\n        grid_seasonal_absolute_phase = True\n        variogramplot = True\n\n    logger.info(\"***Stats Function:***\")\n    # prep dataframe object for plotting/variogram analysis based off of user specifications\n    df_stats = RaiderStats(fname, col_name, unit, workdir, bbox, spacing,\n                           timeinterval, seasonalinterval, obs_errlimit, time_lines, stationsongrids, station_seasonal_phase, cbounds, colorpercentile,\n                           usr_colormap, grid_heatmap, grid_delay_mean, grid_delay_median, grid_delay_stdev, grid_seasonal_phase,\n                           grid_delay_absolute_mean, grid_delay_absolute_median, grid_delay_absolute_stdev,\n                           grid_seasonal_absolute_phase, grid_to_raster, min_span, period_limit, numCPUs, phaseamp_per_station)\n\n    # Station plots\n    # Plot each individual station\n    if station_distribution:\n        logger.info(\"- Plot spatial distribution of stations.\")\n        unique_points = df_stats.df.groupby(['Lon', 'Lat']).size()\n        df_stats([unique_points.index.get_level_values('Lon').tolist(), unique_points.index.get_level_values('Lat').tolist(\n        )], 'station_distribution', workdir=os.path.join(workdir, 'figures'), plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean delay per station\n    if station_delay_mean:\n        logger.info(\"- Plot mean delay for each station.\")\n        unique_points = df_stats.df.groupby(\n            ['Lon', 'Lat'])[col_name].median()\n        unique_points.dropna(how='any', inplace=True)\n        df_stats([unique_points.index.get_level_values('Lon').tolist(), unique_points.index.get_level_values('Lat').tolist(\n        ), unique_points.values], 'station_delay_mean', workdir=os.path.join(workdir, 'figures'), plotFormat=plot_fmt, userTitle=user_title)\n    # Plot median delay per station\n    if station_delay_median:\n        logger.info(\"- Plot median delay for each station.\")\n        unique_points = df_stats.df.groupby(\n            ['Lon', 'Lat'])[col_name].mean()\n        unique_points.dropna(how='any', inplace=True)\n        df_stats([unique_points.index.get_level_values('Lon').tolist(), unique_points.index.get_level_values('Lat').tolist(\n        ), unique_points.values], 'station_delay_median', workdir=os.path.join(workdir, 'figures'), plotFormat=plot_fmt, userTitle=user_title)\n    # Plot delay stdev per station\n    if station_delay_stdev:\n        logger.info(\"- Plot delay stdev for each station.\")\n        unique_points = df_stats.df.groupby(\n            ['Lon', 'Lat'])[col_name].std()\n        unique_points.dropna(how='any', inplace=True)\n        df_stats([unique_points.index.get_level_values('Lon').tolist(), unique_points.index.get_level_values('Lat').tolist(\n        ), unique_points.values], 'station_delay_stdev', workdir=os.path.join(workdir, 'figures'), plotFormat=plot_fmt, userTitle=user_title)\n    # Plot delay phase/amplitude per station\n    if station_seasonal_phase:\n        logger.info(\"- Plot delay phase/amplitude for each station.\")\n        # phase\n        unique_points_phase = df_stats.df.groupby(\n            ['Lon', 'Lat'])['phsfit'].mean()\n        unique_points_phase.dropna(how='any', inplace=True)\n        df_stats([unique_points_phase.index.get_level_values('Lon').tolist(), unique_points_phase.index.get_level_values('Lat').tolist(\n        ), unique_points_phase.values], 'station_seasonal_phase', workdir=os.path.join(workdir, 'figures'),\n            colorbarfmt='%.1i', plotFormat=plot_fmt, userTitle=user_title)\n        # amplitude\n        unique_points_amplitude = df_stats.df.groupby(\n            ['Lon', 'Lat'])['ampfit'].mean()\n        unique_points_amplitude.dropna(how='any', inplace=True)\n        df_stats([unique_points_amplitude.index.get_level_values('Lon').tolist(), unique_points_amplitude.index.get_level_values('Lat').tolist(\n        ), unique_points_amplitude.values], 'station_seasonal_amplitude', workdir=os.path.join(workdir, 'figures'),\n            colorbarfmt='%.3f', plotFormat=plot_fmt, userTitle=user_title)\n        # period\n        unique_points_period = df_stats.df.groupby(\n            ['Lon', 'Lat'])['periodfit'].mean()\n        df_stats([unique_points_period.index.get_level_values('Lon').tolist(), unique_points_period.index.get_level_values('Lat').tolist(\n        ), unique_points_period.values], 'station_delay_period', workdir=os.path.join(workdir, 'figures'),\n            colorbarfmt='%.2f', plotFormat=plot_fmt, userTitle=user_title)\n\n    # Gridded station plots\n    # Plot density of stations for each gridcell\n    if isinstance(df_stats.grid_heatmap, np.ndarray):\n        logger.info(\"- Plot density of stations per gridcell.\")\n        df_stats(df_stats.grid_heatmap, 'grid_heatmap', workdir=os.path.join(workdir, 'figures'), drawgridlines=drawgridlines,\n                 colorbarfmt='%.1i', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise mean delay across each gridcell\n    if isinstance(df_stats.grid_delay_mean, np.ndarray):\n        logger.info(\"- Plot mean of station-wise mean delay across each gridcell.\")\n        df_stats(df_stats.grid_delay_mean, 'grid_delay_mean', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise median delay across each gridcell\n    if isinstance(df_stats.grid_delay_median, np.ndarray):\n        logger.info(\"- Plot mean of station-wise median delay across each gridcell.\")\n        df_stats(df_stats.grid_delay_median, 'grid_delay_median', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise stdev delay across each gridcell\n    if isinstance(df_stats.grid_delay_stdev, np.ndarray):\n        logger.info(\"- Plot mean of station-wise stdev delay across each gridcell.\")\n        df_stats(df_stats.grid_delay_stdev, 'grid_delay_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise delay phase across each gridcell\n    if isinstance(df_stats.grid_seasonal_phase, np.ndarray):\n        logger.info(\"- Plot mean of station-wise delay phase across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_phase, 'grid_seasonal_phase', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.1i', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise delay amplitude across each gridcell\n    if isinstance(df_stats.grid_seasonal_amplitude, np.ndarray):\n        logger.info(\"- Plot mean of station-wise delay amplitude across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_amplitude, 'grid_seasonal_amplitude', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.3f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of station-wise delay period across each gridcell\n    if isinstance(df_stats.grid_seasonal_period, np.ndarray):\n        logger.info(\"- Plot mean of station-wise delay period across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_period, 'grid_seasonal_period', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean stdev of station-wise delay phase across each gridcell\n    if isinstance(df_stats.grid_seasonal_phase_stdev, np.ndarray):\n        logger.info(\"- Plot mean stdev of station-wise delay phase across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_phase_stdev, 'grid_seasonal_phase_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.1i', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean stdev of station-wise delay amplitude across each gridcell\n    if isinstance(df_stats.grid_seasonal_amplitude_stdev, np.ndarray):\n        logger.info(\"- Plot mean stdev of station-wise delay amplitude across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_amplitude_stdev, 'grid_seasonal_amplitude_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.3f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean stdev of station-wise delay period across each gridcell\n    if isinstance(df_stats.grid_seasonal_period_stdev, np.ndarray):\n        logger.info(\"- Plot mean stdev of station-wise delay period across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_period_stdev, 'grid_seasonal_period_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2e', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot mean of seasonal fit RMSE across each gridcell\n    if isinstance(df_stats.grid_seasonal_fit_rmse, np.ndarray):\n        logger.info(\"- Plot mean of seasonal fit RMSE across each gridcell.\")\n        df_stats(df_stats.grid_seasonal_fit_rmse, 'grid_seasonal_fit_rmse', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.3f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute mean delay for each gridcell\n    if isinstance(df_stats.grid_delay_absolute_mean, np.ndarray):\n        logger.info(\"- Plot absolute mean delay per gridcell.\")\n        df_stats(df_stats.grid_delay_absolute_mean, 'grid_delay_absolute_mean', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute median delay for each gridcell\n    if isinstance(df_stats.grid_delay_absolute_median, np.ndarray):\n        logger.info(\"- Plot absolute median delay per gridcell.\")\n        df_stats(df_stats.grid_delay_absolute_median, 'grid_delay_absolute_median', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute stdev delay for each gridcell\n    if isinstance(df_stats.grid_delay_absolute_stdev, np.ndarray):\n        logger.info(\"- Plot absolute delay stdev per gridcell.\")\n        df_stats(df_stats.grid_delay_absolute_stdev, 'grid_delay_absolute_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay phase for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_phase, np.ndarray):\n        logger.info(\"- Plot absolute delay phase per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_phase, 'grid_seasonal_absolute_phase', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.1i', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay amplitude for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_amplitude, np.ndarray):\n        logger.info(\"- Plot absolute delay amplitude per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_amplitude, 'grid_seasonal_absolute_amplitude', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.3f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay period for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_period, np.ndarray):\n        logger.info(\"- Plot absolute delay period per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_period, 'grid_seasonal_absolute_period', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay phase stdev for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_phase_stdev, np.ndarray):\n        logger.info(\"- Plot absolute delay phase stdev per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_phase_stdev, 'grid_seasonal_absolute_phase_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.1i', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay amplitude stdev for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_amplitude_stdev, np.ndarray):\n        logger.info(\"- Plot absolute delay amplitude stdev per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_amplitude_stdev, 'grid_seasonal_absolute_amplitude_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.3f', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute delay period stdev for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_period_stdev, np.ndarray):\n        logger.info(\"- Plot absolute delay period stdev per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_period_stdev, 'grid_seasonal_absolute_period_stdev', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2e', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    # Plot absolute mean seasonal fit RMSE for each gridcell\n    if isinstance(df_stats.grid_seasonal_absolute_fit_rmse, np.ndarray):\n        logger.info(\"- Plot absolute mean seasonal fit RMSE per gridcell.\")\n        df_stats(df_stats.grid_seasonal_absolute_fit_rmse, 'grid_seasonal_absolute_fit_rmse', workdir=os.path.join(workdir, 'figures'),\n                 drawgridlines=drawgridlines, colorbarfmt='%.2e', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n\n    # Perform variogram analysis\n    if variogramplot and not isinstance(df_stats.grid_range, np.ndarray) \\\n            and not isinstance(df_stats.grid_variance, np.ndarray) \\\n            and not isinstance(df_stats.grid_variogram_rmse, np.ndarray):\n        logger.info(\"***Variogram Analysis Function:***\")\n        if unit in ['minute', 'hour', 'day', 'year']:\n            unit = 'm'\n            df_stats.unit = 'm'\n            logger.warning(\"Output unit {} specified for Variogram analysis. Reverted to meters\".format(unit))\n        make_variograms = VariogramAnalysis(df_stats.df, df_stats.gridpoints, col_name, unit, workdir,\n                                            df_stats.seasonalinterval, densitythreshold, binnedvariogram,\n                                            numCPUs, variogram_per_timeslice, variogram_errlimit)\n        TOT_grids, TOT_res_robust_arr, TOT_res_robust_rmse = make_variograms.create_variograms()\n        # get range\n        df_stats.grid_range = np.array([np.nan if i[0] not in TOT_grids else float(TOT_res_robust_arr[TOT_grids.index(\n            i[0])][0]) for i in enumerate(df_stats.gridpoints)]).reshape(df_stats.grid_dim).T\n        # convert range to specified output unit\n        df_stats.grid_range = convert_SI(df_stats.grid_range, 'm', unit)\n        # get sill\n        df_stats.grid_variance = np.array([np.nan if i[0] not in TOT_grids else float(TOT_res_robust_arr[TOT_grids.index(\n            i[0])][1]) for i in enumerate(df_stats.gridpoints)]).reshape(df_stats.grid_dim).T\n        # convert sill to specified output unit\n        df_stats.grid_range = convert_SI(df_stats.grid_range, 'm^2', unit.split('^2')[0] + '^2')\n        # get variogram rmse\n        df_stats.grid_variogram_rmse = np.array([np.nan if i[0] not in TOT_grids else float(TOT_res_robust_rmse[TOT_grids.index(\n            i[0])]) for i in enumerate(df_stats.gridpoints)]).reshape(df_stats.grid_dim).T\n        # convert range to specified output unit\n        df_stats.grid_variogram_rmse = convert_SI(df_stats.grid_variogram_rmse, 'm', unit)\n        # If specified, save gridded array(s)\n        if grid_to_raster:\n            # write range\n            gridfile_name = os.path.join(workdir, col_name + '_' + 'grid_range' + '.tif')\n            save_gridfile(df_stats.grid_range, 'grid_range', gridfile_name, df_stats.plotbbox, df_stats.spacing,\n                          df_stats.unit, colorbarfmt='%1i',\n                          stationsongrids=df_stats.stationsongrids, dtype='float32')\n            # write sill\n            gridfile_name = os.path.join(workdir, col_name + '_' + 'grid_variance' + '.tif')\n            save_gridfile(df_stats.grid_variance, 'grid_variance', gridfile_name, df_stats.plotbbox, df_stats.spacing,\n                          df_stats.unit + '^2', colorbarfmt='%.3e',\n                          stationsongrids=df_stats.stationsongrids, dtype='float32')\n            # write variogram rmse\n            gridfile_name = os.path.join(workdir, col_name + '_' + 'grid_variogram_rmse' + '.tif')\n            save_gridfile(df_stats.grid_variogram_rmse, 'grid_variogram_rmse', gridfile_name, df_stats.plotbbox, df_stats.spacing,\n                          df_stats.unit, colorbarfmt='%.2e',\n                          stationsongrids=df_stats.stationsongrids, dtype='float32')\n\n    if isinstance(df_stats.grid_range, np.ndarray):\n        # plot range heatmap\n        logger.info(\"- Plot variogram range per gridcell.\")\n        df_stats(df_stats.grid_range, 'grid_range', workdir=os.path.join(workdir, 'figures'),\n                 colorbarfmt='%1i', drawgridlines=drawgridlines, stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    if isinstance(df_stats.grid_variance, np.ndarray):\n        # plot sill heatmap\n        logger.info(\"- Plot variogram sill per gridcell.\")\n        df_stats(df_stats.grid_variance, 'grid_variance', workdir=os.path.join(workdir, 'figures'), drawgridlines=drawgridlines,\n                 colorbarfmt='%.3e', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n    if isinstance(df_stats.grid_variogram_rmse, np.ndarray):\n        # plot variogram rmse heatmap\n        logger.info(\"- Plot variogram RMSE per gridcell.\")\n        df_stats(df_stats.grid_variogram_rmse, 'grid_variogram_rmse', workdir=os.path.join(workdir, 'figures'), drawgridlines=drawgridlines,\n                 colorbarfmt='%.2e', stationsongrids=stationsongrids, plotFormat=plot_fmt, userTitle=user_title)\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators","title":"<code>validators</code>","text":""},{"location":"reference/#RAiDER.cli.validators.BBoxAction","title":"<code>BBoxAction</code>","text":"<p>             Bases: <code>Action</code></p> <p>An Action that parses and stores a valid bounding box</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>class BBoxAction(Action):\n\"\"\"An Action that parses and stores a valid bounding box\"\"\"\n\n    def __init__(\n        self,\n        option_strings,\n        dest,\n        nargs=None,\n        const=None,\n        default=None,\n        type=None,\n        choices=None,\n        required=False,\n        help=None,\n        metavar=None\n    ):\n        if nargs != 4:\n            raise ValueError(\"nargs must be 4!\")\n\n        super().__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=nargs,\n            const=const,\n            default=default,\n            type=type,\n            choices=choices,\n            required=required,\n            help=help,\n            metavar=metavar\n        )\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        S, N, W, E = values\n\n        if N &lt;= S or E &lt;= W:\n            raise ArgumentError(self, 'Bounding box has no size; make sure you use \"S N W E\"')\n\n        for sn in (S, N):\n            if sn &lt; -90 or sn &gt; 90:\n                raise ArgumentError(self, 'Lats are out of S/N bounds (-90 to 90).')\n\n        for we in (W, E):\n            if we &lt; -180 or we &gt; 180:\n                raise ArgumentError(self, 'Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.')\n\n        setattr(namespace, self.dest, values)\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.DateListAction","title":"<code>DateListAction</code>","text":"<p>             Bases: <code>Action</code></p> <p>An Action that parses and stores a list of dates</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>class DateListAction(Action):\n\"\"\"An Action that parses and stores a list of dates\"\"\"\n\n    def __init__(\n        self,\n        option_strings,\n        dest,\n        nargs=None,\n        const=None,\n        default=None,\n        type=None,\n        choices=None,\n        required=False,\n        help=None,\n        metavar=None\n    ):\n        if type is not date_type:\n            raise ValueError(\"type must be `date_type`!\")\n\n        super().__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=nargs,\n            const=const,\n            default=default,\n            type=type,\n            choices=choices,\n            required=required,\n            help=help,\n            metavar=metavar\n        )\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if len(values) &gt; 3 or not values:\n            raise ArgumentError(self, \"Only 1, 2 dates, or 2 dates and interval may be supplied\")\n\n        if len(values) == 2:\n            start, end = values\n            values = [start + timedelta(days=k) for k in range(0, (end - start).days + 1, 1)]\n        elif len(values) == 3:\n            start, end, stepsize = values\n\n            if not isinstance(stepsize.day, int):\n                raise ArgumentError(self, \"The stepsize should be in integer days\")\n\n            new_year = date(year=stepsize.year, month=1, day=1)\n            stepsize = (stepsize - new_year).days + 1\n\n            values = [start + timedelta(days=k)\n                      for k in range(0, (end - start).days + 1, stepsize)]\n\n        setattr(namespace, self.dest, values)\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.IntegerMappingType","title":"<code>IntegerMappingType</code>","text":"<p>             Bases: <code>MappingType</code>, <code>IntegerType</code></p> <p>An integer type that converts non-integer types through a mapping.</p>"},{"location":"reference/#RAiDER.cli.validators.IntegerMappingType--example","title":"Example","text":"<pre><code>integer = IntegerMappingType(0, 100, random=42)\nassert integer(\"0\") == 0\nassert integer(\"100\") == 100\nassert integer(\"random\") == 42\n</code></pre> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>class IntegerMappingType(MappingType, IntegerType):\n\"\"\"\n    An integer type that converts non-integer types through a mapping.\n\n    # Example\n    ```\n    integer = IntegerMappingType(0, 100, random=42)\n    assert integer(\"0\") == 0\n    assert integer(\"100\") == 100\n    assert integer(\"random\") == 42\n    ```\n    \"\"\"\n\n    def __init__(self, lo=None, hi=None, mapping={}, **kwargs):\n        IntegerType.__init__(self, lo, hi)\n        kwargs.update(mapping)\n        MappingType.__init__(self, **kwargs)\n\n    def __call__(self, arg):\n        try:\n            return IntegerType.__call__(self, arg)\n        except ValueError:\n            return MappingType.__call__(self, arg)\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.IntegerType","title":"<code>IntegerType</code>","text":"<p>             Bases: <code>object</code></p> <p>A type that converts arguments to integers.</p>"},{"location":"reference/#RAiDER.cli.validators.IntegerType--example","title":"Example","text":"<pre><code>integer = IntegerType(0, 100)\nassert integer(\"0\") == 0\nassert integer(\"100\") == 100\ninteger(\"-10\")  # Raises exception\n</code></pre> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>class IntegerType(object):\n\"\"\"\n    A type that converts arguments to integers.\n\n    # Example\n    ```\n    integer = IntegerType(0, 100)\n    assert integer(\"0\") == 0\n    assert integer(\"100\") == 100\n    integer(\"-10\")  # Raises exception\n    ```\n    \"\"\"\n\n    def __init__(self, lo=None, hi=None):\n        self.lo = lo\n        self.hi = hi\n\n    def __call__(self, arg):\n        integer = int(arg)\n\n        if self.lo is not None and integer &lt; self.lo:\n            raise ArgumentTypeError(\"Must be greater than {}\".format(self.lo))\n        if self.hi is not None and integer &gt; self.hi:\n            raise ArgumentTypeError(\"Must be less than {}\".format(self.hi))\n\n        return integer\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.MappingType","title":"<code>MappingType</code>","text":"<p>             Bases: <code>object</code></p> <p>A type that maps arguments to constants.</p>"},{"location":"reference/#RAiDER.cli.validators.MappingType--example","title":"Example","text":"<pre><code>mapping = MappingType(foo=42, bar=\"baz\").default(None)\nassert mapping(\"foo\") == 42\nassert mapping(\"bar\") == \"baz\"\nassert mapping(\"hello\") is None\n</code></pre> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>class MappingType(object):\n\"\"\"\n    A type that maps arguments to constants.\n\n    # Example\n    ```\n    mapping = MappingType(foo=42, bar=\"baz\").default(None)\n    assert mapping(\"foo\") == 42\n    assert mapping(\"bar\") == \"baz\"\n    assert mapping(\"hello\") is None\n    ```\n    \"\"\"\n    UNSET = object()\n\n    def __init__(self, **kwargs):\n        self.mapping = kwargs\n        self._default = self.UNSET\n\n    def default(self, default):\n\"\"\"Set a default value if no mapping is found\"\"\"\n        self._default = default\n        return self\n\n    def __call__(self, arg):\n        if arg in self.mapping:\n            return self.mapping[arg]\n\n        if self._default is self.UNSET:\n            raise KeyError(\n                \"Invalid choice '{}', must be one of {}\".format(\n                    arg, list(self.mapping.keys())\n                )\n            )\n\n        return self._default\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.MappingType.default","title":"<code>default(default)</code>","text":"<p>Set a default value if no mapping is found</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def default(self, default):\n\"\"\"Set a default value if no mapping is found\"\"\"\n    self._default = default\n    return self\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.date_type","title":"<code>date_type(arg)</code>","text":"<p>Parse a date from a string in pseudo-ISO 8601 format.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def date_type(arg):\n\"\"\"\n    Parse a date from a string in pseudo-ISO 8601 format.\n    \"\"\"\n    year_formats = (\n        '%Y-%m-%d',\n        '%Y%m%d',\n        '%d',\n        '%j',\n    )\n\n    for yf in year_formats:\n        try:\n            return date(*strptime(arg, yf)[0:3])\n        except ValueError:\n            pass\n\n    raise ArgumentTypeError(\n        'Unable to coerce {} to a date. Try %Y-%m-%d'.format(arg)\n    )\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.enforce_bbox","title":"<code>enforce_bbox(bbox)</code>","text":"<p>Enforce a valid bounding box</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def enforce_bbox(bbox):\n\"\"\"\n    Enforce a valid bounding box\n    \"\"\"\n    if isinstance(bbox, str):\n        bbox = [float(d) for d in bbox.strip().split()]\n    else:\n        bbox = [float(d) for d in bbox]\n\n    # Check the bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bounding box must have 4 elements!\")\n    S, N, W, E = bbox\n\n    if N &lt;= S or E &lt;= W:\n        raise ValueError('Bounding box has no size; make sure you use \"S N W E\"')\n\n    for sn in (S, N):\n        if sn &lt; -90 or sn &gt; 90:\n            raise ValueError('Lats are out of S/N bounds (-90 to 90).')\n\n    for we in (W, E):\n        if we &lt; -180 or we &gt; 180:\n            raise ValueError('Lons are out of W/E bounds (-180 to 180); Lons in the format of (0 to 360) are not supported.')\n\n    return bbox\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.enforce_time","title":"<code>enforce_time(arg_dict)</code>","text":"<p>Parse an input time (required to be ISO 8601)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def enforce_time(arg_dict):\n'''\n    Parse an input time (required to be ISO 8601)\n    '''\n    try:\n        arg_dict['time'] = convert_time(arg_dict['time'])\n    except KeyError:\n        raise ValueError('You must specify a \"time\" in the input config file')\n\n    if 'end_time' in arg_dict.keys():\n        arg_dict['end_time'] = convert_time(arg_dict['end_time'])\n    return arg_dict\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.enforce_valid_dates","title":"<code>enforce_valid_dates(arg)</code>","text":"<p>Parse a date from a string in pseudo-ISO 8601 format.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def enforce_valid_dates(arg):\n\"\"\"\n    Parse a date from a string in pseudo-ISO 8601 format.\n    \"\"\"\n    year_formats = (\n        '%Y-%m-%d',\n        '%Y%m%d',\n        '%d',\n        '%j',\n    )\n\n    for yf in year_formats:\n        try:\n            return datetime.strptime(str(arg), yf)\n        except ValueError:\n            pass\n\n\n    raise ValueError(\n        'Unable to coerce {} to a date. Try %Y-%m-%d'.format(arg)\n    )\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.getBufferedExtent","title":"<code>getBufferedExtent(lats, lons=None, buf=0.0)</code>","text":"<p>get the bounding box around a set of lats/lons</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def getBufferedExtent(lats, lons=None, buf=0.):\n'''\n    get the bounding box around a set of lats/lons\n    '''\n    if lons is None:\n        lats, lons = lats[..., 0], lons[..., 1]\n\n    try:\n        if (lats.size == 1) &amp; (lons.size == 1):\n            out = [lats - buf, lats + buf, lons - buf, lons + buf]\n        elif (lats.size &gt; 1) &amp; (lons.size &gt; 1):\n            out = [np.nanmin(lats), np.nanmax(lats), np.nanmin(lons), np.nanmax(lons)]\n        elif lats.size == 1:\n            out = [lats - buf, lats + buf, np.nanmin(lons), np.nanmax(lons)]\n        elif lons.size == 1:\n            out = [np.nanmin(lats), np.nanmax(lats), lons - buf, lons + buf]\n    except AttributeError:\n        if (isinstance(lats, tuple) or isinstance(lats, list)) and len(lats) == 2:\n            out = [min(lats) - buf, max(lats) + buf, min(lons) - buf, max(lons) + buf]\n    except Exception as e:\n        raise RuntimeError('Not a valid lat/lon shape or variable')\n\n    return np.array(out)\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.get_heights","title":"<code>get_heights(args, out, station_file, bounding_box=None)</code>","text":"<p>Parse the Height info and download a DEM if needed</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def get_heights(args, out, station_file, bounding_box=None):\n'''\n    Parse the Height info and download a DEM if needed\n    '''\n    dem_path = out\n\n    out = {\n            'dem': args.get('dem'),\n            'height_file_rdr': None,\n            'height_levels': None,\n        }\n\n    if args.get('dem'):\n        if (station_file is not None):\n            if 'Hgt_m' not in pd.read_csv(station_file):\n                out['dem'] = os.path.join(dem_path, 'GLO30.dem')\n        elif os.path.exists(args.dem):\n            out['dem'] = args.dem\n            # crop the DEM\n            if bounding_box is not None:\n                dem_bounds = rio_extents(rio_profile(args.dem))\n                lats = dem_bounds[:2]\n                lons = dem_bounds[2:]\n                if isOutside(\n                    bounding_box,\n                    getBufferedExtent(\n                        lats,\n                        lons,\n                        buf=_BUFFER_SIZE,\n                    )\n                ):\n                    raise ValueError(\n                                'Existing DEM does not cover the area of the input lat/lon '\n                                'points; either move the DEM, delete it, or change the input '\n                                'points.'\n                            )\n        else:\n            pass # will download the dem later\n\n    elif args.get('height_file_rdr'):\n        out['height_file_rdr'] = args.height_file_rdr\n\n    else:\n        # download the DEM if needed\n        out['dem'] = os.path.join(dem_path, 'GLO30.dem')\n\n    if args.get('height_levels'):\n        if isinstance(args.height_levels, str):\n            l = re.findall('[-0-9]+', args.height_levels)\n        else:\n            l = args.height_levels\n\n        out['height_levels'] = np.array([float(ll) for ll in l])\n        if np.any(out['height_levels'] &lt; 0):\n            logger.warning('Weather model only extends to the surface topography; '\n            'height levels below the topography will be interpolated from the surface '\n            'and may be inaccurate.')\n\n    return out\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.get_query_region","title":"<code>get_query_region(args)</code>","text":"<p>Parse the query region from inputs</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def get_query_region(args):\n'''\n    Parse the query region from inputs\n    '''\n    # Get bounds from the inputs\n    # make sure this is first\n    if args.get('use_dem_latlon'):\n        query = GeocodedFile(args.dem, is_dem=True)\n\n    elif args.get('lat_file'):\n        hgt_file = args.get('height_file_rdr') # only get it if exists\n        dem_file = args.get('dem')\n        query    = RasterRDR(args.lat_file, args.lon_file, hgt_file, dem_file)\n\n    elif args.get('station_file'):\n        query = StationFile(args.station_file)\n\n    elif args.get('bounding_box'):\n        bbox = enforce_bbox(args.bounding_box)\n        if (np.min(bbox[0]) &lt; -90) | (np.max(bbox[1]) &gt; 90):\n            raise ValueError('Lats are out of N/S bounds; are your lat/lon coordinates switched? Should be SNWE')\n        query = BoundingBox(bbox)\n\n    elif args.get('geocoded_file'):\n        gfile  = os.path.basename(args.geocoded_file).upper()\n        if (gfile.startswith('SRTM') or gfile.startswith('GLO')):\n            logger.debug('Using user DEM: %s', gfile)\n            is_dem = True\n        else:\n            is_dem = False\n\n        query  = GeocodedFile(args.geocoded_file, is_dem=is_dem)\n\n    ## untested\n    elif args.get('geo_cube'):\n        query = Geocube(args.geo_cube)\n\n    else:\n        # TODO: Need to incorporate the cube\n        raise ValueError('No valid query points or bounding box found in the configuration file')\n\n\n    return query\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.isInside","title":"<code>isInside(extent1, extent2)</code>","text":"<p>Determine whether all of extent1 lies inside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]. Equal extents are considered \"inside\"</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def isInside(extent1, extent2):\n'''\n    Determine whether all of extent1 lies inside extent2\n    extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon].\n    Equal extents are considered \"inside\"\n    '''\n    t1 = extent1[0] &lt;= extent2[0]\n    t2 = extent1[1] &gt;= extent2[1]\n    t3 = extent1[2] &lt;= extent2[2]\n    t4 = extent1[3] &gt;= extent2[3]\n    if np.all([t1, t2, t3, t4]):\n        return True\n    return False\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.isOutside","title":"<code>isOutside(extent1, extent2)</code>","text":"<p>Determine whether any of extent1  lies outside extent2 extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon] Equal extents are considered \"inside\"</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def isOutside(extent1, extent2):\n'''\n    Determine whether any of extent1  lies outside extent2\n    extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]\n    Equal extents are considered \"inside\"\n    '''\n    t1 = extent1[0] &lt; extent2[0]\n    t2 = extent1[1] &gt; extent2[1]\n    t3 = extent1[2] &lt; extent2[2]\n    t4 = extent1[3] &gt; extent2[3]\n    if np.any([t1, t2, t3, t4]):\n        return True\n    return False\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.modelName2Module","title":"<code>modelName2Module(model_name)</code>","text":"<p>Turn an arbitrary string into a module name. Takes as input a model name, which hopefully looks like ERA-I, and converts it to a module name, which will look like erai. I doesn't always produce a valid module name, but that's not the goal. The goal is just to handle common cases. Inputs:    model_name  - Name of an allowed weather model (e.g., 'era-5') Outputs:    module_name - Name of the module    wmObject    - callable, weather model object</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def modelName2Module(model_name):\n\"\"\"Turn an arbitrary string into a module name.\n    Takes as input a model name, which hopefully looks like ERA-I, and\n    converts it to a module name, which will look like erai. I doesn't\n    always produce a valid module name, but that's not the goal. The\n    goal is just to handle common cases.\n    Inputs:\n       model_name  - Name of an allowed weather model (e.g., 'era-5')\n    Outputs:\n       module_name - Name of the module\n       wmObject    - callable, weather model object\n    \"\"\"\n    module_name = 'RAiDER.models.' + model_name.lower().replace('-', '')\n    model_module = importlib.import_module(module_name)\n    wmObject = getattr(model_module, model_name.upper().replace('-', ''))\n    return module_name, wmObject\n</code></pre>"},{"location":"reference/#RAiDER.cli.validators.parse_dates","title":"<code>parse_dates(arg_dict)</code>","text":"<p>Determine the requested dates from the input parameters</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/cli/validators.py</code> <pre><code>def parse_dates(arg_dict):\n'''\n    Determine the requested dates from the input parameters\n    '''\n\n    if arg_dict.get('date_list'):\n        l = arg_dict['date_list']\n        if isinstance(l, str):\n            l = re.findall('[0-9]+', l)\n        elif isinstance(l, int):\n            l = [l]\n        L = [enforce_valid_dates(d) for d in l]\n\n    else:\n        try:\n            start = arg_dict['date_start']\n        except KeyError:\n            raise ValueError('Inputs must include either date_list or date_start')\n        start = enforce_valid_dates(start)\n\n        if arg_dict.get('date_end'):\n            end = arg_dict['date_end']\n            end = enforce_valid_dates(end)\n        else:\n           end = start\n\n        if arg_dict.get('date_step'):\n            step = int(arg_dict['date_step'])\n        else:\n            step = 1\n\n        L = [start + timedelta(days=step) for step in range(0, (end - start).days + 1, step)]\n\n    return L\n</code></pre>"},{"location":"reference/#RAiDER.delay","title":"<code>delay</code>","text":"<p>RAiDER tropospheric delay calculation</p> <p>This module provides the main RAiDER functionality for calculating tropospheric wet and hydrostatic delays from a weather model. Weather models are accessed as NETCDF files and should have \"wet\" \"hydro\" \"wet_total\" and \"hydro_total\" fields specified.</p>"},{"location":"reference/#RAiDER.delay.transformPoints","title":"<code>transformPoints(lats, lons, hgts, old_proj, new_proj)</code>","text":"<p>Transform lat/lon/hgt data to an array of points in a new projection</p> <p>Parameters:</p> Name Type Description Default <code>lats</code> <code>ndarray</code> <p>ndarray   - WGS-84 latitude (EPSG: 4326)</p> required <code>lons</code> <code>ndarray</code> <p>ndarray   - ditto for longitude</p> required <code>hgts</code> <code>ndarray</code> <p>ndarray   - Ellipsoidal height in meters</p> required <code>old_proj</code> <code>CRS</code> <p>CRS   - the original projection of the points</p> required <code>new_proj</code> <code>CRS</code> <p>CRS   - the new projection in which to return the points</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>the array of query points in the weather model coordinate system (YX)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def transformPoints(lats: np.ndarray, lons: np.ndarray, hgts: np.ndarray, old_proj: CRS, new_proj: CRS) -&gt; np.ndarray:\n'''\n    Transform lat/lon/hgt data to an array of points in a new\n    projection\n\n    Args:\n        lats: ndarray   - WGS-84 latitude (EPSG: 4326)\n        lons: ndarray   - ditto for longitude\n        hgts: ndarray   - Ellipsoidal height in meters\n        old_proj: CRS   - the original projection of the points\n        new_proj: CRS   - the new projection in which to return the points\n\n    Returns:\n        ndarray: the array of query points in the weather model coordinate system (YX)\n    '''\n    # Flags for flipping inputs or outputs\n    if not isinstance(new_proj, CRS):\n        new_proj = CRS.from_epsg(new_proj.lstrip('EPSG:'))\n    if not isinstance(old_proj, CRS):\n        old_proj = CRS.from_epsg(old_proj.lstrip('EPSG:'))\n\n    t = Transformer.from_crs(old_proj, new_proj, always_xy=True)\n\n    # in_flip = old_proj.axis_info[0].direction\n    # out_flip = new_proj.axis_info[0].direction\n\n    res  = t.transform(lons, lats, hgts)\n\n    # lat/lon/height\n    return  np.stack([res[1], res[0], res[2]], axis=-1)\n</code></pre>"},{"location":"reference/#RAiDER.delay.tropo_delay","title":"<code>tropo_delay(dt, weather_model_file, aoi, los, height_levels=None, out_proj=4326, zref=_ZREF)</code>","text":"<p>Calculate integrated delays on query points. Options are: 1. Zenith delays (ZTD) 2. Zenith delays projected to the line-of-sight (STD-projected) 3. Slant delays integrated along the raypath (STD-raytracing)</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <p>Datetime                - Datetime object for determining when to calculate delays</p> required <code>weather_model_File</code> <p>string  - Name of the NETCDF file containing a pre-processed weather model</p> required <code>aoi</code> <p>AOI object             - AOI object</p> required <code>los</code> <p>LOS object             - LOS object</p> required <code>height_levels</code> <code>List[float]</code> <p>list         - (optional) list of height levels on which to calculate delays. Only needed for cube generation.</p> <code>None</code> <code>out_proj</code> <code>Union[int, str]</code> <p>int,str           - (optional) EPSG code for output projection</p> <code>4326</code> <code>zref</code> <code>Union[int, float]</code> <p>int,float             - (optional) maximum height to integrate up to during raytracing</p> <code>_ZREF</code> <p>Returns:</p> Type Description <p>xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def tropo_delay(\n        dt,\n        weather_model_file: str,\n        aoi,\n        los,\n        height_levels: List[float]=None,\n        out_proj: Union[int, str] =4326,\n        zref: Union[int, float]=_ZREF,\n    ):\n\"\"\"\n    Calculate integrated delays on query points. Options are:\n    1. Zenith delays (ZTD)\n    2. Zenith delays projected to the line-of-sight (STD-projected)\n    3. Slant delays integrated along the raypath (STD-raytracing)\n\n    Args:\n        dt: Datetime                - Datetime object for determining when to calculate delays\n        weather_model_File: string  - Name of the NETCDF file containing a pre-processed weather model\n        aoi: AOI object             - AOI object\n        los: LOS object             - LOS object\n        height_levels: list         - (optional) list of height levels on which to calculate delays. Only needed for cube generation.\n        out_proj: int,str           - (optional) EPSG code for output projection\n        zref: int,float             - (optional) maximum height to integrate up to during raytracing\n\n\n    Returns:\n        xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points.\n    \"\"\"\n    crs = CRS(out_proj)\n\n    # Load CRS from weather model file\n    with xarray.load_dataset(weather_model_file) as ds:\n        try:\n            wm_proj = CRS.from_wkt(ds['proj'].attrs['crs_wkt'])\n        except KeyError:\n            logger.warning(\"WARNING: I can't find a CRS in the weather model file, so I will assume you are using WGS84\")\n            wm_proj = CRS.from_epsg(4326)\n\n    # get heights\n    with xarray.load_dataset(weather_model_file) as ds:\n        wm_levels = ds.z.values\n        toa       = wm_levels.max() - 1\n\n\n    if height_levels is None:\n        if aoi.type() == 'Geocube':\n            height_levels = aoi.readZ()\n        else:\n            height_levels = wm_levels\n\n    if not zref:\n        zref = toa\n\n    if zref &gt; toa:\n        zref = toa\n        logger.warning('Requested integration height (zref) is higher than top of weather model. Forcing to top ({toa}).')\n\n\n    #TODO: expose this as library function\n    ds = _get_delays_on_cube(dt, weather_model_file, wm_proj, aoi, height_levels,\n            los, crs, zref)\n\n    if (aoi.type() == 'bounding_box') or (aoi.type() == 'Geocube'):\n        return ds, None\n\n    else:\n        # CRS can be an int, str, or CRS object\n        try:\n            out_proj = CRS.from_epsg(out_proj)\n        except pyproj.exceptions.CRSError:\n            out_proj = out_proj\n\n        pnt_proj = CRS.from_epsg(4326)\n        lats, lons = aoi.readLL()\n        hgts = aoi.readZ()\n        pnts = transformPoints(lats, lons, hgts, pnt_proj, out_proj)\n\n        try:\n            ifWet, ifHydro = getInterpolators(ds, \"ztd\")\n        except RuntimeError:\n            logger.exception('Failed to get weather model %s interpolators.', weather_model_file)\n\n        wetDelay = ifWet(pnts)\n        hydroDelay = ifHydro(pnts)\n\n        # return the delays (ZTD or STD)\n        if los.is_Projected():\n            los.setTime(dt)\n            los.setPoints(lats, lons, hgts)\n            wetDelay   = los(wetDelay)\n            hydroDelay = los(hydroDelay)\n\n    return wetDelay, hydroDelay\n</code></pre>"},{"location":"reference/#RAiDER.delay.writeResultsToXarray","title":"<code>writeResultsToXarray(dt, xpts, ypts, zpts, crs, wetDelay, hydroDelay, weather_model_file, out_type)</code>","text":"<p>write a 1-D array to a NETCDF5 file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def writeResultsToXarray(dt, xpts, ypts, zpts, crs, wetDelay, hydroDelay, weather_model_file, out_type):\n'''\n    write a 1-D array to a NETCDF5 file\n    '''\n       # Modify this as needed for NISAR / other projects\n    ds = xarray.Dataset(\n        data_vars=dict(\n            wet=([\"z\", \"y\", \"x\"],\n                 wetDelay,\n                 {\"units\" : \"m\",\n                  \"description\": f\"wet {out_type} delay\",\n                  # 'crs': crs.to_epsg(),\n                  \"grid_mapping\": \"crs\",\n\n                 }),\n            hydro=([\"z\", \"y\", \"x\"],\n                   hydroDelay,\n                   {\"units\": \"m\",\n                    # 'crs': crs.to_epsg(),\n                    \"description\": f\"hydrostatic {out_type} delay\",\n                    \"grid_mapping\": \"crs\",\n                   }),\n        ),\n        coords=dict(\n            x=([\"x\"], xpts),\n            y=([\"y\"], ypts),\n            z=([\"z\"], zpts),\n        ),\n        attrs=dict(\n            Conventions=\"CF-1.7\",\n            title=\"RAiDER geo cube\",\n            source=os.path.basename(weather_model_file),\n            history=str(datetime.utcnow()) + \" RAiDER\",\n            description=f\"RAiDER geo cube - {out_type}\",\n            reference_time=str(dt),\n        ),\n    )\n\n    # Write projection system mapping\n    ds[\"crs\"] = int(-2147483647) # dummy placeholder\n    for k, v in crs.to_cf().items():\n        ds.crs.attrs[k] = v\n\n    # Write z-axis information\n    ds.z.attrs[\"axis\"] = \"Z\"\n    ds.z.attrs[\"units\"] = \"m\"\n    ds.z.attrs[\"description\"] = \"height above ellipsoid\"\n\n    # If in degrees\n    if crs.axis_info[0].unit_name == \"degree\":\n        ds.y.attrs[\"units\"] = \"degrees_north\"\n        ds.y.attrs[\"standard_name\"] = \"latitude\"\n        ds.y.attrs[\"long_name\"] = \"latitude\"\n\n        ds.x.attrs[\"units\"] = \"degrees_east\"\n        ds.x.attrs[\"standard_name\"] = \"longitude\"\n        ds.x.attrs[\"long_name\"] = \"longitude\"\n\n    else:\n        ds.y.attrs[\"axis\"] = \"Y\"\n        ds.y.attrs[\"standard_name\"] = \"projection_y_coordinate\"\n        ds.y.attrs[\"long_name\"] = \"y-coordinate in projected coordinate system\"\n        ds.y.attrs[\"units\"] = \"m\"\n\n        ds.x.attrs[\"axis\"] = \"X\"\n        ds.x.attrs[\"standard_name\"] = \"projection_x_coordinate\"\n        ds.x.attrs[\"long_name\"] = \"x-coordinate in projected coordinate system\"\n        ds.x.attrs[\"units\"] = \"m\"\n\n    return ds\n</code></pre>"},{"location":"reference/#RAiDER.delayFcns","title":"<code>delayFcns</code>","text":""},{"location":"reference/#RAiDER.delayFcns.getInterpolators","title":"<code>getInterpolators(wm_file, kind='pointwise', shared=False)</code>","text":"<p>Read 3D gridded data from a processed weather model file and wrap it with the scipy RegularGridInterpolator</p> <p>The interpolator grid is (y, x, z)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delayFcns.py</code> <pre><code>def getInterpolators(wm_file, kind='pointwise', shared=False):\n'''\n    Read 3D gridded data from a processed weather model file and wrap it with\n    the scipy RegularGridInterpolator\n\n    The interpolator grid is (y, x, z)\n    '''\n    # Get the weather model data\n    try:\n        ds = xarray.load_dataset(wm_file)\n    except ValueError:\n        ds = wm_file\n\n    xs_wm = np.array(ds.variables['x'][:])\n    ys_wm = np.array(ds.variables['y'][:])\n    zs_wm = np.array(ds.variables['z'][:])\n\n    wet = ds.variables['wet_total' if kind=='total' else 'wet'][:]\n    hydro = ds.variables['hydro_total' if kind=='total' else 'hydro'][:]\n\n    wet = np.array(wet).transpose(1, 2, 0)\n    hydro = np.array(hydro).transpose(1, 2, 0)\n\n    if np.any(np.isnan(wet)) or np.any(np.isnan(hydro)):\n        logger.critical(f'Weather model contains NaNs!')\n\n    # If shared interpolators are requested\n    # The arrays are not modified - so turning off lock for performance\n    if shared:\n        xs_wm = make_shared_raw(xs_wm)\n        ys_wm = make_shared_raw(ys_wm)\n        zs_wm = make_shared_raw(zs_wm)\n        wet   = make_shared_raw(wet)\n        hydro = make_shared_raw(hydro)\n\n\n    ifWet = Interpolator((ys_wm, xs_wm, zs_wm), wet, fill_value=np.nan, bounds_error = False)\n    ifHydro = Interpolator((ys_wm, xs_wm, zs_wm), hydro, fill_value=np.nan, bounds_error = False)\n\n    return ifWet, ifHydro\n</code></pre>"},{"location":"reference/#RAiDER.delayFcns.interpolate2","title":"<code>interpolate2(fun, x, y, z)</code>","text":"<p>helper function to make the interpolation step cleaner</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delayFcns.py</code> <pre><code>def interpolate2(fun, x, y, z):\n'''\n    helper function to make the interpolation step cleaner\n    '''\n    in_shape = x.shape\n    out = fun((y.ravel(), x.ravel(), z.ravel()))  # note that this re-ordering is on purpose to match the weather model\n    outData = out.reshape(in_shape)\n    return outData\n</code></pre>"},{"location":"reference/#RAiDER.delayFcns.make_shared_raw","title":"<code>make_shared_raw(inarr)</code>","text":"<p>Make numpy view array of mp.Array</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delayFcns.py</code> <pre><code>def make_shared_raw(inarr):\n\"\"\"\n    Make numpy view array of mp.Array\n    \"\"\"\n    # Create flat shared array\n    if mp is None:\n        raise ImportError('multiprocessing is not available')\n\n    shared_arr = mp.RawArray('d', inarr.size)\n    # Create a numpy view of it\n    shared_arr_np = np.ndarray(inarr.shape, dtype=np.float64,\n                               buffer=shared_arr)\n    # Copy data to shared array\n    np.copyto(shared_arr_np, inarr)\n\n    return shared_arr_np\n</code></pre>"},{"location":"reference/#RAiDER.dem","title":"<code>dem</code>","text":""},{"location":"reference/#RAiDER.dem.download_dem","title":"<code>download_dem(ll_bounds, writeDEM=False, outName='warpedDEM', buf=0.02, overwrite=False)</code>","text":"<p>Download a DEM if one is not already present.  Args:         llbounds: list/ndarry of floats   -lat/lon bounds of the area to download. Values should be ordered in the following way: [S, N, W, E]         writeDEM: boolean                 -write the DEM to file         outName: string                   -name of the DEM file          buf: float                        -buffer to add to the bounds         overwrite: boolean                -overwrite existing DEM Returns:         zvals: np.array         -DEM heights         metadata:               -metadata for the DEM</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/dem.py</code> <pre><code>def download_dem(\n    ll_bounds,\n    writeDEM=False,\n    outName='warpedDEM',\n    buf=0.02,\n    overwrite=False,\n):\n\"\"\"  \n    Download a DEM if one is not already present. \n    Args:\n            llbounds: list/ndarry of floats   -lat/lon bounds of the area to download. Values should be ordered in the following way: [S, N, W, E]\n            writeDEM: boolean                 -write the DEM to file\n            outName: string                   -name of the DEM file   \n            buf: float                        -buffer to add to the bounds\n            overwrite: boolean                -overwrite existing DEM\n    Returns:\n            zvals: np.array         -DEM heights\n            metadata:               -metadata for the DEM\n    \"\"\"\n    if os.path.exists(outName) and not overwrite:\n        logger.info('Using existing DEM: %s', outName)\n        zvals, metadata = rio_open(outName, returnProj=True)\n\n    else:\n        # inExtent is SNWE\n        # dem-stitcher wants WSEN\n        bounds = [\n            np.floor(ll_bounds[2]) - buf, np.floor(ll_bounds[0]) - buf,\n            np.ceil(ll_bounds[3]) + buf, np.ceil(ll_bounds[1]) + buf\n        ]\n\n        zvals, metadata = stitch_dem(\n            bounds,\n            dem_name='glo_30',\n            dst_ellipsoidal_height=True,\n            dst_area_or_point='Area',\n        )\n        if writeDEM:\n            with rasterio.open(outName, 'w', **metadata) as ds:\n                ds.write(zvals, 1)\n                ds.update_tags(AREA_OR_POINT='Point')\n            logger.info('Wrote DEM: %s', outName)\n\n    return zvals, metadata\n</code></pre>"},{"location":"reference/#RAiDER.dem.getHeights","title":"<code>getHeights(ll_bounds, dem_type, dem_file, lats=None, lons=None)</code>","text":"<p>Fcn to return heights from a DEM, either one that already exists or will download one if needed.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/dem.py</code> <pre><code>def getHeights(ll_bounds, dem_type, dem_file, lats=None, lons=None):\n'''\n    Fcn to return heights from a DEM, either one that already exists\n    or will download one if needed.\n    '''\n    # height_type, height_data = heights\n    if dem_type == 'hgt':\n        htinfo = get_file_and_band(dem_file)\n        hts = rio_open(htinfo[0], band=htinfo[1])\n\n    elif dem_type == 'csv':\n        # Heights are in the .csv file\n        hts = pd.read_csv(dem_file)['Hgt_m'].values\n\n    elif dem_type == 'interpolate':\n        # heights will be vertically interpolated to the heightlvs\n        hts = None\n\n    elif (dem_type == 'download') or (dem_type == 'dem'):\n        zvals, metadata = download_dem(ll_bounds, writeDEM=True, outName=dem_file)\n\n        #TODO: check this\n        lons, lats = np.meshgrid(lons, lats)\n        # Interpolate to the query points\n        hts = interpolateDEM(zvals, metadata['transform'], (lats, lons), method='nearest')\n\n    return hts\n</code></pre>"},{"location":"reference/#RAiDER.getStationDelays","title":"<code>getStationDelays</code>","text":""},{"location":"reference/#RAiDER.getStationDelays.get_date","title":"<code>get_date(stationFile)</code>","text":"<p>extract the date from a station delay file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/getStationDelays.py</code> <pre><code>def get_date(stationFile):\n'''\n    extract the date from a station delay file\n    '''\n\n    # find the date info\n    year = int(stationFile[1])\n    doy = int(stationFile[2])\n    date = dt.datetime(year, 1, 1) + dt.timedelta(doy - 1)\n\n    return date, year, doy\n</code></pre>"},{"location":"reference/#RAiDER.getStationDelays.get_delays_UNR","title":"<code>get_delays_UNR(stationFile, filename, dateList, returnTime=None)</code>","text":"<p>Parses and returns a dictionary containing either (1) all the GPS delays, if returnTime is None, or (2) only the delay at the closest times to to returnTime. Inputs:      stationFile - a .gz station delay file      returnTime  - specified time of GPS delay Outputs:      a dict and CSV file containing the times and delay information      (delay in mm, delay uncertainty, delay gradients) *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG). Source  \u2014&gt; http://geodesy.unr.edu/gps_timeseries/README_trop2.txt)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/getStationDelays.py</code> <pre><code>def get_delays_UNR(stationFile, filename, dateList, returnTime=None):\n'''\n    Parses and returns a dictionary containing either (1) all\n    the GPS delays, if returnTime is None, or (2) only the delay\n    at the closest times to to returnTime.\n    Inputs:\n         stationFile - a .gz station delay file\n         returnTime  - specified time of GPS delay\n    Outputs:\n         a dict and CSV file containing the times and delay information\n         (delay in mm, delay uncertainty, delay gradients)\n    *NOTE: Due to a formatting error in the tropo SINEX files, the two tropospheric gradient columns\n    (TGNTOT and TGETOT) are interchanged, as are the formal error columns (_SIG).\n    Source  \u2014&gt; http://geodesy.unr.edu/gps_timeseries/README_trop2.txt)\n    '''\n    # Refer to the following sites to interpret stationFile variable names:\n    # ftp://igs.org/pub/data/format/sinex_tropo.txt\n    # http://geodesy.unr.edu/gps_timeseries/README_trop2.txt\n    # Wet and hydrostratic delays were derived as so:\n    # Constants \u2014&gt; k1 = 0.704, k2 = 0.776, k3 = 3739.0, m = 18.0152/28.9644,\n    # k2' = k2-(k1*m) = 0.33812796398337275, Rv = 461.5 J/(kg\u00b7K), \u03c1l = 997 kg/m^3\n    # Note wet delays passed here may be computed as so\n    # where PMV = precipitable water vapor, P = total atm pressure, Tm = mean temp of the column \u2014&gt;\n    # Wet zenith delay = 10^-6 \u03c1lRv(k2' + k3/Tm) PMV\n    # Hydrostatic zenith delay = Total zenith delay - wet zenith delay = k1*(P/Tm)\n    # Source \u2014&gt; Hanssen, R. F. (2001) eqns. 6.2.7-10\n\n    # sort through station zip files\n    allstationTarfiles = []\n    # if URL\n    if stationFile.startswith('http'):\n        r = requests.get(stationFile)\n        ziprepo = zipfile.ZipFile(io.BytesIO(r.content))\n    # if downloaded file\n    else:\n        ziprepo = zipfile.ZipFile(stationFile)\n    # iterate through tarfiles\n    stationTarlist = sorted(ziprepo.namelist())\n\n    final_stationTarlist = []\n    for j in stationTarlist:\n        # get the date of the file\n        time, yearFromFile, doyFromFile = get_date(os.path.basename(j).split('.'))\n        # check if in list of specified input dates\n        if time.strftime('%Y-%m-%d') not in dateList:\n            continue\n        final_stationTarlist.append(j)\n        f = gzip.open(ziprepo.open(j), 'rb')\n        # initialize variables\n        d, Sig, dwet, dhydro, timesList = [], [], [], [], []\n        flag = False\n        for line in f.readlines():\n            try:\n                line = line.decode('utf-8')\n            except UnicodeDecodeError:\n                line = line.decode('latin-1')\n            if flag:\n                # Do not attempt to read header\n                if 'SITE' in line:\n                    continue\n                # Attempt to read data\n                try:\n                    split_lines = line.split()\n                    # units: mm, mm, mm, deg, deg, deg, deg, mm, mm, K\n                    trotot, trototSD, trwet, tgetot, tgetotSD, tgntot, tgntotSD, wvapor, wvaporSD, mtemp = \\\n                        [float(t) for t in split_lines[2:]]\n                except BaseException:  # TODO: What error(s)?\n                    continue\n                site = split_lines[0]\n                year, doy, seconds = [int(n)\n                                      for n in split_lines[1].split(':')]\n                # Break iteration if time from line in file does not match date reported in filename\n                if doy != doyFromFile:\n                    logger.warning(\n                        'time %s from line in conflict with time %s from file '\n                        '%s, will continue reading next tarfile(s)',\n                        doy, doyFromFile, j\n                    )\n                    continue\n                # convert units from mm to m\n                d.append(trotot * 0.001)\n                Sig.append(trototSD * 0.001)\n                dwet.append(trwet * 0.001)\n                dhydro.append((trotot - trwet) * 0.001)\n                timesList.append(seconds)\n            if 'TROP/SOLUTION' in line:\n                flag = True\n        del f\n        # Break iteration if file contains no data.\n        if d == []:\n            logger.warning(\n                'file %s for station %s is empty, will continue reading next '\n                'tarfile(s)', j, j.split('.')[0]\n            )\n            continue\n\n        # check for missing times\n        true_times = list(range(0, 86400, 300))\n        if len(timesList) != len(true_times):\n            missing = [\n                True if t not in timesList else False for t in true_times]\n            mask = np.array(missing)\n            delay, sig, wet_delay, hydro_delay = [np.full((288,), np.nan)] * 4\n            delay[~mask] = d\n            sig[~mask] = Sig\n            wet_delay[~mask] = dwet\n            hydro_delay[~mask] = dhydro\n            times = true_times.copy()\n        else:\n            delay = np.array(d)\n            times = np.array(timesList)\n            sig = np.array(Sig)\n            wet_delay = np.array(dwet)\n            hydro_delay = np.array(dhydro)\n\n        # if time not specified, pass all times\n        if returnTime is None:\n            filtoutput = {'ID': [site] * len(wet_delay), 'Date': [time] * len(wet_delay), 'ZTD': delay, 'wet_delay': wet_delay,\n                          'hydrostatic_delay': hydro_delay, 'times': times, 'sigZTD': sig}\n            filtoutput = [{key: value[k] for key, value in filtoutput.items()}\n                          for k in range(len(filtoutput['ID']))]\n        else:\n            index = np.argmin(np.abs(np.array(timesList) - returnTime))\n            filtoutput = [{'ID': site, 'Date': time, 'ZTD': delay[index], 'wet_delay': wet_delay[index],\n                           'hydrostatic_delay': hydro_delay[index], 'times': times[index], 'sigZTD': sig[index]}]\n        # setup pandas array and write output to CSV, making sure to update existing CSV.\n        filtoutput = pd.DataFrame(filtoutput)\n        if os.path.exists(filename):\n            filtoutput.to_csv(filename, index=False, mode='a', header=False)\n        else:\n            filtoutput.to_csv(filename, index=False)\n\n    # record all used tar files\n    allstationTarfiles.extend([os.path.join(stationFile, k)\n                               for k in stationTarlist])\n    allstationTarfiles.sort()\n    del ziprepo\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.getStationDelays.get_station_data","title":"<code>get_station_data(inFile, dateList, gps_repo=None, numCPUs=8, outDir=None, returnTime=None)</code>","text":"<p>Pull tropospheric delay data for a given station name</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/getStationDelays.py</code> <pre><code>def get_station_data(inFile, dateList, gps_repo=None, numCPUs=8, outDir=None, returnTime=None):\n'''\n    Pull tropospheric delay data for a given station name\n    '''\n    if outDir is None:\n        outDir = os.getcwd()\n\n    pathbase = os.path.join(outDir, 'GPS_delays')\n    if not os.path.exists(pathbase):\n        os.mkdir(pathbase)\n\n    returnTime = seconds_of_day(returnTime)\n    # print warning if not divisible by 3 seconds\n    if returnTime % 3 != 0:\n        index = np.argmin(\n            np.abs(np.array(list(range(0, 86400, 300))) - returnTime))\n        updatedreturnTime = str(dt.timedelta(\n            seconds=list(range(0, 86400, 300))[index]))\n        logger.warning(\n            'input time %s not divisible by 3 seconds, so next closest time %s '\n            'will be chosen', returnTime, updatedreturnTime\n        )\n        returnTime = updatedreturnTime\n\n    # get list of station zip files\n    inFile_df = pd.read_csv(inFile)\n    stationFiles = inFile_df['path'].to_list()\n    del inFile_df\n\n    if len(stationFiles) &gt; 0:\n        outputfiles = []\n        args = []\n        # parse delays from UNR\n        if gps_repo == 'UNR':\n            for sf in stationFiles:\n                StationID = os.path.basename(sf).split('.')[0]\n                name = os.path.join(pathbase, StationID + '_ztd.csv')\n                args.append((sf, name, dateList, returnTime))\n                outputfiles.append(name)\n            # Parallelize remote querying of zenith delays\n            with multiprocessing.Pool(numCPUs) as multipool:\n                multipool.starmap(get_delays_UNR, args)\n\n    # confirm file exists (i.e. valid delays exists for specified time/region).\n    outputfiles = [i for i in outputfiles if os.path.exists(i)]\n    # Consolidate all CSV files into one object\n    if outputfiles == []:\n        raise Exception('No valid delays found for specified time/region.')\n    name = os.path.join(outDir, '{}combinedGPS_ztd.csv'.format(gps_repo))\n    statsFile = pd.concat([pd.read_csv(i) for i in outputfiles])\n    # drop all duplicate lines\n    statsFile.drop_duplicates(inplace=True)\n    # Convert the above object into a csv file and export\n    statsFile.to_csv(name, index=False, encoding=\"utf-8\")\n    del statsFile\n\n    # Add lat/lon/height info\n    origstatsFile = pd.read_csv(inFile)\n    statsFile = pd.read_csv(name)\n    statsFile = pd.merge(left=statsFile, right=origstatsFile[['ID', 'Lat', 'Lon', 'Hgt_m']], how='left', left_on='ID', right_on='ID')\n    # drop all lines with nans and sort by station ID and year\n    statsFile.dropna(how='any', inplace=True)\n    # drop all duplicate lines\n    statsFile.drop_duplicates(inplace=True)\n    statsFile.sort_values(['ID', 'Date'])\n    statsFile.to_csv(name, index=False)\n    del origstatsFile, statsFile\n</code></pre>"},{"location":"reference/#RAiDER.getStationDelays.seconds_of_day","title":"<code>seconds_of_day(returnTime)</code>","text":"<p>Convert HH:MM:SS format time-tag to seconds of day.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/getStationDelays.py</code> <pre><code>def seconds_of_day(returnTime):\n'''\n    Convert HH:MM:SS format time-tag to seconds of day.\n    '''\n    if isinstance(returnTime, dt.time):\n        h, m, s = returnTime.hour, returnTime.minute, returnTime.second\n    else:\n        h, m, s = map(int, returnTime.split(\":\"))\n\n    return  h * 3600 + m * 60 + s\n</code></pre>"},{"location":"reference/#RAiDER.gnss","title":"<code>gnss</code>","text":""},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays","title":"<code>downloadGNSSDelays</code>","text":""},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.check_url","title":"<code>check_url(url)</code>","text":"<p>Check whether a file exists at a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def check_url(url):\n'''\n    Check whether a file exists at a URL. Modified from\n    https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n    '''\n    session = requests_retry_session()\n    r = session.head(url)\n    if r.status_code == 404:\n        url = ''\n    return url\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_UNR","title":"<code>download_UNR(statID, year, writeDir='.', download=False, baseURL=_UNR_URL)</code>","text":"<p>Download a zip file containing tropospheric delays for a given station and year The URL format is http://geodesy.unr.edu/gps_timeseries/trop//..trop.zip Inputs:     statID   - 4-character station identifier     year     - 4-numeral year Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def download_UNR(statID, year, writeDir='.', download=False, baseURL=_UNR_URL):\n'''\n    Download a zip file containing tropospheric delays for a given station and year\n    The URL format is http://geodesy.unr.edu/gps_timeseries/trop/&lt;ssss&gt;/&lt;ssss&gt;.&lt;yyyy&gt;.trop.zip\n    Inputs:\n        statID   - 4-character station identifier\n        year     - 4-numeral year\n    '''\n    URL = \"{0}gps_timeseries/trop/{1}/{1}.{2}.trop.zip\".format(\n        baseURL, statID.upper(), year)\n    logger.debug('Currently checking station %s in %s', statID, year)\n    if download:\n        saveLoc = os.path.abspath(os.path.join(\n            writeDir, '{0}.{1}.trop.zip'.format(statID.upper(), year)))\n        filepath = download_url(URL, saveLoc)\n    else:\n        filepath = check_url(URL)\n    return {'ID': statID, 'year': year, 'path': filepath}\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_tropo_delays","title":"<code>download_tropo_delays(stats, years, gps_repo=None, writeDir='.', numCPUs=8, download=False)</code>","text":"<p>Check for and download GNSS tropospheric delays from an archive. If download is True then files will be physically downloaded, which again is not necessary as data can be virtually accessed.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def download_tropo_delays(stats, years, gps_repo=None, writeDir='.', numCPUs=8, download=False):\n'''\n    Check for and download GNSS tropospheric delays from an archive. If download is True then\n    files will be physically downloaded, which again is not necessary as data can be virtually accessed.\n    '''\n\n    # argument checking\n    if not isinstance(stats, (list, str)):\n        raise TypeError('stats should be a string or a list of strings')\n    if not isinstance(years, (list, int)):\n        raise TypeError('years should be an int or a list of ints')\n\n    # Iterate over stations and years and check or download data\n    stat_year_tup = itertools.product(stats, years)\n    stat_year_tup = ((*tup, writeDir, download) for tup in stat_year_tup)\n    # Parallelize remote querying of station locations\n    with multiprocessing.Pool(numCPUs) as multipool:\n        # only record valid path\n        if gps_repo == 'UNR':\n            results = [\n                fileurl for fileurl in multipool.starmap(download_UNR, stat_year_tup)\n                if fileurl['path']\n            ]\n\n    # Write results to file\n    statDF = pd.DataFrame(results).set_index('ID')\n    statDF.to_csv(os.path.join(writeDir, '{}gnssStationList_overbbox_withpaths.csv'.format(gps_repo)))\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.download_url","title":"<code>download_url(url, save_path, chunk_size=2048)</code>","text":"<p>Download a file from a URL. Modified from https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def download_url(url, save_path, chunk_size=2048):\n'''\n    Download a file from a URL. Modified from\n    https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n    '''\n    session = requests_retry_session()\n    r = session.get(url, stream=True)\n\n    if r.status_code == 404:\n        return ''\n    else:\n        logger.debug('Beginning download of %s to %s', url, save_path)\n        with open(save_path, 'wb') as fd:\n            for chunk in r.iter_content(chunk_size=chunk_size):\n                fd.write(chunk)\n        logger.debug('Completed download of %s to %s', url, save_path)\n        return save_path\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.fix_lons","title":"<code>fix_lons(lon)</code>","text":"<p>Fix the given longitudes into the range <code>[-180, 180]</code>.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def fix_lons(lon):\n\"\"\"\n    Fix the given longitudes into the range ``[-180, 180]``.\n    \"\"\"\n    fixed_lon = ((lon + 180) % 360) - 180\n    # Make the positive 180s positive again.\n    if fixed_lon == -180 and lon &gt; 0:\n        fixed_lon *= -1\n    return fixed_lon\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_ID","title":"<code>get_ID(line)</code>","text":"<p>Pulls the station ID, lat, lon, and height for a given entry in the UNR text file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def get_ID(line):\n'''\n    Pulls the station ID, lat, lon, and height for a given entry in the UNR text file\n    '''\n    stat_id, lat, lon, height = line.split()[:4]\n    return stat_id, float(lat), float(lon), float(height)\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_station_list","title":"<code>get_station_list(bbox=None, writeLoc=None, userstatList=None, name_appendix='')</code>","text":"<p>Creates a list of stations inside a lat/lon bounding box from a source Inputs:     bbox    - length-4 list of floats that describes a bounding box. Format is               S N W E</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def get_station_list(bbox=None, writeLoc=None, userstatList=None, name_appendix=''):\n'''\n    Creates a list of stations inside a lat/lon bounding box from a source\n    Inputs:\n        bbox    - length-4 list of floats that describes a bounding box. Format is\n                  S N W E\n    '''\n    writeLoc = os.path.join(writeLoc or os.getcwd(), 'gnssStationList_overbbox' + name_appendix + '.csv')\n\n    if userstatList:\n        userstatList = read_text_file(userstatList)\n\n    statList = get_stats_by_llh(llhBox=bbox, userstatList=userstatList)\n\n    # write to file and pass final stations list\n    statList.to_csv(writeLoc, index=False)\n    stations = list(statList['ID'].values)\n\n    return stations, writeLoc\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.get_stats_by_llh","title":"<code>get_stats_by_llh(llhBox=None, baseURL=_UNR_URL, userstatList=None)</code>","text":"<p>Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox. llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2 define the upper right corner.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def get_stats_by_llh(llhBox=None, baseURL=_UNR_URL, userstatList=None):\n'''\n    Function to pull lat, lon, height, beginning date, end date, and number of solutions for stations inside the bounding box llhBox.\n    llhBox should be a tuple with format (lat1, lat2, lon1, lon2), where lat1, lon1 define the lower left-hand corner and lat2, lon2\n    define the upper right corner.\n    '''\n    if llhBox is None:\n        llhBox = [-90, 90, 0, 360]\n\n    stationHoldings = '{}NGLStationPages/DataHoldings.txt'.format(baseURL)\n    # it's a file like object and works just like a file\n\n    session = requests_retry_session()\n    data = session.get(stationHoldings)\n    stations = []\n    for ind, line in enumerate(data.text.splitlines()):  # files are iterable\n        if ind == 0:\n            continue\n        statID, lat, lon, height = get_ID(line)\n        # Only pass if in bbox\n        # And if user list of stations specified, only pass info for stations within list\n        if in_box(lat, lon, llhBox) and (not userstatList or statID in userstatList):\n            # convert lon into range [-180,180]\n            lon = fix_lons(lon)\n            stations.append({'ID': statID, 'Lat': lat, 'Lon': lon, 'Hgt_m': height})\n\n    logger.info('%d stations were found in %s', len(stations), llhBox)\n    stations = pd.DataFrame(stations)\n    # Report stations from user's list that do not cover bbox\n    if userstatList:\n        userstatList = [\n            i for i in userstatList if i not in stations['ID'].to_list()]\n        if userstatList:\n            logger.warning(\n                \"The following user-input stations are not covered by the input \"\n                \"bounding box %s: %s\",\n                str(llhBox).strip('[]'), str(userstatList).strip('[]')\n            )\n\n    return stations\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.in_box","title":"<code>in_box(lat, lon, llhbox)</code>","text":"<p>Checks whether the given lat, lon pair are inside the bounding box llhbox</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def in_box(lat, lon, llhbox):\n'''\n    Checks whether the given lat, lon pair are inside the bounding box llhbox\n    '''\n    return lat &lt; llhbox[1] and lat &gt; llhbox[0] and lon &lt; llhbox[3] and lon &gt; llhbox[2]\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.main","title":"<code>main(inps=None)</code>","text":"<p>Main workflow for querying supported GPS repositories for zenith delay information.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def main(inps=None):\n\"\"\"\n    Main workflow for querying supported GPS repositories for zenith delay information.\n    \"\"\"\n    try:\n        dateList     = inps.date_list\n        returnTime   = inps.time\n    except:\n        dateList     = inps.dateList\n        returnTime   = inps.returnTime\n\n    station_file = inps.station_file\n    bounding_box = inps.bounding_box\n    gps_repo     = inps.gps_repo\n    out          = inps.out\n    download     = inps.download\n    cpus         = inps.cpus\n    verbose      = inps.verbose\n\n    if verbose:\n        logger.setLevel(logging.DEBUG)\n\n    # Create specified output directory if it does not exist.\n    if not os.path.exists(out):\n        os.mkdir(out)\n\n    # Setup bounding box\n    if bounding_box:\n        if isinstance(bounding_box, str) and not os.path.isfile(bounding_box):\n            try:\n                bbox = [float(val) for val in bounding_box.split()]\n            except ValueError:\n                raise Exception(\n                    'Cannot understand the --bbox argument. String input is incorrect or path does not exist.')\n        elif isinstance(bounding_box, list):\n            bbox = bounding_box\n\n        else:\n            raise Exception('Passing a file with a bounding box not yet supported.')\n\n        long_cross_zero = 1 if bbox[2] * bbox[3] &lt; 0 else 0\n\n        # if necessary, convert negative longitudes to positive\n        if bbox[2] &lt; 0:\n            bbox[2] += 360\n\n        if bbox[3] &lt; 0:\n            bbox[3] += 360\n\n    # If bbox not specified, query stations across the entire globe\n    else:\n        bbox = [-90, 90, 0, 360]\n        long_cross_zero = 1\n\n    # Handle station query\n    if long_cross_zero == 1:\n        bbox1 = bbox.copy()\n        bbox2 = bbox.copy()\n        bbox1[3] = 360.0\n        bbox2[2] = 0.0\n        stats1, origstatsFile1 = get_station_list(bbox=bbox1, writeLoc=out, userstatList=station_file, name_appendix='_a')\n        stats2, origstatsFile2 = get_station_list(bbox=bbox2, writeLoc=out, userstatList=station_file, name_appendix='_b')\n        stats = stats1 + stats2\n        origstatsFile = origstatsFile1[:-6] + '.csv'\n        file_a = pd.read_csv(origstatsFile1)\n        file_b = pd.read_csv(origstatsFile2)\n        frames = [file_a, file_b]\n        result = pd.concat(frames, ignore_index=True)\n        result.to_csv(origstatsFile, index=False)\n    else:\n        if bbox[3] &lt; bbox[2]:\n            bbox[3] = 360.0\n        stats, origstatsFile = get_station_list(\n            bbox=bbox, writeLoc=out, userstatList=station_file)\n\n    # iterate over years\n    years = list(set([i.year for i in dateList]))\n    download_tropo_delays(\n        stats, years, gps_repo=gps_repo, writeDir=out, download=download\n    )\n\n    # Add lat/lon info\n    origstatsFile = pd.read_csv(origstatsFile)\n    statsFile = pd.read_csv(os.path.join(\n        out, '{}gnssStationList_overbbox_withpaths.csv'.format(gps_repo)))\n    statsFile = pd.merge(left=statsFile, right=origstatsFile,\n                         how='left', left_on='ID', right_on='ID')\n    statsFile.to_csv(os.path.join(\n        out, '{}gnssStationList_overbbox_withpaths.csv'.format(gps_repo)), index=False)\n    del origstatsFile, statsFile\n\n    # Extract delays for each station\n    dateList = [k.strftime('%Y-%m-%d') for k in dateList]\n    get_station_data(\n        os.path.join(out, '{}gnssStationList_overbbox_withpaths.csv'.format(gps_repo)),\n        dateList,\n        gps_repo=gps_repo,\n        numCPUs=cpus,\n        outDir=out,\n        returnTime=returnTime\n    )\n\n    logger.debug('Completed processing')\n</code></pre>"},{"location":"reference/#RAiDER.gnss.downloadGNSSDelays.read_text_file","title":"<code>read_text_file(filename)</code>","text":"<p>Read a list of GNSS station names from a plain text file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/downloadGNSSDelays.py</code> <pre><code>def read_text_file(filename):\n'''\n    Read a list of GNSS station names from a plain text file\n    '''\n    with open(filename, 'r') as f:\n        return [line.strip() for line in f]\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles","title":"<code>processDelayFiles</code>","text":""},{"location":"reference/#RAiDER.gnss.processDelayFiles.addDateTimeToFiles","title":"<code>addDateTimeToFiles(fileList, force=False, verbose=False)</code>","text":"<p>Run through a list of files and add the datetime of each file as a column</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def addDateTimeToFiles(fileList, force=False, verbose=False):\n''' Run through a list of files and add the datetime of each file as a column '''\n\n    print('Adding Datetime to delay files')\n\n    for f in tqdm(fileList):\n        data = pd.read_csv(f)\n\n        if 'Datetime' in data.columns and not force:\n            if verbose:\n                print(\n                    'File {} already has a \"Datetime\" column, pass'\n                    '\"force = True\" if you want to override and '\n                    're-process'.format(f)\n                )\n        else:\n            try:\n                dt = getDateTime(f)\n                data['Datetime'] = dt\n                # drop all lines with nans\n                data.dropna(how='any', inplace=True)\n                # drop all duplicate lines\n                data.drop_duplicates(inplace=True)\n                data.to_csv(f, index=False)\n            except (AttributeError, ValueError):\n                print(\n                    'File {} does not contain datetime info, skipping'\n                    .format(f)\n                )\n        del data\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.concatDelayFiles","title":"<code>concatDelayFiles(fileList, sort_list=['ID', 'Datetime'], return_df=False, outName=None, source='model', ref=None, col_name='ZTD')</code>","text":"<p>Read a list of .csv files containing the same columns and append them together, sorting by specified columns</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def concatDelayFiles(\n    fileList,\n    sort_list=['ID', 'Datetime'],\n    return_df=False,\n    outName=None,\n    source='model',\n    ref=None,\n    col_name='ZTD'\n):\n'''\n    Read a list of .csv files containing the same columns and append them\n    together, sorting by specified columns\n    '''\n    dfList = []\n\n    print('Concatenating delay files')\n\n    for f in tqdm(fileList):\n        if source == 'model':\n            dfList.append(pd.read_csv(f, parse_dates=['Datetime']))\n        else:\n            dfList.append(readZTDFile(f, col_name=col_name))\n    # drop lines not found in reference file\n    if ref:\n        dfr = pd.read_csv(ref, parse_dates=['Datetime'])\n        for i in enumerate(dfList):\n            dfList[i[0]] = pass_common_obs(dfr, i[1])\n        del dfr\n\n    df_c = pd.concat(\n        dfList,\n        ignore_index=True\n    ).drop_duplicates().reset_index(drop=True)\n    df_c.sort_values(by=sort_list, inplace=True)\n\n    print('Total number of rows in the concatenated file: {}'.format(df_c.shape[0]))\n    print('Total number of rows containing NaNs: {}'.format(\n        df_c[df_c.isna().any(axis=1)].shape[0]\n    )\n    )\n\n    if return_df or outName is None:\n        return df_c\n    else:\n        # drop all lines with nans\n        df_c.dropna(how='any', inplace=True)\n        # drop all duplicate lines\n        df_c.drop_duplicates(inplace=True)\n        df_c.to_csv(outName, index=False)\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.create_parser","title":"<code>create_parser()</code>","text":"<p>Parse command line arguments using argparse.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def create_parser():\n\"\"\"Parse command line arguments using argparse.\"\"\"\n    p = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=dedent(\"\"\"\\\n            Combine delay files from a weather model and GPS Zenith delays\n            Usage examples:\n            raiderCombine.py --raiderDir './*' --raider 'combined_raider_delays.csv'\n            raiderCombine.py  --raiderDir ERA5/ --raider ERA5_combined_delays.csv --raider_column totalDelay --gnssDir GNSS/ --gnss UNRCombined_gnss.csv --column ZTD -o Combined_delays.csv\n            raiderCombine.py  --raiderDir ERA5_2019/ --raider ERA5_combined_delays_2019.csv --raider_column totalDelay --gnssDir GNSS_2019/ --gnss UNRCombined_gnss_2019.csv --column ZTD -o Combined_delays_2019_UTTC18.csv --localtime '18:00:00 1'\n            \"\"\")\n    )\n\n    p.add_argument(\n        '--raider', dest='raider_file',\n        help=dedent(\"\"\"\\\n            .csv file containing RAiDER-derived Zenith Delays.\n            Should contain columns \"ID\" and \"Datetime\" in addition to the delay column\n            If the file does not exist, I will attempt to create it from a directory of\n            delay files.\n            \"\"\"),\n        required=True\n    )\n    p.add_argument(\n        '--raiderDir', '-d', dest='raider_folder',\n        help=dedent(\"\"\"\\\n            Directory containing RAiDER-derived Zenith Delay files.\n            Files should be named with a Datetime in the name and contain the\n            column \"ID\" as the delay column names.\n            \"\"\"),\n        default=os.getcwd()\n    )\n    p.add_argument(\n        '--gnssDir', '-gd', dest='gnss_folder',\n        help=dedent(\"\"\"\\\n            Directory containing GNSS-derived Zenith Delay files.\n            Files should contain the column \"ID\" as the delay column names\n            and times should be denoted by the \"Date\" key.\n            \"\"\"),\n        default=os.getcwd()\n    )\n\n    p.add_argument(\n        '--gnss', dest='gnss_file',\n        help=dedent(\"\"\"\\\n            Optional .csv file containing GPS Zenith Delays. Should contain columns \"ID\", \"ZTD\", and \"Datetime\"\n            \"\"\"),\n        default=None\n    )\n\n    p.add_argument(\n        '--raider_column',\n        '-r',\n        dest='raider_column_name',\n        help=dedent(\"\"\"\\\n            Name of the column containing RAiDER delays. Only used with the \"--gnss\" option\n            \"\"\"),\n        default='totalDelay'\n    )\n    p.add_argument(\n        '--column',\n        '-c',\n        dest='column_name',\n        help=dedent(\"\"\"\\\n            Name of the column containing GPS Zenith delays. Only used with the \"--gnss\" option\n\n            \"\"\"),\n        default='ZTD'\n    )\n\n    p.add_argument(\n        '--out',\n        '-o',\n        dest='out_name',\n        help=dedent(\"\"\"\\\n            Name to use for the combined delay file. Only used with the \"--gnss\" option\n\n            \"\"\"),\n        default='Combined_delays.csv'\n    )\n\n    p.add_argument(\n        '--localtime',\n        '-lt',\n        dest='local_time',\n        help=dedent(\"\"\"\\\n            \"Optional control to pass only data at local-time (in integer hours) WRT user-defined time at 0 longitude (1st argument),\n             and within +/- specified hour threshold (2nd argument).\n             By default UTC is passed as is without local-time conversions.\n             Input in 'HH H', e.g. '16 1'\"\n\n            \"\"\"),\n        default=None\n    )\n\n    return p\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.getDateTime","title":"<code>getDateTime(filename)</code>","text":"<p>Parse a datetime from a RAiDER delay filename</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def getDateTime(filename):\n''' Parse a datetime from a RAiDER delay filename '''\n    filename = os.path.basename(filename)\n    dtr = re.compile(r'\\d{8}T\\d{6}')\n    dt = dtr.search(filename)\n    return datetime.datetime.strptime(\n        dt.group(),\n        '%Y%m%dT%H%M%S'\n    )\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.local_time_filter","title":"<code>local_time_filter(raiderFile, ztdFile, dfr, dfz, localTime)</code>","text":"<p>Convert to local-time reference frame WRT 0 longitude</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def local_time_filter(raiderFile, ztdFile, dfr, dfz, localTime):\n'''\n    Convert to local-time reference frame WRT 0 longitude\n    '''\n    localTime_hrs = int(localTime.split(' ')[0])\n    localTime_hrthreshold = int(localTime.split(' ')[1])\n    # with rotation rate and distance to 0 lon, get localtime shift WRT 00 UTC at 0 lon\n    # *rotation rate at given point = (360deg/23.9333333333hr) = 15.041782729825965 deg/hr\n    dfr['Localtime'] = (dfr['Lon'] / 15.041782729825965)\n    dfz['Localtime'] = (dfz['Lon'] / 15.041782729825965)\n\n    # estimate local-times\n    dfr['Localtime'] = dfr.apply(lambda r: update_time(r, localTime_hrs),\n                                 axis=1)\n    dfz['Localtime'] = dfz.apply(lambda r: update_time(r, localTime_hrs),\n                                 axis=1)\n\n    # filter out data outside of --localtime hour threshold\n    dfr['Localtime_u'] = dfr['Localtime'] + \\\n        datetime.timedelta(hours=localTime_hrthreshold)\n    dfr['Localtime_l'] = dfr['Localtime'] - \\\n        datetime.timedelta(hours=localTime_hrthreshold)\n    OG_total = dfr.shape[0]\n    dfr = dfr[(dfr['Datetime'] &gt;= dfr['Localtime_l']) &amp;\n              (dfr['Datetime'] &lt;= dfr['Localtime_u'])]\n\n    # only keep observation closest to Localtime\n    print('Total number of datapoints dropped in {} for not being within '\n          '{} hrs of specified local-time {}: {} out of {}'.format(\n              raiderFile, localTime.split(' ')[1], localTime.split(' ')[0],\n              dfr.shape[0], OG_total))\n    dfz['Localtime_u'] = dfz['Localtime'] + \\\n        datetime.timedelta(hours=localTime_hrthreshold)\n    dfz['Localtime_l'] = dfz['Localtime'] - \\\n        datetime.timedelta(hours=localTime_hrthreshold)\n    OG_total = dfz.shape[0]\n    dfz = dfz[(dfz['Datetime'] &gt;= dfz['Localtime_l']) &amp;\n              (dfz['Datetime'] &lt;= dfz['Localtime_u'])]\n    # only keep observation closest to Localtime\n    print('Total number of datapoints dropped in {} for not being within '\n          '{} hrs of specified local-time {}: {} out of {}'.format(\n              ztdFile, localTime.split(' ')[1], localTime.split(' ')[0],\n              dfz.shape[0], OG_total))\n\n    # drop all lines with nans\n    dfr.dropna(how='any', inplace=True)\n    dfz.dropna(how='any', inplace=True)\n    # drop all duplicate lines\n    dfr.drop_duplicates(inplace=True)\n    dfz.drop_duplicates(inplace=True)\n    # drop and rename columns\n    dfr.drop(columns=['Localtime_l', 'Localtime_u'], inplace=True)\n    dfz.drop(columns=['Localtime_l', 'Localtime_u'], inplace=True)\n\n    return dfr, dfz\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.main","title":"<code>main(raiderFile, ztdFile, col_name='ZTD', raider_delay='totalDelay', outName=None, localTime=None)</code>","text":"<p>Merge a combined RAiDER delays file with a GPS ZTD delay file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def main(raiderFile, ztdFile, col_name='ZTD', raider_delay='totalDelay', outName=None, localTime=None):\n'''\n    Merge a combined RAiDER delays file with a GPS ZTD delay file\n    '''\n    print('Merging delay files {} and {}'.format(raiderFile, ztdFile))\n    dfr = pd.read_csv(raiderFile, parse_dates=['Datetime'])\n    # drop extra columns\n    expected_data_columns = ['ID', 'Lat', 'Lon', 'Hgt_m', 'Datetime', 'wetDelay',\n                             'hydroDelay', raider_delay]\n    dfr = dfr.drop(columns=[col for col in dfr if col not in\n                            expected_data_columns])\n    dfz = pd.read_csv(ztdFile, parse_dates=['Date'])\n    if not 'Datetime' in dfz.keys():\n        dfz.rename(columns={'Date': 'Datetime'}, inplace=True)\n    # drop extra columns\n    expected_data_columns = ['ID', 'Datetime', 'wet_delay', 'hydrostatic_delay',\n                             'times', 'sigZTD', 'Lat', 'Lon', 'Hgt_m',\n                             col_name]\n    dfz = dfz.drop(columns=[col for col in dfz if col not in\n                            expected_data_columns])\n    # only pass common locations and times\n    dfz = pass_common_obs(dfr, dfz)\n    dfr = pass_common_obs(dfz, dfr)\n\n    # If specified, convert to local-time reference frame WRT 0 longitude\n    common_keys = ['Datetime', 'ID']\n    if localTime is not None:\n        dfr, dfz = local_time_filter(raiderFile, ztdFile, dfr, dfz, localTime)\n        common_keys.append('Localtime')\n        # only pass common locations and times\n        dfz = pass_common_obs(dfr, dfz, localtime='Localtime')\n        dfr = pass_common_obs(dfz, dfr, localtime='Localtime')\n\n    # drop all lines with nans\n    dfr.dropna(how='any', inplace=True)\n    dfz.dropna(how='any', inplace=True)\n    # drop all duplicate lines\n    dfr.drop_duplicates(inplace=True)\n    dfz.drop_duplicates(inplace=True)\n\n    print('Beginning merge')\n\n    dfc = dfr.merge(\n        dfz[common_keys + ['ZTD', 'sigZTD']],\n        how='left',\n        left_on=common_keys,\n        right_on=common_keys,\n        sort=True\n    )\n\n    # only keep observation closest to Localtime\n    if 'Localtime' in dfc.keys():\n        dfc['Localtimediff'] = abs((dfc['Datetime'] -\n                                    dfc['Localtime']).dt.total_seconds() / 3600)\n        dfc = dfc.loc[dfc.groupby(['ID', 'Localtime']).Localtimediff.idxmin()\n                      ].reset_index(drop=True)\n        dfc.drop(columns=['Localtimediff'], inplace=True)\n\n    # estimate residual\n    dfc['ZTD_minus_RAiDER'] = dfc['ZTD'] - dfc[raider_delay]\n\n    print('Total number of rows in the concatenated file: '\n          '{}'.format(dfc.shape[0]))\n    print('Total number of rows containing NaNs: {}'.format(\n        dfc[dfc.isna().any(axis=1)].shape[0]\n    )\n    )\n    print('Merge finished')\n\n    if outName is None:\n        return dfc\n    else:\n        # drop all lines with nans\n        dfc.dropna(how='any', inplace=True)\n        # drop all duplicate lines\n        dfc.drop_duplicates(inplace=True)\n        dfc.to_csv(outName, index=False)\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.pass_common_obs","title":"<code>pass_common_obs(reference, target, localtime=None)</code>","text":"<p>Pass only observations in target spatiotemporally common to reference</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def pass_common_obs(reference, target, localtime=None):\n'''Pass only observations in target spatiotemporally common to reference'''\n    if isinstance(target['Datetime'].iloc[0], str):\n        target['Datetime'] = target['Datetime'].apply(lambda x:\n                          datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n    if localtime:\n        return target[target['Datetime'].dt.date.isin(reference['Datetime']\n                      .dt.date) &amp;\n                      target['ID'].isin(reference['ID']) &amp;\n                      target[localtime].isin(reference[localtime])]\n    else:\n        return target[target['Datetime'].dt.date.isin(reference['Datetime']\n                      .dt.date) &amp;\n                      target['ID'].isin(reference['ID'])]\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.readZTDFile","title":"<code>readZTDFile(filename, col_name='ZTD')</code>","text":"<p>Read and parse a GPS zenith delay file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def readZTDFile(filename, col_name='ZTD'):\n'''\n    Read and parse a GPS zenith delay file\n    '''\n    try:\n        data = pd.read_csv(filename, parse_dates=['Date'])\n        times = data['times'].apply(lambda x: datetime.timedelta(seconds=x))\n        data['Datetime'] = data['Date'] + times\n    except (KeyError, ValueError):\n        data = pd.read_csv(filename, parse_dates=['Datetime'])\n\n    data.rename(columns={col_name: 'ZTD'}, inplace=True)\n    return data\n</code></pre>"},{"location":"reference/#RAiDER.gnss.processDelayFiles.update_time","title":"<code>update_time(row, localTime_hrs)</code>","text":"<p>Update with local origin time</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/gnss/processDelayFiles.py</code> <pre><code>def update_time(row, localTime_hrs):\n'''Update with local origin time'''\n    localTime_estimate = row['Datetime'].replace(hour=localTime_hrs,\n                                                 minute=0, second=0)\n    # determine if you need to shift days\n    time_shift = datetime.timedelta(days=0)\n    # round to nearest hour\n    days_diff = (row['Datetime'] -\n                 datetime.timedelta(seconds=math.floor(\n                     row['Localtime']) * 3600)).day - \\\n        localTime_estimate.day\n    # if lon &lt;0, check if you need to add day\n    if row['Lon'] &lt; 0:\n        # add day\n        if days_diff != 0:\n            time_shift = datetime.timedelta(days=1)\n    # if lon &gt;0, check if you need to subtract day\n    if row['Lon'] &gt; 0:\n        # subtract day\n        if days_diff != 0:\n            time_shift = -datetime.timedelta(days=1)\n    return localTime_estimate + datetime.timedelta(seconds=row['Localtime']\n                                                   * 3600) + time_shift\n</code></pre>"},{"location":"reference/#RAiDER.interpolate","title":"<code>interpolate</code>","text":"<p>Fast linear interpolator over a regular grid</p>"},{"location":"reference/#RAiDER.interpolate.__doc__","title":"<code>__doc__ = 'Fast linear interpolator over a regular grid'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.interpolate.__file__","title":"<code>__file__ = '/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolate.cpython-310-x86_64-linux-gnu.so'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.interpolate.__name__","title":"<code>__name__ = 'RAiDER.interpolate'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.interpolate.__package__","title":"<code>__package__ = 'RAiDER'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.interpolate.interpolate","title":"<code>interpolate()</code>  <code>builtin</code>","text":"<p>interpolate(points: List[numpy.ndarray[numpy.float64]], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -&gt; numpy.ndarray[numpy.float64]</p> <p>Linear interpolator in any dimension. Arguments are similar to scipy.interpolate.RegularGridInterpolator</p> <p>:param points: Tuple of N axis coordinates specifying the grid. :param values: Nd array containing the grid point values. :param interp_points: List of points to interpolate, should have     dimension (x, N). If this list is guaranteed to be sorted make sure     to use the <code>assume_sorted</code> option. :param fill_value: The value to return for interpolation points       outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation     points is sorted. :param max_threads: Limit the number of threads to a certain amount.     Note: The number of threads will always be one of {1, 2, 4, 8}</p>"},{"location":"reference/#RAiDER.interpolate.interpolate_along_axis","title":"<code>interpolate_along_axis()</code>  <code>builtin</code>","text":"<p>interpolate_along_axis(points: numpy.ndarray[numpy.float64], values: numpy.ndarray[numpy.float64], interp_points: numpy.ndarray[numpy.float64], axis: int = -1, fill_value: Optional[float] = None, assume_sorted: bool = False, max_threads: int = 8) -&gt; numpy.ndarray[numpy.float64]</p> <p>1D linear interpolator along a specific axis.</p> <p>:param points: N-dimensional x coordinates. Axis specified by       'axis' must contain at least 2 points. :param values: N-dimensional y values. Must have the same shape as       'points'. :param interp_points: N-dimensional x coordinates to interpolate at.       The shape may only differ from that of 'points' at the axis       specified by 'axis'. For example if 'points' has shape (1, 2, 3)       and 'axis' is 2, then and shape like (1, 2, X) is valid. :param axis: The axis to interpolate along. :param fill_value: The value to return for interpolation points       outside of the grid range. :param assume_sorted: Enable optimization when the list of interpolation       points is sorted along the axis of interpolation. :param max_threads: Limit the number of threads to a certain amount.</p>"},{"location":"reference/#RAiDER.interpolator","title":"<code>interpolator</code>","text":""},{"location":"reference/#RAiDER.interpolator.RegularGridInterpolator","title":"<code>RegularGridInterpolator</code>","text":"<p>             Bases: <code>object</code></p> <p>Provides a wrapper around RAiDER.interpolate.interpolate with a similar interface to scipy.interpolate.RegularGridInterpolator.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>class RegularGridInterpolator(object):\n\"\"\"\n    Provides a wrapper around RAiDER.interpolate.interpolate with a similar\n    interface to scipy.interpolate.RegularGridInterpolator.\n    \"\"\"\n\n    def __init__(\n        self,\n        grid,\n        values,\n        fill_value=None,\n        assume_sorted=False,\n        max_threads=8\n    ):\n        self.grid = grid\n        self.values = values\n        self.fill_value = fill_value\n        self.assume_sorted = assume_sorted\n        self.max_threads = max_threads\n\n    def __call__(self, points):\n        if isinstance(points, tuple):\n            shape = points[0].shape\n            for arr in points:\n                assert arr.shape == shape, \"All dimensions must contain the same number of points!\"\n            interp_points = np.stack(points, axis=-1)\n            in_shape = interp_points.shape\n        elif points.ndim &gt; 2:\n            in_shape = points.shape\n            interp_points = points.reshape((np.prod(points.shape[:-1]),) + (points.shape[-1],))\n        else:\n            interp_points = points\n            in_shape = interp_points.shape\n\n        out = interpolate(\n            self.grid,\n            self.values,\n            interp_points,\n            fill_value=self.fill_value,\n            assume_sorted=self.assume_sorted,\n            max_threads=self.max_threads\n        )\n        return out.reshape(in_shape[:-1])\n</code></pre>"},{"location":"reference/#RAiDER.interpolator.fillna3D","title":"<code>fillna3D(array, axis=-1, fill_value=0.0)</code>","text":"<p>This function fills in NaNs in 3D arrays, specifically using the nearest non-nan value for \"low\" NaNs and 0s for \"high\" NaNs. </p> <p>Returns:</p> Type Description <p>3D array with low NaNs filled as nearest neighbors and high NaNs filled as 0s</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>def fillna3D(array, axis=-1, fill_value=0.):\n'''\n    This function fills in NaNs in 3D arrays, specifically using the nearest non-nan value\n    for \"low\" NaNs and 0s for \"high\" NaNs. \n\n    Arguments: \n        array   - 3D array, where the last axis is the \"z\" dimension\n\n    Returns: \n        3D array with low NaNs filled as nearest neighbors and high NaNs filled as 0s\n    '''\n\n    # fill lower NaNs with nearest neighbor\n    narr = np.moveaxis(array, axis, -1)\n    nars = narr.reshape((np.prod(narr.shape[:-1]),) + (narr.shape[-1],))\n    dfd = pd.DataFrame(data=nars).interpolate(axis=1, limit_direction='backward')\n    out = dfd.values.reshape(array.shape)\n\n    # fill upper NaNs with 0s\n    outmat = np.moveaxis(out, -1, axis)\n    outmat[np.isnan(outmat)] = fill_value\n    return outmat\n</code></pre>"},{"location":"reference/#RAiDER.interpolator.interpV","title":"<code>interpV(y, old_x, new_x, left=None, right=None, period=None)</code>","text":"<p>Rearrange np.interp's arguments</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>def interpV(y, old_x, new_x, left=None, right=None, period=None):\n'''\n    Rearrange np.interp's arguments\n    '''\n    return np.interp(new_x, old_x, y, left=left, right=right, period=period)\n</code></pre>"},{"location":"reference/#RAiDER.interpolator.interpVector","title":"<code>interpVector(vec, Nx)</code>","text":"<p>Interpolate data from a single vector containing the original x, the original y, and the new x, in that order. Nx tells the number of original x-points.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>def interpVector(vec, Nx):\n'''\n    Interpolate data from a single vector containing the original\n    x, the original y, and the new x, in that order. Nx tells the\n    number of original x-points.\n    '''\n    x = vec[:Nx]\n    y = vec[Nx:2 * Nx]\n    xnew = vec[2 * Nx:]\n    f = interp1d(x, y, bounds_error=False, copy=False, assume_sorted=True)\n    return f(xnew)\n</code></pre>"},{"location":"reference/#RAiDER.interpolator.interp_along_axis","title":"<code>interp_along_axis(oldCoord, newCoord, data, axis=2, pad=False)</code>","text":"<p>DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is much faster). This function now primarily exists to verify the behavior of the new one.</p> <p>Interpolate an array of 3-D data along one axis. This function assumes that the x-coordinate increases monotonically.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>def interp_along_axis(oldCoord, newCoord, data, axis=2, pad=False):\n'''\n    DEPRECATED: Use RAiDER.interpolate.interpolate_along_axis instead (it is\n    much faster). This function now primarily exists to verify the behavior of\n    the new one.\n\n    Interpolate an array of 3-D data along one axis. This function\n    assumes that the x-coordinate increases monotonically.\n    '''\n    if oldCoord.ndim &gt; 1:\n        stackedData = np.concatenate([oldCoord, data, newCoord], axis=axis)\n        out = np.apply_along_axis(interpVector, axis=axis, arr=stackedData, Nx=oldCoord.shape[axis])\n    else:\n        out = np.apply_along_axis(interpV, axis=axis, arr=data, old_x=oldCoord, new_x=newCoord,\n                                  left=np.nan, right=np.nan)\n\n    return out\n</code></pre>"},{"location":"reference/#RAiDER.interpolator.interpolateDEM","title":"<code>interpolateDEM(demFile, outLL, method='nearest')</code>","text":"<p>Interpolate a DEM raster to a set of lat/lon query points using rioxarray</p> <p>outLL will be a tuple of (lats, lons). lats/lons can either be 1D arrays or 2     For now will only use first row/col of 2D</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/interpolator.py</code> <pre><code>def interpolateDEM(demFile, outLL, method='nearest'):\n\"\"\" Interpolate a DEM raster to a set of lat/lon query points using rioxarray\n\n    outLL will be a tuple of (lats, lons). lats/lons can either be 1D arrays or 2\n        For now will only use first row/col of 2D\n    \"\"\"\n    import rioxarray as xrr\n    da_dem     = xrr.open_rasterio(demFile, band_as_variable=True)['band_1']\n    lats, lons = outLL\n    lats  = lats[:, 0] if lats.ndim==2 else lats\n    lons  = lons[0, :] if lons.ndim==2 else lons\n    z_out = da_dem.interp(y=np.sort(lats)[::-1], x=lons).data\n    return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader","title":"<code>llreader</code>","text":""},{"location":"reference/#RAiDER.llreader.AOI","title":"<code>AOI</code>","text":"<p>             Bases: <code>object</code></p> <p>This instantiates a generic AOI class object.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class AOI(object):\n'''\n    This instantiates a generic AOI class object.\n\n    Attributes:\n       _bounding_box    - S N W E bounding box\n       _proj            - pyproj-compatible CRS\n       _type            - Type of AOI\n    '''\n    def __init__(self):\n        self._output_directory = os.getcwd()\n        self._bounding_box   = None\n        self._proj           = CRS.from_epsg(4326)\n        self._geotransform   = None\n        self._cube_spacing_m = None\n\n\n    def type(self):\n        return self._type\n\n\n    def bounds(self):\n        return list(self._bounding_box).copy()\n\n\n    def geotransform(self):\n        return self._geotransform\n\n\n    def projection(self):\n        return self._proj\n\n\n    def get_output_spacing(self, crs=4326):\n\"\"\" Return the output spacing in desired units \"\"\"\n        output_spacing_deg = self._output_spacing\n        if not isinstance(crs, CRS):\n            crs = CRS.from_epsg(crs)\n\n        ## convert it to meters users wants a projected coordinate system\n        if all(axis_info.unit_name == 'degree' for axis_info in crs.axis_info):\n            output_spacing = output_spacing_deg\n        else:\n            output_spacing = output_spacing_deg*1e5\n\n        return output_spacing\n\n\n    def set_output_spacing(self, ll_res=None):\n\"\"\" Calculate the spacing for the output grid and weather model\n\n        Use the requested spacing if exists or the weather model grid itself\n\n        Returns:\n            None. Sets self._output_spacing\n        \"\"\"\n        assert ll_res or self._cube_spacing_m, \\\n            'Must pass lat/lon resolution if _cube_spacing_m is None'\n\n        out_spacing = self._cube_spacing_m / 1e5  \\\n            if self._cube_spacing_m else ll_res\n\n        logger.debug(f'Output cube spacing: {out_spacing} degrees')\n        self._output_spacing = out_spacing\n\n\n    def add_buffer(self, ll_res, digits=2):\n\"\"\"\n        Add a fixed buffer to the AOI, accounting for the cube spacing.\n\n        Ensures cube is slighly larger than requested area.\n        The AOI will always be in EPSG:4326\n        Args:\n            ll_res          - weather model lat/lon resolution\n            digits          - number of decimal digits to include in the output\n\n        Returns:\n            None. Updates self._bounding_box\n\n        Example:\n        &gt;&gt;&gt; from RAiDER.models.hrrr import HRRR\n        &gt;&gt;&gt; from RAiDER.llreader import BoundingBox\n        &gt;&gt;&gt; wm = HRRR()\n        &gt;&gt;&gt; aoi = BoundingBox([37, 38, -92, -91])\n        &gt;&gt;&gt; aoi.add_buffer(buffer = 1.5 * wm.getLLRes())\n        &gt;&gt;&gt; aoi.bounds()\n         [36.93, 38.07, -92.07, -90.93]\n        \"\"\"\n        from RAiDER.utilFcns import clip_bbox\n\n        ## add an extra buffer around the user specified region\n        S, N, W, E = self.bounds()\n        buffer     = (1.5 * ll_res)\n        S, N = np.max([S-buffer, -90]),  np.min([N+buffer, 90])\n        W, E = W-buffer, E+buffer # TODO: handle dateline crossings\n\n        ## clip the buffered region to a multiple of the spacing\n        self.set_output_spacing(ll_res)\n        S, N, W, E  = clip_bbox([S,N,W,E], self._output_spacing)\n\n        if np.max([np.abs(W), np.abs(E)]) &gt; 180:\n            logger.warning('Bounds extend past +/- 180. Results may be incorrect.')\n\n        self._bounding_box = [np.round(a, digits) for a in (S, N, W, E)]\n\n        return\n\n\n    def calc_buffer_ray(self, direction, lookDir='right', incAngle=30, maxZ=80, digits=2):\n\"\"\"\n        Calculate the buffer for ray tracing. This only needs to be done in the east-west\n        direction due to satellite orbits, and only needs extended on the side closest to\n        the sensor.\n\n        Args:\n            lookDir (str)      - Sensor look direction, can be \"right\" or \"left\"\n            losAngle (float)   - Incidence angle in degrees\n            maxZ (float)       - maximum integration elevation in km\n        \"\"\"\n        direction = direction.lower()\n        # for isce object\n        try:\n            lookDir = lookDir.name.lower()\n        except AttributeError:\n            lookDir = lookDir.lower()\n\n        assert direction in 'asc desc'.split(), \\\n            f'Incorrection orbital direction: {direction}. Choose asc or desc.'\n        assert lookDir in 'right light'.split(), \\\n            f'Incorrection look direction: {lookDir}. Choose right or left.'\n\n        S, N, W, E = self.bounds()\n\n        # use a small look angle to calculate near range\n        lat_max = np.max([np.abs(S), np.abs(N)])\n        near    = maxZ * np.tan(np.deg2rad(incAngle))\n        buffer  = near / (np.cos(np.deg2rad(lat_max)) * 100)\n\n        # buffer on the side nearest the sensor\n        if ((lookDir == 'right') and (direction == 'asc')) or ((lookDir == 'left') and (direction == 'desc')):\n            W = W - buffer\n        else:\n            E = E + buffer\n\n        bounds = [np.round(a, digits) for a in (S, N, W, E)]\n        if np.max([np.abs(W), np.abs(E)]) &gt; 180:\n            logger.warning('Bounds extend past +/- 180. Results may be incorrect.')\n        return bounds\n\n\n    def set_output_directory(self, output_directory):\n        self._output_directory = output_directory\n        return\n\n\n    def set_output_xygrid(self, dst_crs=4326):\n\"\"\" Define the locations where the delays will be returned \"\"\"\n        from RAiDER.utilFcns import transform_bbox\n\n        try:\n            out_proj = CRS.from_epsg(dst_crs.replace('EPSG:', ''))\n        except pyproj.exceptions.CRSError:\n            out_proj = dst_crs\n\n        out_snwe = transform_bbox(self.bounds(), src_crs=4326, dest_crs=out_proj)\n        logger.debug(f\"Output SNWE: {out_snwe}\")\n\n        # Build the output grid\n        out_spacing = self.get_output_spacing(out_proj)\n        self.xpts = np.arange(out_snwe[2], out_snwe[3] + out_spacing, out_spacing)\n        self.ypts = np.arange(out_snwe[1], out_snwe[0] - out_spacing, -out_spacing)\n        return\n</code></pre>"},{"location":"reference/#RAiDER.llreader.AOI.add_buffer","title":"<code>add_buffer(ll_res, digits=2)</code>","text":"<p>Add a fixed buffer to the AOI, accounting for the cube spacing.</p> <p>Ensures cube is slighly larger than requested area. The AOI will always be in EPSG:4326 Args:     ll_res          - weather model lat/lon resolution     digits          - number of decimal digits to include in the output</p> <p>Returns:</p> Type Description <p>None. Updates self._bounding_box</p> <p>Example:</p> <p>from RAiDER.models.hrrr import HRRR from RAiDER.llreader import BoundingBox wm = HRRR() aoi = BoundingBox([37, 38, -92, -91]) aoi.add_buffer(buffer = 1.5 * wm.getLLRes()) aoi.bounds()  [36.93, 38.07, -92.07, -90.93]</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def add_buffer(self, ll_res, digits=2):\n\"\"\"\n    Add a fixed buffer to the AOI, accounting for the cube spacing.\n\n    Ensures cube is slighly larger than requested area.\n    The AOI will always be in EPSG:4326\n    Args:\n        ll_res          - weather model lat/lon resolution\n        digits          - number of decimal digits to include in the output\n\n    Returns:\n        None. Updates self._bounding_box\n\n    Example:\n    &gt;&gt;&gt; from RAiDER.models.hrrr import HRRR\n    &gt;&gt;&gt; from RAiDER.llreader import BoundingBox\n    &gt;&gt;&gt; wm = HRRR()\n    &gt;&gt;&gt; aoi = BoundingBox([37, 38, -92, -91])\n    &gt;&gt;&gt; aoi.add_buffer(buffer = 1.5 * wm.getLLRes())\n    &gt;&gt;&gt; aoi.bounds()\n     [36.93, 38.07, -92.07, -90.93]\n    \"\"\"\n    from RAiDER.utilFcns import clip_bbox\n\n    ## add an extra buffer around the user specified region\n    S, N, W, E = self.bounds()\n    buffer     = (1.5 * ll_res)\n    S, N = np.max([S-buffer, -90]),  np.min([N+buffer, 90])\n    W, E = W-buffer, E+buffer # TODO: handle dateline crossings\n\n    ## clip the buffered region to a multiple of the spacing\n    self.set_output_spacing(ll_res)\n    S, N, W, E  = clip_bbox([S,N,W,E], self._output_spacing)\n\n    if np.max([np.abs(W), np.abs(E)]) &gt; 180:\n        logger.warning('Bounds extend past +/- 180. Results may be incorrect.')\n\n    self._bounding_box = [np.round(a, digits) for a in (S, N, W, E)]\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.llreader.AOI.calc_buffer_ray","title":"<code>calc_buffer_ray(direction, lookDir='right', incAngle=30, maxZ=80, digits=2)</code>","text":"<p>Calculate the buffer for ray tracing. This only needs to be done in the east-west direction due to satellite orbits, and only needs extended on the side closest to the sensor.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def calc_buffer_ray(self, direction, lookDir='right', incAngle=30, maxZ=80, digits=2):\n\"\"\"\n    Calculate the buffer for ray tracing. This only needs to be done in the east-west\n    direction due to satellite orbits, and only needs extended on the side closest to\n    the sensor.\n\n    Args:\n        lookDir (str)      - Sensor look direction, can be \"right\" or \"left\"\n        losAngle (float)   - Incidence angle in degrees\n        maxZ (float)       - maximum integration elevation in km\n    \"\"\"\n    direction = direction.lower()\n    # for isce object\n    try:\n        lookDir = lookDir.name.lower()\n    except AttributeError:\n        lookDir = lookDir.lower()\n\n    assert direction in 'asc desc'.split(), \\\n        f'Incorrection orbital direction: {direction}. Choose asc or desc.'\n    assert lookDir in 'right light'.split(), \\\n        f'Incorrection look direction: {lookDir}. Choose right or left.'\n\n    S, N, W, E = self.bounds()\n\n    # use a small look angle to calculate near range\n    lat_max = np.max([np.abs(S), np.abs(N)])\n    near    = maxZ * np.tan(np.deg2rad(incAngle))\n    buffer  = near / (np.cos(np.deg2rad(lat_max)) * 100)\n\n    # buffer on the side nearest the sensor\n    if ((lookDir == 'right') and (direction == 'asc')) or ((lookDir == 'left') and (direction == 'desc')):\n        W = W - buffer\n    else:\n        E = E + buffer\n\n    bounds = [np.round(a, digits) for a in (S, N, W, E)]\n    if np.max([np.abs(W), np.abs(E)]) &gt; 180:\n        logger.warning('Bounds extend past +/- 180. Results may be incorrect.')\n    return bounds\n</code></pre>"},{"location":"reference/#RAiDER.llreader.AOI.get_output_spacing","title":"<code>get_output_spacing(crs=4326)</code>","text":"<p>Return the output spacing in desired units</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def get_output_spacing(self, crs=4326):\n\"\"\" Return the output spacing in desired units \"\"\"\n    output_spacing_deg = self._output_spacing\n    if not isinstance(crs, CRS):\n        crs = CRS.from_epsg(crs)\n\n    ## convert it to meters users wants a projected coordinate system\n    if all(axis_info.unit_name == 'degree' for axis_info in crs.axis_info):\n        output_spacing = output_spacing_deg\n    else:\n        output_spacing = output_spacing_deg*1e5\n\n    return output_spacing\n</code></pre>"},{"location":"reference/#RAiDER.llreader.AOI.set_output_spacing","title":"<code>set_output_spacing(ll_res=None)</code>","text":"<p>Calculate the spacing for the output grid and weather model</p> <p>Use the requested spacing if exists or the weather model grid itself</p> <p>Returns:</p> Type Description <p>None. Sets self._output_spacing</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def set_output_spacing(self, ll_res=None):\n\"\"\" Calculate the spacing for the output grid and weather model\n\n    Use the requested spacing if exists or the weather model grid itself\n\n    Returns:\n        None. Sets self._output_spacing\n    \"\"\"\n    assert ll_res or self._cube_spacing_m, \\\n        'Must pass lat/lon resolution if _cube_spacing_m is None'\n\n    out_spacing = self._cube_spacing_m / 1e5  \\\n        if self._cube_spacing_m else ll_res\n\n    logger.debug(f'Output cube spacing: {out_spacing} degrees')\n    self._output_spacing = out_spacing\n</code></pre>"},{"location":"reference/#RAiDER.llreader.AOI.set_output_xygrid","title":"<code>set_output_xygrid(dst_crs=4326)</code>","text":"<p>Define the locations where the delays will be returned</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def set_output_xygrid(self, dst_crs=4326):\n\"\"\" Define the locations where the delays will be returned \"\"\"\n    from RAiDER.utilFcns import transform_bbox\n\n    try:\n        out_proj = CRS.from_epsg(dst_crs.replace('EPSG:', ''))\n    except pyproj.exceptions.CRSError:\n        out_proj = dst_crs\n\n    out_snwe = transform_bbox(self.bounds(), src_crs=4326, dest_crs=out_proj)\n    logger.debug(f\"Output SNWE: {out_snwe}\")\n\n    # Build the output grid\n    out_spacing = self.get_output_spacing(out_proj)\n    self.xpts = np.arange(out_snwe[2], out_snwe[3] + out_spacing, out_spacing)\n    self.ypts = np.arange(out_snwe[1], out_snwe[0] - out_spacing, -out_spacing)\n    return\n</code></pre>"},{"location":"reference/#RAiDER.llreader.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>             Bases: <code>AOI</code></p> <p>Parse a bounding box AOI</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class BoundingBox(AOI):\n'''Parse a bounding box AOI'''\n    def __init__(self, bbox):\n        AOI.__init__(self)\n        self._bounding_box = bbox\n        self._type = 'bounding_box'\n</code></pre>"},{"location":"reference/#RAiDER.llreader.GeocodedFile","title":"<code>GeocodedFile</code>","text":"<p>             Bases: <code>AOI</code></p> <p>Parse a Geocoded file for coordinates</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class GeocodedFile(AOI):\n'''Parse a Geocoded file for coordinates'''\n    def __init__(self, filename, is_dem=False):\n        super().__init__()\n\n        from RAiDER.utilFcns import rio_profile, rio_extents\n\n        self._filename     = filename\n        self.p             = rio_profile(filename)\n        self._bounding_box = rio_extents(self.p)\n        self._is_dem       = is_dem\n        _, self._proj, self._geotransform = rio_stats(filename)\n        self._type = 'geocoded_file'\n\n\n    def readLL(self):\n        # ll_bounds are SNWE\n        S, N, W, E = self._bounding_box\n        w, h = self.p['width'], self.p['height']\n        px   = (E - W) / w\n        py   = (N - S) / h\n        x = np.array([W + (t * px) for t in range(w)])\n        y = np.array([S + (t * py) for t in range(h)])\n        X, Y = np.meshgrid(x,y)\n        return Y, X # lats, lons\n\n\n    def readZ(self):\n'''\n        Download a DEM for the file\n        '''\n        from RAiDER.dem import download_dem\n        from RAiDER.interpolator import interpolateDEM\n\n        demFile = self._filename if self._is_dem else 'GLO30_fullres_dem.tif'\n        bbox    = self._bounding_box\n        _, _ = download_dem(bbox, writeDEM=True, outName=demFile)\n        z_out = interpolateDEM(demFile, self.readLL())\n\n        return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.GeocodedFile.__init__","title":"<code>__init__(filename, is_dem=False)</code>","text":"Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def __init__(self, filename, is_dem=False):\n    super().__init__()\n\n    from RAiDER.utilFcns import rio_profile, rio_extents\n\n    self._filename     = filename\n    self.p             = rio_profile(filename)\n    self._bounding_box = rio_extents(self.p)\n    self._is_dem       = is_dem\n    _, self._proj, self._geotransform = rio_stats(filename)\n    self._type = 'geocoded_file'\n</code></pre>"},{"location":"reference/#RAiDER.llreader.GeocodedFile.readZ","title":"<code>readZ()</code>","text":"<p>Download a DEM for the file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def readZ(self):\n'''\n    Download a DEM for the file\n    '''\n    from RAiDER.dem import download_dem\n    from RAiDER.interpolator import interpolateDEM\n\n    demFile = self._filename if self._is_dem else 'GLO30_fullres_dem.tif'\n    bbox    = self._bounding_box\n    _, _ = download_dem(bbox, writeDEM=True, outName=demFile)\n    z_out = interpolateDEM(demFile, self.readLL())\n\n    return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.Geocube","title":"<code>Geocube</code>","text":"<p>             Bases: <code>AOI</code></p> <p>Pull lat/lon/height from a georeferenced data cube</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class Geocube(AOI):\n\"\"\" Pull lat/lon/height from a georeferenced data cube \"\"\"\n    def __init__(self, path_cube):\n        super().__init__()\n        self.path  = path_cube\n        self._type = 'Geocube'\n        self._bounding_box = self.get_extent()\n        _, self._proj, self._geotransform = rio_stats(path_cube)\n\n    def get_extent(self):\n        with xarray.open_dataset(self.path) as ds:\n            S, N = ds.latitude.min().item(), ds.latitude.max().item()\n            W, E = ds.longitude.min().item(), ds.longitude.max().item()\n        return [S, N, W, E]\n\n\n    ## untested\n    def readLL(self):\n        with xarray.open_dataset(self.path) as ds:\n            lats = ds.latitutde.data()\n            lons = ds.longitude.data()\n        Lats, Lons = np.meshgrid(lats, lons)\n        return Lats, Lons\n\n    def readZ(self):\n        with xarray.open_dataset(self.path) as ds:\n            heights = ds.heights.data\n        return heights\n</code></pre>"},{"location":"reference/#RAiDER.llreader.RasterRDR","title":"<code>RasterRDR</code>","text":"<p>             Bases: <code>AOI</code></p> <p>Use a 2-band raster file containing lat/lon coordinates.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class RasterRDR(AOI):\n'''\n    Use a 2-band raster file containing lat/lon coordinates. \n    '''    \n    def __init__(self, lat_file, lon_file=None, hgt_file=None, dem_file=None, convention='isce'):\n        super().__init__()\n        self._type = 'radar_rasters'\n        self._latfile = lat_file\n        self._lonfile = lon_file\n\n        if (self._latfile is None) and (self._lonfile is None):\n            raise ValueError('You need to specify a 2-band file or two single-band files')\n\n        if not os.path.exists(self._latfile):\n            raise ValueError(f'{self._latfile} cannot be found!')\n\n        try:\n            bpg = bounds_from_latlon_rasters(lat_file, lon_file)\n            self._bounding_box, self._proj, self._geotransform = bpg\n        except Exception as e:\n            raise ValueError(f'Could not read lat/lon rasters: {e}')\n\n        # keep track of the height file, dem and convention\n        self._hgtfile = hgt_file\n        self._demfile = dem_file\n        self._convention = convention\n\n\n    def readLL(self):\n        # allow for 2-band lat/lon raster\n        lats = rio_open(self._latfile)\n\n        if self._lonfile is None:\n            return lats\n        else:\n            return lats, rio_open(self._lonfile)\n\n\n    def readZ(self):\n'''\n        Read the heights from the raster file, or download a DEM if not present\n        '''\n        if self._hgtfile is not None and os.path.exists(self._hgtfile):\n            logger.info('Using existing heights at: %s', self._hgtfile)\n            return rio_open(self._hgtfile)\n\n        else:\n            # Download the DEM\n            from RAiDER.dem import download_dem\n            from RAiDER.interpolator import interpolateDEM\n\n            demFile = os.path.join(self._output_directory, 'GLO30_fullres_dem.tif') \\\n                            if self._demfile is None else self._demfile\n\n            _, _ = download_dem(\n                self._bounding_box,\n                writeDEM=True,\n                outName=demFile,\n            )\n            z_out = interpolateDEM(demFile, self.readLL())\n\n            return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.RasterRDR.readZ","title":"<code>readZ()</code>","text":"<p>Read the heights from the raster file, or download a DEM if not present</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def readZ(self):\n'''\n    Read the heights from the raster file, or download a DEM if not present\n    '''\n    if self._hgtfile is not None and os.path.exists(self._hgtfile):\n        logger.info('Using existing heights at: %s', self._hgtfile)\n        return rio_open(self._hgtfile)\n\n    else:\n        # Download the DEM\n        from RAiDER.dem import download_dem\n        from RAiDER.interpolator import interpolateDEM\n\n        demFile = os.path.join(self._output_directory, 'GLO30_fullres_dem.tif') \\\n                        if self._demfile is None else self._demfile\n\n        _, _ = download_dem(\n            self._bounding_box,\n            writeDEM=True,\n            outName=demFile,\n        )\n        z_out = interpolateDEM(demFile, self.readLL())\n\n        return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.StationFile","title":"<code>StationFile</code>","text":"<p>             Bases: <code>AOI</code></p> <p>Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>class StationFile(AOI):\n'''Use a .csv file containing at least Lat, Lon, and optionally Hgt_m columns'''\n    def __init__(self, station_file, demFile=None):\n        super().__init__()\n        self._filename = station_file\n        self._demfile  = demFile\n        self._bounding_box = bounds_from_csv(station_file)\n        self._type = 'station_file'\n\n\n    def readLL(self):\n'''Read the station lat/lons from the csv file'''\n        df = pd.read_csv(self._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n        return df['Lat'].values, df['Lon'].values\n\n\n    def readZ(self):\n'''\n        Read the station heights from the file, or download a DEM if not present\n        '''\n        df = pd.read_csv(self._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n        if 'Hgt_m' in df.columns:\n            return df['Hgt_m'].values\n        else:\n            # Download the DEM\n            from RAiDER.dem import download_dem\n            from RAiDER.interpolator import interpolateDEM\n\n            demFile = os.path.join(self._output_directory, 'GLO30_fullres_dem.tif') \\\n                            if self._demfile is None else self._demfile\n\n            _, _ = download_dem(\n                self._bounding_box,\n                writeDEM=True,\n                outName=demFile,\n            )\n\n            ## interpolate the DEM to the query points\n            z_out0 = interpolateDEM(demFile, self.readLL())\n            if np.isnan(z_out0).all():\n                raise Exception('DEM interpolation failed. Check DEM bounds and station coords.')\n            z_out = np.diag(z_out0)  # the diagonal is the actual stations coordinates\n\n            # write the elevations to the file\n            df['Hgt_m'] = z_out\n            df.to_csv(self._filename, index=False)\n            self.__init__(self._filename)\n            return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.StationFile.readLL","title":"<code>readLL()</code>","text":"<p>Read the station lat/lons from the csv file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def readLL(self):\n'''Read the station lat/lons from the csv file'''\n    df = pd.read_csv(self._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n    return df['Lat'].values, df['Lon'].values\n</code></pre>"},{"location":"reference/#RAiDER.llreader.StationFile.readZ","title":"<code>readZ()</code>","text":"<p>Read the station heights from the file, or download a DEM if not present</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def readZ(self):\n'''\n    Read the station heights from the file, or download a DEM if not present\n    '''\n    df = pd.read_csv(self._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n    if 'Hgt_m' in df.columns:\n        return df['Hgt_m'].values\n    else:\n        # Download the DEM\n        from RAiDER.dem import download_dem\n        from RAiDER.interpolator import interpolateDEM\n\n        demFile = os.path.join(self._output_directory, 'GLO30_fullres_dem.tif') \\\n                        if self._demfile is None else self._demfile\n\n        _, _ = download_dem(\n            self._bounding_box,\n            writeDEM=True,\n            outName=demFile,\n        )\n\n        ## interpolate the DEM to the query points\n        z_out0 = interpolateDEM(demFile, self.readLL())\n        if np.isnan(z_out0).all():\n            raise Exception('DEM interpolation failed. Check DEM bounds and station coords.')\n        z_out = np.diag(z_out0)  # the diagonal is the actual stations coordinates\n\n        # write the elevations to the file\n        df['Hgt_m'] = z_out\n        df.to_csv(self._filename, index=False)\n        self.__init__(self._filename)\n        return z_out\n</code></pre>"},{"location":"reference/#RAiDER.llreader.bounds_from_csv","title":"<code>bounds_from_csv(station_file)</code>","text":"<p>station_file should be a comma-delimited file with at least \"Lat\" and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def bounds_from_csv(station_file):\n'''\n    station_file should be a comma-delimited file with at least \"Lat\"\n    and \"Lon\" columns, which should be EPSG: 4326 projection (i.e WGS84)\n    '''\n    stats = pd.read_csv(station_file).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n    if 'Hgt_m' in stats.columns:\n        use_csv_heights = True\n    snwe = [stats['Lat'].min(), stats['Lat'].max(), stats['Lon'].min(), stats['Lon'].max()]\n    return snwe\n</code></pre>"},{"location":"reference/#RAiDER.llreader.bounds_from_latlon_rasters","title":"<code>bounds_from_latlon_rasters(latfile, lonfile)</code>","text":"<p>Parse lat/lon/height inputs and return the appropriate outputs</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/llreader.py</code> <pre><code>def bounds_from_latlon_rasters(latfile, lonfile):\n'''\n    Parse lat/lon/height inputs and return\n    the appropriate outputs\n    '''\n    from RAiDER.utilFcns import get_file_and_band\n    latinfo = get_file_and_band(latfile)\n    loninfo = get_file_and_band(lonfile)\n    lat_stats, lat_proj, lat_gt = rio_stats(latinfo[0], band=latinfo[1])\n    lon_stats, lon_proj, lon_gt = rio_stats(loninfo[0], band=loninfo[1])\n\n    if lat_proj != lon_proj:\n        raise ValueError('Projection information for Latitude and Longitude files does not match')\n\n    if lat_gt != lon_gt:\n        raise ValueError('Affine transform for Latitude and Longitude files does not match')\n\n    # TODO - handle dateline crossing here\n    snwe = (lat_stats.min, lat_stats.max,\n            lon_stats.min, lon_stats.max)\n\n    if lat_proj is None:\n        logger.debug('Assuming lat/lon files are in EPSG:4326')\n        lat_proj = CRS.from_epsg(4326)\n\n    return snwe, lat_proj, lat_gt\n</code></pre>"},{"location":"reference/#RAiDER.logger","title":"<code>logger</code>","text":"<p>Global logging configuration</p>"},{"location":"reference/#RAiDER.logger.CustomFormatter","title":"<code>CustomFormatter</code>","text":"<p>             Bases: <code>UnixColorFormatter</code></p> <p>Adds levelname prefixes to the message on warning or above.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/logger.py</code> <pre><code>class CustomFormatter(UnixColorFormatter):\n\"\"\"Adds levelname prefixes to the message on warning or above.\"\"\"\n\n    def formatMessage(self, record):\n        message = super().formatMessage(record)\n        if record.levelno &gt;= logging.WARNING:\n            message = \": \".join((record.levelname, message))\n        return message\n</code></pre>"},{"location":"reference/#RAiDER.losreader","title":"<code>losreader</code>","text":""},{"location":"reference/#RAiDER.losreader.Conventional","title":"<code>Conventional</code>","text":"<p>             Bases: <code>LOS</code></p> <p>Special value indicating that the zenith delay will be projected using the standard cos(inc) scaling.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>class Conventional(LOS):\n\"\"\"\n    Special value indicating that the zenith delay will\n    be projected using the standard cos(inc) scaling.\n    \"\"\"\n    def __init__(self, filename=None, los_convention='isce', time=None, pad=600):\n        super().__init__()\n        self._file = filename\n        self._time = time\n        self._pad  = pad\n        self._is_projected = True\n        self._convention   = los_convention\n        if self._convention.lower() != 'isce':\n            raise NotImplementedError()\n\n\n    def __call__(self, delays):\n'''\n        Read the LOS file and convert it to look vectors\n        '''\n        if self._lats is None:\n            raise ValueError('Target points not set')\n        if self._file is None:\n            raise ValueError('LOS file not set')\n\n        try:\n            # if an ISCE-style los file is passed open it with GDAL\n            LOS_enu = inc_hd_to_enu(*rio_open(self._file))\n\n        except (OSError, TypeError):\n            # Otherwise, treat it as an orbit / statevector file\n            svs = np.stack(\n                get_sv(self._file, self._time, self._pad), axis=-1\n            )\n            LOS_enu = state_to_los(svs,\n                                   [self._lats, self._lons, self._heights],\n                                   )\n\n        if delays.shape == LOS_enu.shape:\n            return delays / LOS_enu\n        else:\n            return delays / LOS_enu[..., -1]\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Conventional.__call__","title":"<code>__call__(delays)</code>","text":"<p>Read the LOS file and convert it to look vectors</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def __call__(self, delays):\n'''\n    Read the LOS file and convert it to look vectors\n    '''\n    if self._lats is None:\n        raise ValueError('Target points not set')\n    if self._file is None:\n        raise ValueError('LOS file not set')\n\n    try:\n        # if an ISCE-style los file is passed open it with GDAL\n        LOS_enu = inc_hd_to_enu(*rio_open(self._file))\n\n    except (OSError, TypeError):\n        # Otherwise, treat it as an orbit / statevector file\n        svs = np.stack(\n            get_sv(self._file, self._time, self._pad), axis=-1\n        )\n        LOS_enu = state_to_los(svs,\n                               [self._lats, self._lons, self._heights],\n                               )\n\n    if delays.shape == LOS_enu.shape:\n        return delays / LOS_enu\n    else:\n        return delays / LOS_enu[..., -1]\n</code></pre>"},{"location":"reference/#RAiDER.losreader.LOS","title":"<code>LOS</code>","text":"<p>             Bases: <code>ABC</code></p> <p>LOS Class definition for handling look vectors</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>class LOS(ABC):\n'''\n    LOS Class definition for handling look vectors\n    '''\n    def __init__(self):\n        self._lats, self._lons, self._heights = None, None, None\n        self._look_vecs = None\n        self._ray_trace = False\n        self._is_zenith = False\n        self._is_projected = False\n\n\n    def setPoints(self, lats, lons=None, heights=None):\n'''Set the pixel locations'''\n        if (lats is None) and (self._lats is None):\n            raise RuntimeError(\"You haven't given any point locations yet\")\n\n        # Will overwrite points by default\n        if lons is None:\n            llh = lats  # assume points are [lats lons heights]\n            self._lats = llh[..., 0]\n            self._lons = llh[..., 1]\n            self._heights = llh[..., 2]\n        elif heights is None:\n            self._lats = lats\n            self._lons = lons\n            self._heights = np.zeros((len(lats), 1))\n        else:\n            self._lats = lats\n            self._lons = lons\n            self._heights = heights\n\n\n    def setTime(self, dt):\n        self._time = dt\n\n\n    def is_Zenith(self):\n        return self._is_zenith\n\n\n    def is_Projected(self):\n        return self._is_projected\n\n\n    def ray_trace(self):\n        return self._ray_trace\n</code></pre>"},{"location":"reference/#RAiDER.losreader.LOS.setPoints","title":"<code>setPoints(lats, lons=None, heights=None)</code>","text":"<p>Set the pixel locations</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def setPoints(self, lats, lons=None, heights=None):\n'''Set the pixel locations'''\n    if (lats is None) and (self._lats is None):\n        raise RuntimeError(\"You haven't given any point locations yet\")\n\n    # Will overwrite points by default\n    if lons is None:\n        llh = lats  # assume points are [lats lons heights]\n        self._lats = llh[..., 0]\n        self._lons = llh[..., 1]\n        self._heights = llh[..., 2]\n    elif heights is None:\n        self._lats = lats\n        self._lons = lons\n        self._heights = np.zeros((len(lats), 1))\n    else:\n        self._lats = lats\n        self._lons = lons\n        self._heights = heights\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing","title":"<code>Raytracing</code>","text":"<p>             Bases: <code>LOS</code></p> <p>Special value indicating that full raytracing will be used to calculate slant delays.</p> <p>Get unit look vectors pointing from the ground (target) pixels to the sensor, or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a file containing orbital statevectors.</p> <p>NOTE: These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR because they are in an ECEF reference frame instead of a local ENU. This is done because the construction of rays is done in ECEF rather than the local ENU.</p>"},{"location":"reference/#RAiDER.losreader.Raytracing--args","title":"Args:","text":"<p>time: python datetime                  - user-requested query time. Must be                                          compatible with the orbit file passed.                                          Only required for a statevector file. pad: int                               - integer number of seconds to pad around                                          the user-specified time; default 10 min                                          Only required for a statevector file.</p>"},{"location":"reference/#RAiDER.losreader.Raytracing--returns","title":"Returns:","text":"<p>ndarray - an  x 3 array of unit look vectors, defined in         an Earth-centered, earth-fixed reference frame (ECEF).         Convention is vectors point from the target pixel to the         sensor. ndarray - array of  of the distnce from the surface to         the top of the troposphere (denoted by zref)"},{"location":"reference/#RAiDER.losreader.Raytracing--example","title":"Example:","text":"<p>from RAiDER.losreader import Raytracing import numpy as np</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>class Raytracing(LOS):\n\"\"\"\n    Special value indicating that full raytracing will be\n    used to calculate slant delays.\n\n    Get unit look vectors pointing from the ground (target) pixels to the sensor,\n    or to Zenith. Can be accomplished using an ISCE-style 2-band LOS file or a\n    file containing orbital statevectors.\n\n    *NOTE*:\n    These line-of-sight vectors will NOT match ordinary LOS vectors for InSAR\n    because they are in an ECEF reference frame instead of a local ENU. This is done\n    because the construction of rays is done in ECEF rather than the local ENU.\n\n    Args:\n    ----------\n    time: python datetime                  - user-requested query time. Must be\n                                             compatible with the orbit file passed.\n                                             Only required for a statevector file.\n    pad: int                               - integer number of seconds to pad around\n                                             the user-specified time; default 10 min\n                                             Only required for a statevector file.\n\n    Returns:\n    -------\n    ndarray - an &lt;in_shape&gt; x 3 array of unit look vectors, defined in\n            an Earth-centered, earth-fixed reference frame (ECEF).\n            Convention is vectors point from the target pixel to the\n            sensor.\n    ndarray - array of &lt;in_shape&gt; of the distnce from the surface to\n            the top of the troposphere (denoted by zref)\n\n    Example:\n    --------\n    &gt;&gt;&gt; from RAiDER.losreader import Raytracing\n    &gt;&gt;&gt; import numpy as np\n    \"\"\"\n    def __init__(self, filename=None, los_convention='isce', time=None, look_dir = 'right', pad=600):\n'''read in and parse a statevector file'''\n        super().__init__()\n        self._ray_trace = True\n        self._file = filename\n        self._time = time\n        self._pad  = pad\n        self._convention = los_convention\n        self._orbit = None\n        if self._convention.lower() != 'isce':\n            raise NotImplementedError()\n\n        # ISCE3 data structures\n        import isce3.ext.isce3 as isce\n        if self._time is not None:\n            # __call__ called in checkArgs; keep for modularity\n            self._orbit = get_orbit(self._file, self._time, pad=pad)\n        self._elp = isce.core.Ellipsoid()\n        self._dop = isce.core.LUT2d()\n        if look_dir.lower() == \"right\":\n            self._look_dir = isce.core.LookSide.Right\n        elif look_dir.lower() == \"left\":\n            self._look_dir = isce.core.LookSide.Left\n        else:\n            raise RuntimeError(f\"Unknown look direction: {look_dir}\")\n\n\n    def getSensorDirection(self):\n        if self._orbit is None:\n            raise ValueError('The orbit has not been set')\n        z = self._orbit.position[:,2]\n        t = self._orbit.time\n        start = np.argmin(t)\n        end = np.argmax(t)\n        if z[start] &gt; z[end]:\n            return 'desc'\n        else:\n            return 'asc'\n\n\n    def getLookDirection(self):\n        return self._look_dir\n\n    # Called in checkArgs\n    def setTime(self, time, pad=600):\n        self._time = time\n        self._orbit = get_orbit(self._file, self._time, pad=pad)\n\n\n    def getLookVectors(self, ht, llh, xyz, yy):\n'''\n        Calculate look vectors for raytracing\n        '''\n        # TODO - Modify when isce3 vectorization is available\n        los = np.full(yy.shape + (3,), np.nan)\n        llh = llh.copy()\n        llh[0] = np.deg2rad(llh[0])\n        llh[1] = np.deg2rad(llh[1])\n\n        import isce3.ext.isce3 as isce\n\n        for ii in range(yy.shape[0]):\n            for jj in range(yy.shape[1]):\n                inp = np.array([llh[0][ii, jj], llh[1][ii, jj], ht])\n                inp_xyz = xyz[ii, jj, :]\n\n                if any(np.isnan(inp)) or any(np.isnan(inp_xyz)):\n                    continue\n\n                # Wavelength does not matter for\n                try:\n                    aztime, slant_range = isce.geometry.geo2rdr(\n                        inp, self._elp, self._orbit, self._dop, 0.06, self._look_dir,\n                        threshold=1.0e-7,\n                        maxiter=30,\n                        delta_range=10.0)\n                    sat_xyz, _ = self._orbit.interpolate(aztime)\n                    los[ii, jj, :] = (sat_xyz - inp_xyz) / slant_range\n                except Exception as e:\n                    los[ii, jj, :] = np.nan\n        return los\n\n\n    def getIntersectionWithHeight(self, height):\n\"\"\"\n        This function computes the intersection point of a ray at a height\n        level\n        \"\"\"\n        # We just leverage the same code as finding top of atmosphere here\n        return getTopOfAtmosphere(self._xyz, self._look_vecs, height)\n\n\n    def getIntersectionWithLevels(self, levels):\n\"\"\"\n        This function returns the points at which rays intersect the\n        given height levels. This way we have same number of points in\n        each ray and only at level transitions.\n\n        For targets that are above a given height level, the ray points are set\n        to nan to indicate that it does not contribute to the integration of\n        rays.\n\n        Output:\n            rays: (self._lats.shape, len(levels),  3)\n        \"\"\"\n        rays = np.zeros(list(self._lats.shape) + [len(levels), 3])\n\n        # This can be further vectorized, if there is enough memory\n        for ind, z in enumerate(levels):\n            rays[..., ind, :] = self.getIntersectionWithHeight(z)\n\n            # Set pixels above level to nan\n            value = rays[..., ind, :]\n            value[self._heights &gt; z, :] = np.nan\n\n        return rays\n\n\n    def calculateDelays(self, delays):\n'''\n        Here \"delays\" is point-wise delays (i.e. refractivities), not\n        integrated ZTD/STD.\n        '''\n        # Create rays  (Use getIntersectionWithLevels above)\n        # Interpolate delays to rays\n        # Integrate along rays\n        # Return STD\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing.__init__","title":"<code>__init__(filename=None, los_convention='isce', time=None, look_dir='right', pad=600)</code>","text":"<p>read in and parse a statevector file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def __init__(self, filename=None, los_convention='isce', time=None, look_dir = 'right', pad=600):\n'''read in and parse a statevector file'''\n    super().__init__()\n    self._ray_trace = True\n    self._file = filename\n    self._time = time\n    self._pad  = pad\n    self._convention = los_convention\n    self._orbit = None\n    if self._convention.lower() != 'isce':\n        raise NotImplementedError()\n\n    # ISCE3 data structures\n    import isce3.ext.isce3 as isce\n    if self._time is not None:\n        # __call__ called in checkArgs; keep for modularity\n        self._orbit = get_orbit(self._file, self._time, pad=pad)\n    self._elp = isce.core.Ellipsoid()\n    self._dop = isce.core.LUT2d()\n    if look_dir.lower() == \"right\":\n        self._look_dir = isce.core.LookSide.Right\n    elif look_dir.lower() == \"left\":\n        self._look_dir = isce.core.LookSide.Left\n    else:\n        raise RuntimeError(f\"Unknown look direction: {look_dir}\")\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing.calculateDelays","title":"<code>calculateDelays(delays)</code>","text":"<p>Here \"delays\" is point-wise delays (i.e. refractivities), not integrated ZTD/STD.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def calculateDelays(self, delays):\n'''\n    Here \"delays\" is point-wise delays (i.e. refractivities), not\n    integrated ZTD/STD.\n    '''\n    # Create rays  (Use getIntersectionWithLevels above)\n    # Interpolate delays to rays\n    # Integrate along rays\n    # Return STD\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing.getIntersectionWithHeight","title":"<code>getIntersectionWithHeight(height)</code>","text":"<p>This function computes the intersection point of a ray at a height level</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def getIntersectionWithHeight(self, height):\n\"\"\"\n    This function computes the intersection point of a ray at a height\n    level\n    \"\"\"\n    # We just leverage the same code as finding top of atmosphere here\n    return getTopOfAtmosphere(self._xyz, self._look_vecs, height)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing.getIntersectionWithLevels","title":"<code>getIntersectionWithLevels(levels)</code>","text":"<p>This function returns the points at which rays intersect the given height levels. This way we have same number of points in each ray and only at level transitions.</p> <p>For targets that are above a given height level, the ray points are set to nan to indicate that it does not contribute to the integration of rays.</p> Output <p>rays: (self._lats.shape, len(levels),  3)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def getIntersectionWithLevels(self, levels):\n\"\"\"\n    This function returns the points at which rays intersect the\n    given height levels. This way we have same number of points in\n    each ray and only at level transitions.\n\n    For targets that are above a given height level, the ray points are set\n    to nan to indicate that it does not contribute to the integration of\n    rays.\n\n    Output:\n        rays: (self._lats.shape, len(levels),  3)\n    \"\"\"\n    rays = np.zeros(list(self._lats.shape) + [len(levels), 3])\n\n    # This can be further vectorized, if there is enough memory\n    for ind, z in enumerate(levels):\n        rays[..., ind, :] = self.getIntersectionWithHeight(z)\n\n        # Set pixels above level to nan\n        value = rays[..., ind, :]\n        value[self._heights &gt; z, :] = np.nan\n\n    return rays\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Raytracing.getLookVectors","title":"<code>getLookVectors(ht, llh, xyz, yy)</code>","text":"<p>Calculate look vectors for raytracing</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def getLookVectors(self, ht, llh, xyz, yy):\n'''\n    Calculate look vectors for raytracing\n    '''\n    # TODO - Modify when isce3 vectorization is available\n    los = np.full(yy.shape + (3,), np.nan)\n    llh = llh.copy()\n    llh[0] = np.deg2rad(llh[0])\n    llh[1] = np.deg2rad(llh[1])\n\n    import isce3.ext.isce3 as isce\n\n    for ii in range(yy.shape[0]):\n        for jj in range(yy.shape[1]):\n            inp = np.array([llh[0][ii, jj], llh[1][ii, jj], ht])\n            inp_xyz = xyz[ii, jj, :]\n\n            if any(np.isnan(inp)) or any(np.isnan(inp_xyz)):\n                continue\n\n            # Wavelength does not matter for\n            try:\n                aztime, slant_range = isce.geometry.geo2rdr(\n                    inp, self._elp, self._orbit, self._dop, 0.06, self._look_dir,\n                    threshold=1.0e-7,\n                    maxiter=30,\n                    delta_range=10.0)\n                sat_xyz, _ = self._orbit.interpolate(aztime)\n                los[ii, jj, :] = (sat_xyz - inp_xyz) / slant_range\n            except Exception as e:\n                los[ii, jj, :] = np.nan\n    return los\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Zenith","title":"<code>Zenith</code>","text":"<p>             Bases: <code>LOS</code></p> <p>Class definition for a \"Zenith\" object.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>class Zenith(LOS):\n\"\"\"\n    Class definition for a \"Zenith\" object.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._is_zenith = True\n\n\n    def setLookVectors(self):\n'''Set point locations and calculate Zenith look vectors'''\n        if self._lats is None:\n            raise ValueError('Target points not set')\n        if self._look_vecs is None:\n            self._look_vecs = getZenithLookVecs(self._lats, self._lons, self._heights)\n\n\n    def __call__(self, delays):\n'''Placeholder method for consistency with the other classes'''\n        return delays\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Zenith.__call__","title":"<code>__call__(delays)</code>","text":"<p>Placeholder method for consistency with the other classes</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def __call__(self, delays):\n'''Placeholder method for consistency with the other classes'''\n    return delays\n</code></pre>"},{"location":"reference/#RAiDER.losreader.Zenith.setLookVectors","title":"<code>setLookVectors()</code>","text":"<p>Set point locations and calculate Zenith look vectors</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def setLookVectors(self):\n'''Set point locations and calculate Zenith look vectors'''\n    if self._lats is None:\n        raise ValueError('Target points not set')\n    if self._look_vecs is None:\n        self._look_vecs = getZenithLookVecs(self._lats, self._lons, self._heights)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.build_ray","title":"<code>build_ray(model_zs, ht, xyz, LOS, MAX_TROPO_HEIGHT=_ZREF)</code>","text":"<p>Compute the ray length in ECEF between each  weather model layers</p> <p>Only heights up to MAX_TROPO_HEIGHT are considered Assumption: model_zs (model) are assumed to be sorted in height We start integrating bottom up</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def build_ray(model_zs, ht, xyz, LOS, MAX_TROPO_HEIGHT=_ZREF):\n\"\"\"\n    Compute the ray length in ECEF between each  weather model layers\n\n    Only heights up to MAX_TROPO_HEIGHT are considered\n    Assumption: model_zs (model) are assumed to be sorted in height\n    We start integrating bottom up\n    \"\"\"\n    low_xyz = None\n    high_xyz = None\n    cos_factor = None\n\n    ray_lengths, low_xyzs, high_xyzs = [], [], []\n    for zz in range(model_zs.size-1):\n        # Low and High for model interval\n        low_ht = model_zs[zz]\n        high_ht = model_zs[zz + 1]\n\n        # this will force ray lengths to stay within the weather model domain\n        if high_ht == model_zs[-1]:\n            high_ht -= 0.01\n\n        # If high_ht &lt; height of point - no contribution to integral\n        # If low_ht &gt; max_tropo_height - no contribution to integral\n        if (high_ht &lt; ht) or (low_ht &gt;= MAX_TROPO_HEIGHT):\n            continue\n\n        # If low_ht &lt; requested height, start integral at requested height\n        if low_ht &lt; ht:\n            low_ht = ht\n\n        # If high_ht &gt; max_tropo_height - integral only up to max tropo height\n        if high_ht &gt; MAX_TROPO_HEIGHT:\n            high_ht = MAX_TROPO_HEIGHT\n\n        # Continue only if needed - 1m troposphere does nothing\n        if np.abs(high_ht - low_ht) &lt; 1.0:\n            continue\n\n        # If high_xyz was defined, make new low_xyz - save computation\n        if high_xyz is not None:\n            low_xyz = high_xyz\n        else:\n            low_xyz = getTopOfAtmosphere(xyz, LOS, low_ht, factor=cos_factor)\n\n        # Compute high_xyz (upper model level)\n        high_xyz = getTopOfAtmosphere(xyz, LOS, high_ht, factor=cos_factor)\n\n        # Compute ray length\n        ray_length =  np.linalg.norm(high_xyz - low_xyz, axis=-1)\n\n        # Compute cos_factor for first iteration\n        if cos_factor is None:\n            cos_factor = (high_ht - low_ht) / ray_length\n\n        ray_lengths.append(ray_length)\n        low_xyzs.append(low_xyz)\n        high_xyzs.append(high_xyz)\n\n    ## if all weather model levels are requested the top most layer might not contribute anything\n    if not ray_lengths:\n        return None, None, None\n    else:\n        return np.stack(ray_lengths), np.stack(low_xyzs), np.stack(high_xyzs)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.cut_times","title":"<code>cut_times(times, ref_time, pad)</code>","text":"<p>Slice the orbit file around the reference aquisition time. This is done by default using a three-hour window, which for Sentinel-1 empirically works out to be roughly the largest window allowed by the orbit time. Args:</p> <p>times: Nt x 1 ndarray     - Vector of orbit times as datetime ref_time: datetime        - Reference time pad: int                  - integer time in seconds to use as padding Returns:</p> <p>idx: Nt x 1 logical ndarray - a mask of times within the padded request time.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def cut_times(times, ref_time, pad):\n\"\"\"\n    Slice the orbit file around the reference aquisition time. This is done\n    by default using a three-hour window, which for Sentinel-1 empirically\n    works out to be roughly the largest window allowed by the orbit time.\n    Args:\n    ----------\n    times: Nt x 1 ndarray     - Vector of orbit times as datetime\n    ref_time: datetime        - Reference time\n    pad: int                  - integer time in seconds to use as padding\n    Returns:\n    -------\n    idx: Nt x 1 logical ndarray - a mask of times within the padded request time.\n    \"\"\"\n    diff = np.array(\n        [(x - ref_time).total_seconds() for x in times]\n    )\n    return np.abs(diff) &lt; pad\n</code></pre>"},{"location":"reference/#RAiDER.losreader.filter_ESA_orbit_file","title":"<code>filter_ESA_orbit_file(orbit_xml, ref_time)</code>","text":"<p>Returns true or false depending on whether orbit file contains ref time</p>"},{"location":"reference/#RAiDER.losreader.filter_ESA_orbit_file--parameters","title":"Parameters","text":"<p>orbit_xml : str     ESA orbit xml ref_time : datetime.datetime</p>"},{"location":"reference/#RAiDER.losreader.filter_ESA_orbit_file--returns","title":"Returns","text":"<p>bool     True if ref time is within orbit_xml</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def filter_ESA_orbit_file(orbit_xml: str,\n                          ref_time: datetime.datetime) -&gt; bool:\n\"\"\"Returns true or false depending on whether orbit file contains ref time\n\n    Parameters\n    ----------\n    orbit_xml : str\n        ESA orbit xml\n    ref_time : datetime.datetime\n\n    Returns\n    -------\n    bool\n        True if ref time is within orbit_xml\n    \"\"\"\n    f = os.path.basename(orbit_xml)\n    t0 = datetime.datetime.strptime(f.split('_')[6].lstrip('V'), '%Y%m%dT%H%M%S')\n    t1 = datetime.datetime.strptime(f.split('_')[7].rstrip('.EOF'), '%Y%m%dT%H%M%S')\n    return (t0 &lt; ref_time &lt; t1)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.getTopOfAtmosphere","title":"<code>getTopOfAtmosphere(xyz, look_vecs, toaheight, factor=None)</code>","text":"<p>Get ray intersection at given height.</p> <p>We use simple Newton-Raphson for this computation. This cannot be done exactly since closed form expression from xyz to llh is super compliated.</p> <p>If a factor (cos of inc angle) is provided - iterations are lot faster. If factor is not provided solutions converges to     - 0.01 mm at heights near zero in 10 iterations     - 10 cm at heights above 40km in 10 iterations</p> <p>If factor is know, we converge in 3 iterations to less than a micron.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def getTopOfAtmosphere(xyz, look_vecs, toaheight, factor=None):\n\"\"\"\n    Get ray intersection at given height.\n\n    We use simple Newton-Raphson for this computation. This cannot be done\n    exactly since closed form expression from xyz to llh is super compliated.\n\n    If a factor (cos of inc angle) is provided - iterations are lot faster.\n    If factor is not provided solutions converges to\n        - 0.01 mm at heights near zero in 10 iterations\n        - 10 cm at heights above 40km in 10 iterations\n\n    If factor is know, we converge in 3 iterations to less than a micron.\n    \"\"\"\n    if factor is not None:\n        maxIter = 3\n    else:\n        maxIter = 10\n        factor = 1.\n\n    # Guess top point\n    pos = xyz + toaheight * look_vecs\n\n    for _ in range(maxIter):\n        pos_llh = ecef2lla(pos[..., 0], pos[..., 1], pos[..., 2])\n        pos = pos + look_vecs * ((toaheight - pos_llh[2])/factor)[..., None]\n\n    return pos\n</code></pre>"},{"location":"reference/#RAiDER.losreader.getZenithLookVecs","title":"<code>getZenithLookVecs(lats, lons, heights)</code>","text":"<p>Returns look vectors when Zenith is used.</p> <p>Parameters:</p> Name Type Description Default <code>lats/lons/heights</code> <code>ndarray</code> <ul> <li>Numpy arrays containing WGS-84 target locations</li> </ul> required <p>Returns:</p> Name Type Description <code>zenLookVecs</code> <code>ndarray</code> <ul> <li>(in_shape) x 3 unit look vectors in an ECEF reference frame</li> </ul> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def getZenithLookVecs(lats, lons, heights):\n'''\n    Returns look vectors when Zenith is used.\n\n    Args:\n        lats/lons/heights (ndarray):  - Numpy arrays containing WGS-84 target locations\n\n    Returns:\n        zenLookVecs (ndarray):        - (in_shape) x 3 unit look vectors in an ECEF reference frame\n    '''\n    x = np.cos(np.radians(lats)) * np.cos(np.radians(lons))\n    y = np.cos(np.radians(lats)) * np.sin(np.radians(lons))\n    z = np.sin(np.radians(lats))\n\n    return np.stack([x, y, z], axis=-1)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.get_orbit","title":"<code>get_orbit(orbit_file, ref_time, pad)</code>","text":"<p>Returns state vectors from an orbit file; state vectors are unique and ordered in terms of time orbit file (str | list):   - user-passed file(s) containing statevectors                              for the sensor (can be download with sentineleof libray). Lists of files                              are only accepted for Sentinel-1 EOF files. pad (int):                 - number of seconds to keep around the                              requested time (should be about 600 seconds)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def get_orbit(orbit_file: Union[list, str],\n              ref_time: datetime.datetime,\n              pad: int):\n'''\n    Returns state vectors from an orbit file; state vectors are unique and ordered in terms of time\n    orbit file (str | list):   - user-passed file(s) containing statevectors\n                                 for the sensor (can be download with sentineleof libray). Lists of files\n                                 are only accepted for Sentinel-1 EOF files.\n    pad (int):                 - number of seconds to keep around the\n                                 requested time (should be about 600 seconds)\n\n    '''\n    # First load the state vectors into an isce orbit\n    import isce3.ext.isce3 as isce\n\n    svs = np.stack(get_sv(orbit_file, ref_time, pad), axis=-1)\n    sv_objs = []\n    # format for ISCE\n    for sv in svs:\n        sv = isce.core.StateVector(isce.core.DateTime(sv[0]), sv[1:4], sv[4:7])\n        sv_objs.append(sv)\n\n    sv_objs = sorted(sv_objs, key=lambda sv: sv.datetime)\n    # Ensure only unique state vectors; unfortunately builtin set does not work.\n    visited_times = []\n    sv_objs_filtered = []\n    for sv in sv_objs:\n        if sv.datetime in visited_times:\n            continue\n        visited_times.append(sv.datetime)\n        sv_objs_filtered.append(sv)\n\n    orb = isce.core.Orbit(sv_objs_filtered)\n\n    return orb\n</code></pre>"},{"location":"reference/#RAiDER.losreader.get_radar_pos","title":"<code>get_radar_pos(llh, orb)</code>","text":"<p>Calculate the coordinate of the sensor in ECEF at the time corresponding to ***.</p>"},{"location":"reference/#RAiDER.losreader.get_radar_pos--args","title":"Args:","text":"<p>orb: isce3.core.Orbit   - Nt x 7 matrix of statevectors: [t x y z vx vy vz] llh: ndarray   - position of the target in LLH out: str    - either lookangle or ecef for vector</p>"},{"location":"reference/#RAiDER.losreader.get_radar_pos--returns","title":"Returns:","text":"<p>los: ndarray  - Satellite incidence angle sr:  ndarray  - Slant range in meters</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def get_radar_pos(llh, orb):\n'''\n    Calculate the coordinate of the sensor in ECEF at the time corresponding to ***.\n\n    Args:\n    ----------\n    orb: isce3.core.Orbit   - Nt x 7 matrix of statevectors: [t x y z vx vy vz]\n    llh: ndarray   - position of the target in LLH\n    out: str    - either lookangle or ecef for vector\n\n    Returns:\n    -------\n    los: ndarray  - Satellite incidence angle\n    sr:  ndarray  - Slant range in meters\n    '''\n\n    num_iteration = 30\n    residual_threshold = 1.0e-7\n\n    # Get xyz positions of targets here from lat/lon/height\n    targ_xyz = np.stack(\n        lla2ecef(llh[:, 0], llh[:, 1], llh[:, 2]), axis=-1\n    )\n\n    # Get some isce3 constants for this inversion\n    import isce3.ext.isce3 as isce\n    # TODO - Assuming right-looking for now\n    elp = isce.core.Ellipsoid()\n    dop = isce.core.LUT2d()\n    look = isce.core.LookSide.Right\n\n    # Iterate for each point\n    # TODO - vectorize / parallelize\n    sr = np.empty((llh.shape[0],), dtype=np.float64)\n    output = np.empty((llh.shape[0],), dtype=np.float64)\n\n    for ind, pt in enumerate(llh):\n        if not any(np.isnan(pt)):\n            # ISCE3 always uses xy convention\n            inp = np.array([np.deg2rad(pt[1]),\n                            np.deg2rad(pt[0]),\n                            pt[2]])\n            # Local normal vector\n            nv = elp.n_vector(inp[0], inp[1])\n\n            # Wavelength does not matter  for zero doppler\n            try:\n                aztime, slant_range = isce.geometry.geo2rdr(\n                    inp, elp, orb, dop, 0.06, look,\n                    threshold=residual_threshold,\n                    maxiter=num_iteration,\n                    delta_range=10.0)\n                sat_xyz, _ = orb.interpolate(aztime)\n                sr[ind] = slant_range\n\n\n                delta = sat_xyz - targ_xyz[ind, :]\n\n                # TODO - if we only ever need cos(lookang),\n                # skip the arccos here and cos above\n                delta = delta / np.linalg.norm(delta)\n                output[ind] = np.rad2deg(\n                    np.arccos(np.dot(delta, nv))\n                )\n\n            except Exception as e:\n                raise e\n\n        # in case nans in hgt field\n        else:\n            sr[ind] = np.nan\n            output[ind, ...] = np.nan\n\n    return output, sr\n</code></pre>"},{"location":"reference/#RAiDER.losreader.get_sv","title":"<code>get_sv(los_file, ref_time, pad)</code>","text":"<p>Read an LOS file and return orbital state vectors</p> <p>Parameters:</p> Name Type Description Default <code>los_file</code> <code>(str, Path, list)</code> <ul> <li>user-passed file containing either look                               vectors or statevectors for the sensor</li> </ul> required <code>ref_time</code> <code>datetime</code> <ul> <li>User-requested datetime; if not encompassed                               by the orbit times will raise a ValueError</li> </ul> required <code>pad</code> <code>int</code> <ul> <li>number of seconds to keep around the                               requested time (should be about 600 seconds)</li> </ul> required <p>Returns:</p> Name Type Description <code>svs</code> <code>list of ndarrays</code> <ul> <li>the times, x/y/z positions and velocities</li> </ul> <p>of the sensor for the given window around the reference time</p> <p>Warning - if multiple orbit files are pasted the svs returned are not organized and returned in the order with respect to the files inputted (and statevectors within them).</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def get_sv(los_file: Union[str, list, PosixPath],\n           ref_time: datetime.datetime,\n           pad: int):\n\"\"\"\n    Read an LOS file and return orbital state vectors\n\n    Args:\n        los_file (str, Path, list):     - user-passed file containing either look\n                                          vectors or statevectors for the sensor\n        ref_time (datetime):            - User-requested datetime; if not encompassed\n                                          by the orbit times will raise a ValueError\n        pad (int):                      - number of seconds to keep around the\n                                          requested time (should be about 600 seconds)\n\n    Returns:\n        svs (list of ndarrays): - the times, x/y/z positions and velocities\n        of the sensor for the given window around the reference time\n\n    Warning - if multiple orbit files are pasted the svs returned are not organized and returned in the order\n    with respect to the files inputted (and statevectors within them).\n    \"\"\"\n    try:\n        svs = read_txt_file(los_file)\n    except (ValueError, TypeError):\n        try:\n            los_files = [los_file] if isinstance(los_file, (str, PosixPath)) else los_file\n            # Do not need duplicate xml files\n            # It appears that we want to make sure that we get data from first available orbit file first in our tests\n            # TODO: figure out why - maybe tests data occur before midnight and has to do with midnight crossing\n            # Will need to more thoroughly test and investigate the `sorted` piece\n            los_files = sorted(list(set(los_files)))\n\n            def filter_ESA_orbit_file_p(path: str) -&gt; bool:\n                return filter_ESA_orbit_file(path, ref_time)\n            los_files = list(filter(filter_ESA_orbit_file_p, los_files))\n            if not los_files:\n                raise ValueError('There are no valid orbit files provided')\n            svs = []\n            for orb_path in los_files:\n                svs.extend(read_ESA_Orbit_file(orb_path))\n\n        except BaseException:\n            try:\n                svs = read_shelve(los_file)\n            except BaseException:\n                raise ValueError(\n                    'get_sv: I cannot parse the statevector file {}'.format(los_file)\n                )\n\n    if ref_time:\n        idx = cut_times(svs[0], ref_time, pad=pad)\n        svs = [d[idx] for d in svs]\n\n    return svs\n</code></pre>"},{"location":"reference/#RAiDER.losreader.inc_hd_to_enu","title":"<code>inc_hd_to_enu(incidence, heading)</code>","text":"<p>Convert incidence and heading to line-of-sight vectors from the ground to the top of the troposphere.</p> <p>Parameters:</p> Name Type Description Default <code>incidence</code> <p>ndarray             - incidence angle in deg from vertical</p> required <code>heading</code> <p>ndarray               - heading angle in deg clockwise from north</p> required <code>lats/lons/heights</code> <p>ndarray - WGS84 ellipsoidal target (ground pixel) locations</p> required <p>Returns:</p> Name Type Description <code>LOS</code> <p>ndarray  - (input_shape) x 3 array of unit look vectors in local ENU</p> <p>Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def inc_hd_to_enu(incidence, heading):\n'''\n    Convert incidence and heading to line-of-sight vectors from the ground to the top of\n    the troposphere.\n\n    Args:\n        incidence: ndarray\t       - incidence angle in deg from vertical\n        heading: ndarray \t       - heading angle in deg clockwise from north\n        lats/lons/heights: ndarray - WGS84 ellipsoidal target (ground pixel) locations\n\n    Returns:\n        LOS: ndarray  - (input_shape) x 3 array of unit look vectors in local ENU\n\n    Algorithm referenced from http://earthdef.caltech.edu/boards/4/topics/327\n    '''\n    if np.any(incidence &lt; 0):\n        raise ValueError('inc_hd_to_enu: Incidence angle cannot be less than 0')\n\n    east = sind(incidence) * cosd(heading + 90)\n    north = sind(incidence) * sind(heading + 90)\n    up = cosd(incidence)\n\n    return np.stack((east, north, up), axis=-1)\n</code></pre>"},{"location":"reference/#RAiDER.losreader.pick_ESA_orbit_file","title":"<code>pick_ESA_orbit_file(list_files, ref_time)</code>","text":"<p>From list of .EOF orbit files, pick the one that contains 'ref_time'</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def pick_ESA_orbit_file(list_files:list, ref_time:datetime.datetime):\n\"\"\" From list of .EOF orbit files, pick the one that contains 'ref_time' \"\"\"\n    orb_file = None\n    for path in list_files:\n        f  = os.path.basename(path)\n        t0 = datetime.datetime.strptime(f.split('_')[6].lstrip('V'), '%Y%m%dT%H%M%S')\n        t1 = datetime.datetime.strptime(f.split('_')[7].rstrip('.EOF'), '%Y%m%dT%H%M%S')\n        if t0 &lt; ref_time &lt; t1:\n            orb_file = path\n            break\n\n    assert orb_file, 'Given orbit files did not match given date/time'\n\n    return path\n</code></pre>"},{"location":"reference/#RAiDER.losreader.read_ESA_Orbit_file","title":"<code>read_ESA_Orbit_file(filename)</code>","text":"<p>Read orbit data from an orbit file supplied by ESA</p>"},{"location":"reference/#RAiDER.losreader.read_ESA_Orbit_file--args","title":"Args:","text":"<p>filename: str             - string of the orbit filename</p>"},{"location":"reference/#RAiDER.losreader.read_ESA_Orbit_file--returns","title":"Returns:","text":"<p>t: Nt x 1 ndarray   - a numpy vector with Nt elements containing time                       in python datetime x, y, z: Nt x 1 ndarrays    - x/y/z positions of the sensor at the times t vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def read_ESA_Orbit_file(filename):\n'''\n    Read orbit data from an orbit file supplied by ESA\n\n    Args:\n    ----------\n    filename: str             - string of the orbit filename\n\n    Returns:\n    -------\n    t: Nt x 1 ndarray   - a numpy vector with Nt elements containing time\n                          in python datetime\n    x, y, z: Nt x 1 ndarrays    - x/y/z positions of the sensor at the times t\n    vx, vy, vz: Nt x 1 ndarrays - x/y/z velocities of the sensor at the times t\n    '''\n    if ET is None:\n        raise ImportError('read_ESA_Orbit_file: cannot import xml.etree.ElementTree')\n    tree = ET.parse(filename)\n    root = tree.getroot()\n    data_block = root[1]\n    numOSV = len(data_block[0])\n\n    t = []\n    x = np.ones(numOSV)\n    y = np.ones(numOSV)\n    z = np.ones(numOSV)\n    vx = np.ones(numOSV)\n    vy = np.ones(numOSV)\n    vz = np.ones(numOSV)\n\n    for i, st in enumerate(data_block[0]):\n        t.append(\n            datetime.datetime.strptime(\n                st[1].text,\n                'UTC=%Y-%m-%dT%H:%M:%S.%f'\n            )\n        )\n\n        x[i] = float(st[4].text)\n        y[i] = float(st[5].text)\n        z[i] = float(st[6].text)\n        vx[i] = float(st[7].text)\n        vy[i] = float(st[8].text)\n        vz[i] = float(st[9].text)\n    t = np.array(t)\n    return [t, x, y, z, vx, vy, vz]\n</code></pre>"},{"location":"reference/#RAiDER.losreader.read_txt_file","title":"<code>read_txt_file(filename)</code>","text":"<p>Read a 7-column text file containing orbit statevectors. Time should be denoted as integer time in seconds since the reference epoch (user-requested time).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <ul> <li>user-supplied space-delimited text file with no header             containing orbital statevectors as 7 columns:             - time in seconds since the user-supplied epoch             - x / y / z locations in ECEF cartesian coordinates             - vx / vy / vz velocities in m/s in ECEF coordinates</li> </ul> required <p>Returns:     svs (list):     - a length-7 list of numpy vectors containing the above                     variables</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def read_txt_file(filename):\n'''\n    Read a 7-column text file containing orbit statevectors. Time\n    should be denoted as integer time in seconds since the reference\n    epoch (user-requested time).\n\n    Args:\n        filename (str): - user-supplied space-delimited text file with no header\n                        containing orbital statevectors as 7 columns:\n                        - time in seconds since the user-supplied epoch\n                        - x / y / z locations in ECEF cartesian coordinates\n                        - vx / vy / vz velocities in m/s in ECEF coordinates\n    Returns:\n        svs (list):     - a length-7 list of numpy vectors containing the above\n                        variables\n    '''\n    t = list()\n    x = list()\n    y = list()\n    z = list()\n    vx = list()\n    vy = list()\n    vz = list()\n    with open(filename, 'r') as f:\n        for line in f:\n            try:\n                parts = line.strip().split()\n                t_ = datetime.datetime.fromisoformat(parts[0])\n                x_, y_, z_, vx_, vy_, vz_ = [float(t) for t in parts[1:]]\n            except ValueError:\n                raise ValueError(\n                    \"I need {} to be a 7 column text file, with \".format(filename) +\n                    \"columns t, x, y, z, vx, vy, vz (Couldn't parse line \" +\n                    \"{})\".format(repr(line)))\n            t.append(t_)\n            x.append(x_)\n            y.append(y_)\n            z.append(z_)\n            vx.append(vx_)\n            vy.append(vy_)\n            vz.append(vz_)\n\n    if len(t) &lt; 4:\n        raise ValueError('read_txt_file: File {} does not have enough statevectors'.format(filename))\n\n    return [np.array(a) for a in [t, x, y, z, vx, vy, vz]]\n</code></pre>"},{"location":"reference/#RAiDER.losreader.state_to_los","title":"<code>state_to_los(svs, llh_targets)</code>","text":"<p>Converts information from a state vector for a satellite orbit, given in terms of position and velocity, to line-of-sight information at each (lon,lat, height) coordinate requested by the user. Args:</p> <p>svs            - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor llh_targets    - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels</p>"},{"location":"reference/#RAiDER.losreader.state_to_los--returns","title":"Returns:","text":"<p>LOS                         - * x 3 matrix of LOS unit vectors in ECEF (not ENU) Example:</p> <p>import datetime import numpy from RAiDER.utilFcns import rio_open import RAiDER.losreader as losr lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0]) time = datetime.datetime(2018,11,12,23,0,0)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/losreader.py</code> <pre><code>def state_to_los(svs, llh_targets):\n'''\n    Converts information from a state vector for a satellite orbit, given in terms of\n    position and velocity, to line-of-sight information at each (lon,lat, height)\n    coordinate requested by the user.\n    Args:\n    ----------\n    svs            - t, x, y, z, vx, vy, vz - time, position, and velocity in ECEF of the sensor\n    llh_targets    - lats, lons, heights - Ellipsoidal (WGS84) positions of target ground pixels\n\n    Returns:\n    -------\n    LOS \t\t\t- * x 3 matrix of LOS unit vectors in ECEF (*not* ENU)\n    Example:\n    &gt;&gt;&gt; import datetime\n    &gt;&gt;&gt; import numpy\n    &gt;&gt;&gt; from RAiDER.utilFcns import rio_open\n    &gt;&gt;&gt; import RAiDER.losreader as losr\n    &gt;&gt;&gt; lats, lons, heights = np.array([-76.1]), np.array([36.83]), np.array([0])\n    &gt;&gt;&gt; time = datetime.datetime(2018,11,12,23,0,0)\n    &gt;&gt;&gt; # download the orbit file beforehand\n    &gt;&gt;&gt; esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF'\n    &gt;&gt;&gt; svs = losr.read_ESA_Orbit_file(esa_orbit_file)\n    &gt;&gt;&gt; LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz)\n    '''\n    # check the inputs\n    if np.min(svs.shape) &lt; 4:\n        raise RuntimeError(\n            'state_to_los: At least 4 state vectors are required'\n            ' for orbit interpolation'\n        )\n\n    # Convert svs to isce3 orbit\n    import isce3.ext.isce3 as isce\n    orb = isce.core.Orbit([\n        isce.core.StateVector(\n            isce.core.DateTime(row[0]),\n            row[1:4], row[4:7]\n        ) for row in svs\n    ])\n\n    # Flatten the input array for convenience\n    in_shape   = llh_targets[0].shape\n    target_llh = np.stack([x.flatten() for x in llh_targets], axis=-1)\n    Npts       = len(target_llh)\n\n    # Iterate through targets and compute LOS\n    los_ang, _ = get_radar_pos(target_llh, orb)\n    los_factor = np.cos(np.deg2rad(los_ang)).reshape(in_shape)\n    return los_factor\n</code></pre>"},{"location":"reference/#RAiDER.losreader.state_to_los--download-the-orbit-file-beforehand","title":"download the orbit file beforehand","text":"<p>esa_orbit_file = 'S1A_OPER_AUX_POEORB_OPOD_20181203T120749_V20181112T225942_20181114T005942.EOF' svs = losr.read_ESA_Orbit_file(esa_orbit_file) LOS = losr.state_to_los(*svs, [lats, lons, heights], xyz)</p>"},{"location":"reference/#RAiDER.makePoints","title":"<code>makePoints</code>","text":""},{"location":"reference/#RAiDER.makePoints.__file__","title":"<code>__file__ = '/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/makePoints.cpython-310-x86_64-linux-gnu.so'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.makePoints.__name__","title":"<code>__name__ = 'RAiDER.makePoints'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.makePoints.__package__","title":"<code>__package__ = 'RAiDER'</code>","text":"<p>str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p> <p>Create a new string object from the given object. If encoding or errors is specified, then the object must expose a data buffer that will be decoded using the given encoding and error handler. Otherwise, returns the result of object.str() (if defined) or repr(object). encoding defaults to sys.getdefaultencoding(). errors defaults to 'strict'.</p>"},{"location":"reference/#RAiDER.makePoints.__test__","title":"<code>__test__ = {}</code>","text":"<p>dict() -&gt; new empty dictionary dict(mapping) -&gt; new dictionary initialized from a mapping object's     (key, value) pairs dict(iterable) -&gt; new dictionary initialized as if via:     d = {}     for k, v in iterable:         d[k] = v dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs     in the keyword argument list.  For example:  dict(one=1, two=2)</p>"},{"location":"reference/#RAiDER.makePoints.makePoints0D","title":"<code>makePoints0D(max_len, Rays_SP, Rays_SLV, stepSize)</code>  <code>method descriptor</code>","text":"<p>Fast cython code to create the rays needed for ray-tracing.  Inputs:    max_len: maximum length of the rays   Rays_SP: 1 x 3 numpy array of the location of the ground pixels in an earth-centered,             earth-fixed coordinate system   Rays_SLV: 1 x 3 numpy array of the look vectors pointing from the ground pixel to the sensor   stepSize: Distance between points along the ray-path Output:   ray: a 3 x Npts array containing the rays tracing a path from the ground pixels, along the         line-of-sight vectors, up to the maximum length specified.</p>"},{"location":"reference/#RAiDER.makePoints.makePoints1D","title":"<code>makePoints1D(max_len, Rays_SP, Rays_SLV, stepSize)</code>  <code>method descriptor</code>","text":"<p>Fast cython code to create the rays needed for ray-tracing.  Inputs:    max_len: maximum length of the rays   Rays_SP: Nx x 3 numpy array of the location of the ground pixels in an earth-centered,             earth-fixed coordinate system   Rays_SLV: Nx x 3 numpy array of the look vectors pointing from the ground pixel to the sensor   stepSize: Distance between points along the ray-path Output:   ray: a Nx x 3 x Npts array containing the rays tracing a path from the ground pixels, along the         line-of-sight vectors, up to the maximum length specified.</p>"},{"location":"reference/#RAiDER.makePoints.makePoints2D","title":"<code>makePoints2D(max_len, Rays_SP, Rays_SLV, stepSize)</code>  <code>method descriptor</code>","text":"<p>Fast cython code to create the rays needed for ray-tracing.  Inputs:    max_len: maximum length of the rays   Rays_SP: Nx x Ny x 3 numpy array of the location of the ground pixels in an earth-centered,             earth-fixed coordinate system   Rays_SLV: Nx x Ny x 3 numpy array of the look vectors pointing from the ground pixel to the sensor   stepSize: Distance between points along the ray-path Output:   ray: a Nx x Ny x 3 x Npts array containing the rays tracing a path from the ground pixels, along the         line-of-sight vectors, up to the maximum length specified.</p>"},{"location":"reference/#RAiDER.makePoints.makePoints3D","title":"<code>makePoints3D(max_len, Rays_SP, Rays_SLV, stepSize)</code>  <code>method descriptor</code>","text":"<p>Fast cython code to create the rays needed for ray-tracing Inputs:    max_len: maximum length of the rays   Rays_SP: Nx x Ny x Nz x 3 numpy array of the location of the ground pixels in an earth-centered,             earth-fixed coordinate system   Rays_SLV: Nx x Ny x Nz x 3 numpy array of the look vectors pointing from the ground pixel to the sensor   stepSize: Distance between points along the ray-path Output:   ray: a Nx x Ny x Nz x 3 x Npts array containing the rays tracing a path from the ground pixels, along the         line-of-sight vectors, up to the maximum length specified.</p>"},{"location":"reference/#RAiDER.models","title":"<code>models</code>","text":""},{"location":"reference/#RAiDER.models.ERA5","title":"<code>ERA5</code>","text":"<p>             Bases: <code>ECMWF</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/era5.py</code> <pre><code>class ERA5(ECMWF):\n    # I took this from\n    # https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels.\n    def __init__(self):\n        ECMWF.__init__(self)\n\n        self._humidityType = 'q'\n        self._expver = '0001'\n        self._classname = 'ea'\n        self._dataset = 'era5'\n        self._Name = 'ERA-5'\n        self._proj = CRS.from_epsg(4326)\n\n        # Tuple of min/max years where data is available.\n        lag_time = 3 # months\n        end_date = datetime.datetime.today() - relativedelta(months=lag_time)\n        self._valid_range = (datetime.datetime(1950, 1, 1), end_date)\n\n        # Availability lag time in days\n        self._lag_time = relativedelta(months=lag_time)\n\n        # Default, need to change to ml\n        self.setLevelType('ml')\n\n\n    def _fetch(self, out):\n'''\n        Fetch a weather model from ECMWF\n        '''\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n        time = self._time\n\n        # execute the search at ECMWF\n        self._get_from_cds(lat_min, lat_max, lon_min, lon_max, time, out)\n\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''Load either pressure or model level data'''\n        f = self.files[0] if f is None else f\n        if self._model_level_type == 'pl':\n            self._load_pressure_level(f, *args, **kwargs)\n        elif self._model_level_type == 'ml':\n            self._load_model_level(f, *args, **kwargs)\n        else:\n            raise RuntimeError(f'{self._model_level_type} is not a valid model type')\n</code></pre>"},{"location":"reference/#RAiDER.models.ERA5.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Load either pressure or model level data</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/era5.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''Load either pressure or model level data'''\n    f = self.files[0] if f is None else f\n    if self._model_level_type == 'pl':\n        self._load_pressure_level(f, *args, **kwargs)\n    elif self._model_level_type == 'ml':\n        self._load_model_level(f, *args, **kwargs)\n    else:\n        raise RuntimeError(f'{self._model_level_type} is not a valid model type')\n</code></pre>"},{"location":"reference/#RAiDER.models.GMAO","title":"<code>GMAO</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/gmao.py</code> <pre><code>class GMAO(WeatherModel):\n    # I took this from GMAO model level weblink\n    # https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv\n    def __init__(self):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        self._humidityType = 'q'\n        self._model_level_type = 'ml'  # Default, pressure levels are 'pl'\n\n        self._classname = 'gmao'\n        self._dataset = 'gmao'\n\n        # Tuple of min/max years where data is available.\n        self._valid_range = (dt.datetime(2014, 2, 20), \"Present\")\n        self._lag_time = dt.timedelta(hours=24.0)  # Availability lag time in hours\n\n        # model constants\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n\n\n        # horizontal grid spacing\n        self._lat_res = 0.25\n        self._lon_res = 0.3125\n        self._x_res = 0.3125\n        self._y_res = 0.25\n\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n\n        self._Name = 'GMAO'\n        self.files = None\n        self._bounds = None\n\n        # Projection\n        self._proj = CRS.from_epsg(4326)\n\n\n    def _fetch(self, out):\n'''\n        Fetch weather model data from GMAO\n        '''\n        acqTime = self._time\n\n        # calculate the array indices for slicing the GMAO variable arrays\n        lat_min_ind = int((self._ll_bounds[0] - (-90.0)) / self._lat_res)\n        lat_max_ind = int((self._ll_bounds[1] - (-90.0)) / self._lat_res)\n        lon_min_ind = int((self._ll_bounds[2] - (-180.0)) / self._lon_res)\n        lon_max_ind = int((self._ll_bounds[3] - (-180.0)) / self._lon_res)\n\n        T0 = dt.datetime(2017, 12, 1, 0, 0, 0)\n        # round time to nearest third hour\n        corrected_DT = round_date(acqTime, dt.timedelta(hours=self._time_res))\n        if not corrected_DT == acqTime:\n            logger.warning('Rounded given datetime from  %s to %s', acqTime, corrected_DT)\n\n        DT = corrected_DT - T0\n        time_ind = int(DT.total_seconds() / 3600.0 / self._time_res)\n\n        ml_min = 0\n        ml_max = 71\n        if corrected_DT &gt;= T0:\n            # open the dataset and pull the data\n            url = 'https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv'\n            session = pydap.cas.urs.setup_session('username', 'password', check_url=url)\n            ds = pydap.client.open_url(url, session=session)\n            qv = ds['qv'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n\n            p = ds['pl'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n            t = ds['t'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n            h = ds['h'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n\n        else:\n            root = 'https://portal.nccs.nasa.gov/datashare/gmao/geos-fp/das/Y{}/M{:02d}/D{:02d}'\n            base = f'GEOS.fp.asm.inst3_3d_asm_Nv.{corrected_DT.strftime(\"%Y%m%d\")}_{corrected_DT.hour:02}00.V01.nc4'\n            url = f'{root.format(corrected_DT.year, corrected_DT.month, corrected_DT.day)}/{base}'\n            f = '{}_raw{}'.format(*os.path.splitext(out))\n            if not os.path.exists(f):\n                logger.info('Fetching URL: %s', url)\n                session = requests_retry_session()\n                resp = session.get(url, stream=True)\n                assert resp.ok, f'Could not access url for datetime: {corrected_DT}'\n                with open(f, 'wb') as fh:\n                    shutil.copyfileobj(resp.raw, fh)\n            else:\n                logger.warning('Weather model already exists, skipping download')\n\n            with h5py.File(f, 'r') as ds:\n                q = ds['QV'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                p = ds['PL'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                t = ds['T'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                h = ds['H'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n            os.remove(f)\n\n        lats = np.arange(\n            (-90 + lat_min_ind * self._lat_res),\n            (-90 + (lat_max_ind + 1) * self._lat_res),\n            self._lat_res\n        )\n        lons = np.arange(\n            (-180 + lon_min_ind * self._lon_res),\n            (-180 + (lon_max_ind + 1) * self._lon_res),\n            self._lon_res\n        )\n\n        try:\n            # Note that lat/lon gets written twice for GMAO because they are the same as y/x\n            writeWeatherVars2NETCDF4(self, lats, lons, h, qv, p, t, outName=out)\n        except Exception:\n            logger.exception(\"Unable to save weathermodel to file\")\n\n\n    def load_weather(self, f=None):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n        self._load_model_level(f)\n\n\n    def _load_model_level(self, filename):\n'''\n        Get the variables from the GMAO link using OpenDAP\n        '''\n\n        # adding the import here should become absolute when transition to netcdf\n        from netCDF4 import Dataset\n        with Dataset(filename, mode='r') as f:\n            lons = np.array(f.variables['x'][:])\n            lats = np.array(f.variables['y'][:])\n            h = np.array(f.variables['H'][:])\n            q = np.array(f.variables['QV'][:])\n            p = np.array(f.variables['PL'][:])\n            t = np.array(f.variables['T'][:])\n\n        # restructure the 1-D lat/lon in regular 2D grid\n        _lons, _lats= np.meshgrid(lons, lats)\n\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        p = np.transpose(p)\n        q = np.transpose(q)\n        t = np.transpose(t)\n        h = np.transpose(h)\n\n        # check this\n        # data cube format should be lats,lons,heights\n        p = p.swapaxes(0, 1)\n        q = q.swapaxes(0, 1)\n        t = t.swapaxes(0, 1)\n        h = h.swapaxes(0, 1)\n\n        # For some reason z is opposite the others\n        p = np.flip(p, axis=2)\n        q = np.flip(q, axis=2)\n        t = np.flip(t, axis=2)\n        h = np.flip(h, axis=2)\n\n        # assign the regular-grid (lat/lon/h) variables\n\n        self._p = p\n        self._q = q\n        self._t = t\n        self._lats = _lats\n        self._lons = _lons\n        self._xs = _lons\n        self._ys = _lats\n        self._zs = h\n</code></pre>"},{"location":"reference/#RAiDER.models.GMAO.load_weather","title":"<code>load_weather(f=None)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/gmao.py</code> <pre><code>def load_weather(self, f=None):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n    self._load_model_level(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.HRES","title":"<code>HRES</code>","text":"<p>             Bases: <code>ECMWF</code></p> <p>Implement ECMWF models</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hres.py</code> <pre><code>class HRES(ECMWF):\n'''\n    Implement ECMWF models\n    '''\n\n    def __init__(self, level_type='ml'):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        # model constants\n        self._k1 = 0.776   # [K/Pa]\n        self._k2 = 0.233   # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n\n        # 9 km horizontal grid spacing. This is only used for extending the download-buffer, i.e. not in subsequent processing.\n        self._lon_res = 9. / 111  # 0.08108115\n        self._lat_res = 9. / 111  # 0.08108115\n        self._x_res = 9. / 111  # 0.08108115\n        self._y_res = 9. / 111  # 0.08108115\n\n        self._humidityType = 'q'\n        # Default, pressure levels are 'pl'\n        self._expver = '1'\n        self._classname = 'od'\n        self._dataset = 'hres'\n        self._Name = 'HRES'\n        self._proj = CRS.from_epsg(4326)\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n        # Tuple of min/max years where data is available.\n        self._valid_range = (datetime.datetime(1983, 4, 20), \"Present\")\n        # Availability lag time in days\n        self._lag_time = datetime.timedelta(hours=6)\n\n        self.setLevelType('ml')\n\n    def update_a_b(self):\n        # Before 2013-06-26, there were only 91 model levels. The mapping coefficients below are extracted\n        # based on https://www.ecmwf.int/en/forecasts/documentation-and-support/91-model-levels\n        self._levels = 91\n        self._zlevels = np.flipud(LEVELS_91_HEIGHTS)\n        self._a = A_91_HRES\n        self._b = B_91_HRES\n\n    def load_weather(self, f=None):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n\n        if self._model_level_type == 'ml':\n            if (self._time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n                self.update_a_b()\n            self._load_model_level(f)\n        elif self._model_level_type == 'pl':\n            self._load_pressure_levels(f)\n\n    def _fetch(self,out):\n'''\n        Fetch a weather model from ECMWF\n        '''\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n        time = self._time\n\n        if (time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n            self.update_a_b()\n\n        # execute the search at ECMWF\n        self._download_ecmwf(lat_min, lat_max, self._lat_res, lon_min, lon_max, self._lon_res, time, out)\n</code></pre>"},{"location":"reference/#RAiDER.models.HRES.load_weather","title":"<code>load_weather(f=None)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hres.py</code> <pre><code>def load_weather(self, f=None):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n\n    if self._model_level_type == 'ml':\n        if (self._time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n            self.update_a_b()\n        self._load_model_level(f)\n    elif self._model_level_type == 'pl':\n        self._load_pressure_levels(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.HRRR","title":"<code>HRRR</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>class HRRR(WeatherModel):\n    def __init__(self):\n        # initialize a weather model\n        super().__init__()\n\n        self._humidityType = 'q'\n        self._model_level_type = 'pl'  # Default, pressure levels are 'pl'\n        self._expver = '0001'\n        self._classname = 'hrrr'\n        self._dataset = 'hrrr'\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n\n        # Tuple of min/max years where data is available.\n        self._valid_range = (datetime.datetime(2016, 7, 15), \"Present\")\n        self._lag_time = datetime.timedelta(hours=3)  # Availability lag time in days\n\n        # model constants\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        # 3 km horizontal grid spacing\n        self._lat_res = 3. / 111\n        self._lon_res = 3. / 111\n        self._x_res = 3.\n        self._y_res = 3.\n\n        self._Nproc = 1\n        self._Name = 'HRRR'\n        self._Npl = 0\n        self.files = None\n        self._bounds = None\n\n        # Projection\n        # NOTE: The HRRR projection will get read directly from the downloaded weather model file; however,\n        # we also define it here so that the projection can be used without downloading any data. This is\n        # used for consistency with the other weather models and allows for some nice features, such as\n        # buffering.\n\n        # See https://github.com/blaylockbk/pyBKB_v2/blob/master/demos/HRRR_earthRelative_vs_gridRelative_winds.ipynb and code lower down\n        # '262.5:38.5:38.5:38.5 237.280472:1799:3000.00 21.138123:1059:3000.00'\n        # 'lov:latin1:latin2:latd lon1:nx:dx lat1:ny:dy'\n        # LCC parameters\n        lon0 = 262.5\n        lat0 = 38.5\n        lat1 = 38.5\n        lat2 = 38.5\n        x0 = 0\n        y0 = 0\n        earth_radius = 6371229\n        self._proj = CRS(f'+proj=lcc +lat_1={lat1} +lat_2={lat2} +lat_0={lat0} '\\\n                 f'+lon_0={lon0} +x_0={x0} +y_0={y0} +a={earth_radius} '\\\n                 f'+b={earth_radius} +units=m +no_defs')\n        self._valid_bounds = HRRR_CONUS_COVERAGE_POLYGON\n        self.setLevelType('nat')\n\n\n    def __model_levels__(self):\n        self._levels  = 50\n        self._zlevels = np.flipud(LEVELS_50_HEIGHTS)\n\n\n    def __pressure_levels__(self):\n        raise NotImplementedError('Pressure levels do not go high enough for HRRR.')\n\n\n    def _fetch(self,  out):\n'''\n        Fetch weather model data from HRRR\n        '''\n        self._files = out\n        corrected_DT = round_date(self._time, datetime.timedelta(hours=self._time_res))\n        self.checkTime(corrected_DT)\n        if not corrected_DT == self._time:\n            logger.info('Rounded given datetime from  %s to %s', self._time, corrected_DT)\n\n        # HRRR uses 0-360 longitude, so we need to convert the bounds to that\n        bounds = self._ll_bounds.copy()\n        bounds[2:] = np.mod(bounds[2:], 360)\n\n        download_hrrr_file(bounds, corrected_DT, out, 'hrrr', self._model_level_type)\n\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''\n        Load a weather model into a python weatherModel object, from self.files if no\n        filename is passed.\n        '''\n        if f is None:\n            f = self.files[0] if isinstance(self.files, list) else self.files\n\n\n        _xs, _ys, _lons, _lats, qs, temps, pres, geo_hgt, proj = load_weather_hrrr(f)\n\n        # convert geopotential height to geometric height\n        self._get_heights(_lats, geo_hgt)\n\n        self._t = temps\n        self._q = qs\n        self._p = pres\n        self._xs = _xs\n        self._ys = _ys\n        self._lats = _lats\n        self._lons = _lons\n        self._proj = proj\n\n\n    def checkValidBounds(self: WeatherModel, ll_bounds: np.ndarray):\n'''\n        Checks whether the given bounding box is valid for the HRRR or HRRRAK\n        (i.e., intersects with the model domain at all)\n\n        Args:\n        ll_bounds : np.ndarray\n\n        Returns:\n            The weather model object\n        '''\n        S, N, W, E = ll_bounds\n        aoi = box(W, S, E, N)\n        if self._valid_bounds.contains(aoi):\n            Mod = self\n\n        elif aoi.intersects(self._valid_bounds):\n            Mod = self\n            logger.critical('The HRRR weather model extent does not completely cover your AOI!')\n\n        else:\n            Mod = HRRRAK()\n            # valid bounds are in 0-&gt;360 to account for dateline crossing\n            W, E = np.mod([W, E], 360)\n            aoi  = box(W, S, E, N)\n            if Mod._valid_bounds.contains(aoi):\n                pass\n            elif aoi.intersects(Mod._valid_bounds):\n                logger.critical('The HRRR-AK weather model extent does not completely cover your AOI!')\n\n            else:\n                raise ValueError('The requested location is unavailable for HRRR')\n\n        return Mod\n</code></pre>"},{"location":"reference/#RAiDER.models.HRRR.checkValidBounds","title":"<code>checkValidBounds(ll_bounds)</code>","text":"<p>Checks whether the given bounding box is valid for the HRRR or HRRRAK (i.e., intersects with the model domain at all)</p> <p>Args: ll_bounds : np.ndarray</p> <p>Returns:</p> Type Description <p>The weather model object</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def checkValidBounds(self: WeatherModel, ll_bounds: np.ndarray):\n'''\n    Checks whether the given bounding box is valid for the HRRR or HRRRAK\n    (i.e., intersects with the model domain at all)\n\n    Args:\n    ll_bounds : np.ndarray\n\n    Returns:\n        The weather model object\n    '''\n    S, N, W, E = ll_bounds\n    aoi = box(W, S, E, N)\n    if self._valid_bounds.contains(aoi):\n        Mod = self\n\n    elif aoi.intersects(self._valid_bounds):\n        Mod = self\n        logger.critical('The HRRR weather model extent does not completely cover your AOI!')\n\n    else:\n        Mod = HRRRAK()\n        # valid bounds are in 0-&gt;360 to account for dateline crossing\n        W, E = np.mod([W, E], 360)\n        aoi  = box(W, S, E, N)\n        if Mod._valid_bounds.contains(aoi):\n            pass\n        elif aoi.intersects(Mod._valid_bounds):\n            logger.critical('The HRRR-AK weather model extent does not completely cover your AOI!')\n\n        else:\n            raise ValueError('The requested location is unavailable for HRRR')\n\n    return Mod\n</code></pre>"},{"location":"reference/#RAiDER.models.HRRR.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Load a weather model into a python weatherModel object, from self.files if no filename is passed.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''\n    Load a weather model into a python weatherModel object, from self.files if no\n    filename is passed.\n    '''\n    if f is None:\n        f = self.files[0] if isinstance(self.files, list) else self.files\n\n\n    _xs, _ys, _lons, _lats, qs, temps, pres, geo_hgt, proj = load_weather_hrrr(f)\n\n    # convert geopotential height to geometric height\n    self._get_heights(_lats, geo_hgt)\n\n    self._t = temps\n    self._q = qs\n    self._p = pres\n    self._xs = _xs\n    self._ys = _ys\n    self._lats = _lats\n    self._lons = _lons\n    self._proj = proj\n</code></pre>"},{"location":"reference/#RAiDER.models.credentials","title":"<code>credentials</code>","text":"<p>API credential information and help url for downloading weather model data      saved in a hidden file in home directory </p> <p>api filename      weather models          UID           KEY         URL</p> <p>cdsapirc          ERA5, ERA5T             uid           key         https://cds.climate.copernicus.eu/api/v2 ecmwfapirc        ERAI, HRES              email         key         https://api.ecmwf.int/v1 netrc             GMAO, MERRA2            username      password    urs.earthdata.nasa.gov               HRRR [public access]"},{"location":"reference/#RAiDER.models.credentials.API_CREDENTIALS_DICT","title":"<code>API_CREDENTIALS_DICT = {'cdsapirc': {'api': '                                \\nurl: {host} \\nkey: {uid}:{key}\\n                                ', 'help_url': 'https://cds.climate.copernicus.eu/api-how-to'}, 'ecmwfapirc': {'api': '{{                                 \\n\"url\"   : \"{host}\",                                 \\n\"key\"   : \"{key}\",                                 \\n\"email\" : \"{uid}\"                                 \\n}}\\n                                ', 'help_url': 'https://confluence.ecmwf.int/display/WEBAPI/Access+ECMWF+Public+Datasets#AccessECMWFPublicDatasets-key'}, 'netrc': {'api': '                                \\nmachine {host} \\n        login {uid} \\n        password {key}                                ', 'help_url': 'https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+cURL+And+Wget'}}</code>  <code>module-attribute</code>","text":"<p>ENV variables in cdsapi and ecmwapir</p> <p>cdsapi ['cdsapirc'] : CDSAPI_KEY [UID:KEY], 'CDSAPI_URL' ecmwfapir [ecmwfapirc] : 'ECMWF_API_KEY', 'ECMWF_API_EMAIL','ECMWF_API_URL'</p>"},{"location":"reference/#RAiDER.models.ecmwf","title":"<code>ecmwf</code>","text":""},{"location":"reference/#RAiDER.models.ecmwf.ECMWF","title":"<code>ECMWF</code>","text":"<p>             Bases: <code>WeatherModel</code></p> <p>Implement ECMWF models</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/ecmwf.py</code> <pre><code>class ECMWF(WeatherModel):\n'''\n    Implement ECMWF models\n    '''\n\n    def __init__(self):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        # model constants\n        self._k1 = 0.776   # [K/Pa]\n        self._k2 = 0.233   # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        self._time_res = TIME_RES['ECMWF']\n\n        self._lon_res = 0.25\n        self._lat_res = 0.25\n        self._proj = CRS.from_epsg(4326)\n\n        self._model_level_type = 'ml'  # Default\n\n\n    def __pressure_levels__(self):\n        self._zlevels = np.flipud(LEVELS_25_HEIGHTS)\n        self._levels  = len(self._zlevels)\n\n\n    def __model_levels__(self):\n        self._levels  = 137\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n        self._a = A_137_HRES\n        self._b = B_137_HRES\n\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n        self._load_model_level(f)\n\n\n    def _load_model_level(self, fname):\n        # read data from netcdf file\n        lats, lons, xs, ys, t, q, lnsp, z = self._makeDataCubes(\n            fname,\n            verbose=False\n        )\n\n        # ECMWF appears to give me this backwards\n        if lats[0] &gt; lats[1]:\n            z = z[::-1]\n            lnsp = lnsp[::-1]\n            t = t[:, ::-1]\n            q = q[:, ::-1]\n            lats = lats[::-1]\n        # Lons is usually ok, but we'll throw in a check to be safe\n        if lons[0] &gt; lons[1]:\n            z = z[..., ::-1]\n            lnsp = lnsp[..., ::-1]\n            t = t[..., ::-1]\n            q = q[..., ::-1]\n            lons = lons[::-1]\n        # pyproj gets fussy if the latitude is wrong, plus our\n        # interpolator isn't clever enough to pick up on the fact that\n        # they are the same\n        lons[lons &gt; 180] -= 360\n\n        self._t = t\n        self._q = q\n        geo_hgt, pres, hgt = self._calculategeoh(z, lnsp)\n\n        self._lons, self._lats = np.meshgrid(lons, lats)\n\n        # ys is latitude\n        self._get_heights(self._lats, hgt.transpose(1, 2, 0))\n        h = self._zs.copy()\n\n        # We want to support both pressure levels and true pressure grids.\n        # If the shape has one dimension, we'll scale it up to act as a\n        # grid, otherwise we'll leave it alone.\n        if len(pres.shape) == 1:\n            self._p = np.broadcast_to(pres[:, np.newaxis, np.newaxis], self._zs.shape)\n        else:\n            self._p = pres\n\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        self._p = self._p.transpose(1, 2, 0)\n        self._t = self._t.transpose(1, 2, 0)\n        self._q = self._q.transpose(1, 2, 0)\n\n        # Flip all the axis so that zs are in order from bottom to top\n        # lats / lons are simply replicated to all heights so they don't need flipped\n        self._p = np.flip(self._p, axis=2)\n        self._t = np.flip(self._t, axis=2)\n        self._q = np.flip(self._q, axis=2)\n        self._ys = self._lats.copy()\n        self._xs = self._lons.copy()\n        self._zs = np.flip(h, axis=2)\n\n\n    def _fetch(self, out):\n'''\n        Fetch a weather model from ECMWF\n        '''\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n\n        # execute the search at ECMWF\n        self._get_from_ecmwf(\n            lat_min,\n            lat_max,\n            self._lat_res,\n            lon_min,\n            lon_max,\n            self._lon_res,\n            self._time,\n            out\n        )\n        return\n\n\n    def _get_from_ecmwf(self, lat_min, lat_max, lat_step, lon_min, lon_max,\n                        lon_step, time, out):\n        import ecmwfapi\n\n        server = ecmwfapi.ECMWFDataServer()\n\n        corrected_DT = util.round_date(time, datetime.timedelta(hours=self._time_res))\n        if not corrected_DT == time:\n            logger.warning('Rounded given datetime from  %s to %s', time, corrected_DT)\n\n        server.retrieve({\n            \"class\": self._classname,  # ERA-Interim\n            'dataset': self._dataset,\n            \"expver\": \"{}\".format(self._expver),\n            # They warn me against all, but it works well\n            \"levelist\": 'all',\n            \"levtype\": \"ml\",  # Model levels\n            \"param\": \"lnsp/q/z/t\",  # Necessary variables\n            \"stream\": \"oper\",\n            # date: Specify a single date as \"2015-08-01\" or a period as\n            # \"2015-08-01/to/2015-08-31\".\n            \"date\": datetime.datetime.strftime(corrected_DT, \"%Y-%m-%d\"),\n            # type: Use an (analysis) unless you have a particular reason to\n            # use fc (forecast).\n            \"type\": \"an\",\n            # time: With type=an, time can be any of\n            # \"00:00:00/06:00:00/12:00:00/18:00:00\".  With type=fc, time can\n            # be any of \"00:00:00/12:00:00\",\n            \"time\": datetime.time.strftime(corrected_DT.time(), \"%H:%M:%S\"),\n            # step: With type=an, step is always \"0\". With type=fc, step can\n            # be any of \"3/6/9/12\".\n            \"step\": \"0\",\n            # grid: Only regular lat/lon grids are supported.\n            \"grid\": '{}/{}'.format(lat_step, lon_step),\n            \"area\": '{}/{}/{}/{}'.format(lat_max, lon_min, lat_min, lon_max),  # area: N/W/S/E\n            \"format\": \"netcdf\",\n            \"resol\": \"av\",\n            \"target\": out,    # target: the name of the output file.\n        })\n\n\n    def _get_from_cds(\n        self,\n        lat_min,\n        lat_max,\n        lon_min,\n        lon_max,\n        acqTime,\n        outname\n    ):\n\"\"\" Used for ERA5 \"\"\"\n        import cdsapi\n        c = cdsapi.Client(verify=0)\n\n        if self._model_level_type == 'pl':\n            var = ['z', 'q', 't']\n            levType = 'pressure_level'\n        else:\n            var = \"129/130/133/152\"  # 'lnsp', 'q', 'z', 't'\n            levType = 'model_level'\n\n        bbox = [lat_max, lon_min, lat_min, lon_max]\n\n        # round to the closest legal time\n\n        corrected_DT = util.round_date(acqTime, datetime.timedelta(hours=self._time_res))\n        if not corrected_DT == acqTime:\n            logger.warning('Rounded given datetime from  %s to %s', acqTime, corrected_DT)\n\n\n        # I referenced https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5\n        dataDict = {\n            \"class\": \"ea\",\n            \"expver\": \"1\",\n            \"levelist\": 'all',\n            \"levtype\": \"{}\".format(self._model_level_type),  # 'ml' for model levels or 'pl' for pressure levels\n            'param': var,\n            \"stream\": \"oper\",\n            \"type\": \"an\",\n            \"date\": \"{}\".format(corrected_DT.strftime('%Y-%m-%d')),\n            \"time\": \"{}\".format(datetime.time.strftime(corrected_DT.time(), '%H:%M')),\n            # step: With type=an, step is always \"0\". With type=fc, step can\n            # be any of \"3/6/9/12\".\n            \"step\": \"0\",\n            \"area\": bbox,\n            \"grid\": [0.25, .25],\n            \"format\": \"netcdf\"}\n\n        try:\n            c.retrieve('reanalysis-era5-complete', dataDict, outname)\n        except Exception as e:\n            raise Exception\n\n\n    def _download_ecmwf(self, lat_min, lat_max, lat_step, lon_min, lon_max, lon_step, time, out):\n\"\"\" Used for HRES \"\"\"\n        from ecmwfapi import ECMWFService\n\n        server = ECMWFService(\"mars\")\n\n        # round to the closest legal time\n        corrected_DT = util.round_date(time, datetime.timedelta(hours=self._time_res))\n        if not corrected_DT == time:\n            logger.warning('Rounded given datetime from  %s to %s', time, corrected_DT)\n\n        if self._model_level_type == 'ml':\n            param = \"129/130/133/152\"\n        else:\n            param = \"129.128/130.128/133.128/152\"\n\n        server.execute(\n            {\n                'class': self._classname,\n                'dataset': self._dataset,\n                'expver': \"{}\".format(self._expver),\n                'resol': \"av\",\n                'stream': \"oper\",\n                'type': \"an\",\n                'levelist': \"all\",\n                'levtype': \"{}\".format(self._model_level_type),\n                'param': param,\n                'date': datetime.datetime.strftime(corrected_DT, \"%Y-%m-%d\"),\n                'time': \"{}\".format(datetime.time.strftime(corrected_DT.time(), '%H:%M')),\n                'step': \"0\",\n                'grid': \"{}/{}\".format(lon_step, lat_step),\n                'area': \"{}/{}/{}/{}\".format(lat_max, util.floorish(lon_min, 0.1), util.floorish(lat_min, 0.1), lon_max),\n                'format': \"netcdf\",\n            },\n            out\n        )\n\n\n    def _load_pressure_level(self, filename, *args, **kwargs):\n        with xr.open_dataset(filename) as block:\n            # Pull the data\n            z = np.squeeze(block['z'].values)\n            t = np.squeeze(block['t'].values)\n            q = np.squeeze(block['q'].values)\n            lats = np.squeeze(block.latitude.values)\n            lons = np.squeeze(block.longitude.values)\n            levels = np.squeeze(block.level.values) * 100\n\n        z = np.flip(z, axis=1)\n\n        # ECMWF appears to give me this backwards\n        if lats[0] &gt; lats[1]:\n            z = z[::-1]\n            t = t[:, ::-1]\n            q = q[:, ::-1]\n            lats = lats[::-1]\n        # Lons is usually ok, but we'll throw in a check to be safe\n        if lons[0] &gt; lons[1]:\n            z = z[..., ::-1]\n            t = t[..., ::-1]\n            q = q[..., ::-1]\n            lons = lons[::-1]\n        # pyproj gets fussy if the latitude is wrong, plus our\n        # interpolator isn't clever enough to pick up on the fact that\n        # they are the same\n        lons[lons &gt; 180] -= 360\n\n        self._t = t\n        self._q = q\n\n        geo_hgt = (z / self._g0).transpose(1, 2, 0)\n\n        # re-assign lons, lats to match heights\n        self._lons, self._lats = np.meshgrid(lons, lats)\n\n        # correct heights for latitude\n        self._get_heights(self._lats, geo_hgt)\n\n        self._p = np.broadcast_to(levels[np.newaxis, np.newaxis, :],\n                                  self._zs.shape)\n\n        # Re-structure from (heights, lats, lons) to (lons, lats, heights)\n        self._t = self._t.transpose(1, 2, 0)\n        self._q = self._q.transpose(1, 2, 0)\n        self._ys = self._lats.copy()\n        self._xs = self._lons.copy()\n\n        # flip z to go from surface to toa\n        self._p = np.flip(self._p, axis=2)\n        self._t = np.flip(self._t, axis=2)\n        self._q = np.flip(self._q, axis=2)\n\n\n    def _makeDataCubes(self, fname, verbose=False):\n'''\n        Create a cube of data representing temperature and relative humidity\n        at specified pressure levels\n        '''\n        # get ll_bounds\n        S, N, W, E = self._ll_bounds\n\n        with xr.open_dataset(fname) as ds:\n            ds = ds.assign_coords(longitude=(((ds.longitude + 180) % 360) - 180))\n\n            # mask based on query bounds\n            m1 = (S &lt;= ds.latitude) &amp; (N &gt;= ds.latitude)\n            m2 = (W &lt;= ds.longitude) &amp; (E &gt;= ds.longitude)\n            block = ds.where(m1 &amp; m2, drop=True)\n\n            # Pull the data\n            z = np.squeeze(block['z'].values)[0, ...]\n            t = np.squeeze(block['t'].values)\n            q = np.squeeze(block['q'].values)\n            lnsp = np.squeeze(block['lnsp'].values)[0, ...]\n            lats = np.squeeze(block.latitude.values)\n            lons = np.squeeze(block.longitude.values)\n\n            xs = lons.copy()\n            ys = lats.copy()\n\n        if z.size == 0:\n            raise RuntimeError('There is no data in z, '\n                               'you may have a problem with your mask')\n\n        return lats, lons, xs, ys, t, q, lnsp, z\n</code></pre>"},{"location":"reference/#RAiDER.models.ecmwf.ECMWF.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/ecmwf.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n    self._load_model_level(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.era5","title":"<code>era5</code>","text":""},{"location":"reference/#RAiDER.models.era5.ERA5","title":"<code>ERA5</code>","text":"<p>             Bases: <code>ECMWF</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/era5.py</code> <pre><code>class ERA5(ECMWF):\n    # I took this from\n    # https://www.ecmwf.int/en/forecasts/documentation-and-support/137-model-levels.\n    def __init__(self):\n        ECMWF.__init__(self)\n\n        self._humidityType = 'q'\n        self._expver = '0001'\n        self._classname = 'ea'\n        self._dataset = 'era5'\n        self._Name = 'ERA-5'\n        self._proj = CRS.from_epsg(4326)\n\n        # Tuple of min/max years where data is available.\n        lag_time = 3 # months\n        end_date = datetime.datetime.today() - relativedelta(months=lag_time)\n        self._valid_range = (datetime.datetime(1950, 1, 1), end_date)\n\n        # Availability lag time in days\n        self._lag_time = relativedelta(months=lag_time)\n\n        # Default, need to change to ml\n        self.setLevelType('ml')\n\n\n    def _fetch(self, out):\n'''\n        Fetch a weather model from ECMWF\n        '''\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n        time = self._time\n\n        # execute the search at ECMWF\n        self._get_from_cds(lat_min, lat_max, lon_min, lon_max, time, out)\n\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''Load either pressure or model level data'''\n        f = self.files[0] if f is None else f\n        if self._model_level_type == 'pl':\n            self._load_pressure_level(f, *args, **kwargs)\n        elif self._model_level_type == 'ml':\n            self._load_model_level(f, *args, **kwargs)\n        else:\n            raise RuntimeError(f'{self._model_level_type} is not a valid model type')\n</code></pre>"},{"location":"reference/#RAiDER.models.era5.ERA5.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Load either pressure or model level data</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/era5.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''Load either pressure or model level data'''\n    f = self.files[0] if f is None else f\n    if self._model_level_type == 'pl':\n        self._load_pressure_level(f, *args, **kwargs)\n    elif self._model_level_type == 'ml':\n        self._load_model_level(f, *args, **kwargs)\n    else:\n        raise RuntimeError(f'{self._model_level_type} is not a valid model type')\n</code></pre>"},{"location":"reference/#RAiDER.models.era5t","title":"<code>era5t</code>","text":""},{"location":"reference/#RAiDER.models.erai","title":"<code>erai</code>","text":""},{"location":"reference/#RAiDER.models.generateGACOSVRT","title":"<code>generateGACOSVRT</code>","text":""},{"location":"reference/#RAiDER.models.generateGACOSVRT.convertAllFiles","title":"<code>convertAllFiles(dirLoc)</code>","text":"<p>convert all RSC files to VRT files contained in dirLoc</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/generateGACOSVRT.py</code> <pre><code>def convertAllFiles(dirLoc):\n'''\n    convert all RSC files to VRT files contained in dirLoc\n    '''\n    import glob\n    files = glob.glob('*.rsc')\n    for f in files:\n        makeVRT(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.generateGACOSVRT.makeVRT","title":"<code>makeVRT(filename, dtype='Float32')</code>","text":"<p>Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/generateGACOSVRT.py</code> <pre><code>def makeVRT(filename, dtype='Float32'):\n'''\n    Use an RSC file to create a GDAL-compatible VRT file for opening GACOS weather model files\n    '''\n    fields = readRSC(filename)\n    string = vrtStr(fields['XMAX'], fields['YMAX'], fields['X_FIRST'], fields['Y_FIRST'], fields['X_STEP'], fields['Y_STEP'], filename.replace('.rsc', ''), dtype=dtype)\n    writeStringToFile(string, filename.replace('.rsc', '').replace('.ztd', '') + '.vrt')\n</code></pre>"},{"location":"reference/#RAiDER.models.generateGACOSVRT.writeStringToFile","title":"<code>writeStringToFile(string, filename)</code>","text":"<p>Write a string to a VRT file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/generateGACOSVRT.py</code> <pre><code>def writeStringToFile(string, filename):\n'''\n    Write a string to a VRT file\n    '''\n    with open(filename, 'w') as f:\n        f.write(string)\n</code></pre>"},{"location":"reference/#RAiDER.models.gmao","title":"<code>gmao</code>","text":""},{"location":"reference/#RAiDER.models.gmao.GMAO","title":"<code>GMAO</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/gmao.py</code> <pre><code>class GMAO(WeatherModel):\n    # I took this from GMAO model level weblink\n    # https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv\n    def __init__(self):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        self._humidityType = 'q'\n        self._model_level_type = 'ml'  # Default, pressure levels are 'pl'\n\n        self._classname = 'gmao'\n        self._dataset = 'gmao'\n\n        # Tuple of min/max years where data is available.\n        self._valid_range = (dt.datetime(2014, 2, 20), \"Present\")\n        self._lag_time = dt.timedelta(hours=24.0)  # Availability lag time in hours\n\n        # model constants\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n\n\n        # horizontal grid spacing\n        self._lat_res = 0.25\n        self._lon_res = 0.3125\n        self._x_res = 0.3125\n        self._y_res = 0.25\n\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n\n        self._Name = 'GMAO'\n        self.files = None\n        self._bounds = None\n\n        # Projection\n        self._proj = CRS.from_epsg(4326)\n\n\n    def _fetch(self, out):\n'''\n        Fetch weather model data from GMAO\n        '''\n        acqTime = self._time\n\n        # calculate the array indices for slicing the GMAO variable arrays\n        lat_min_ind = int((self._ll_bounds[0] - (-90.0)) / self._lat_res)\n        lat_max_ind = int((self._ll_bounds[1] - (-90.0)) / self._lat_res)\n        lon_min_ind = int((self._ll_bounds[2] - (-180.0)) / self._lon_res)\n        lon_max_ind = int((self._ll_bounds[3] - (-180.0)) / self._lon_res)\n\n        T0 = dt.datetime(2017, 12, 1, 0, 0, 0)\n        # round time to nearest third hour\n        corrected_DT = round_date(acqTime, dt.timedelta(hours=self._time_res))\n        if not corrected_DT == acqTime:\n            logger.warning('Rounded given datetime from  %s to %s', acqTime, corrected_DT)\n\n        DT = corrected_DT - T0\n        time_ind = int(DT.total_seconds() / 3600.0 / self._time_res)\n\n        ml_min = 0\n        ml_max = 71\n        if corrected_DT &gt;= T0:\n            # open the dataset and pull the data\n            url = 'https://opendap.nccs.nasa.gov/dods/GEOS-5/fp/0.25_deg/assim/inst3_3d_asm_Nv'\n            session = pydap.cas.urs.setup_session('username', 'password', check_url=url)\n            ds = pydap.client.open_url(url, session=session)\n            qv = ds['qv'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n\n            p = ds['pl'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n            t = ds['t'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n            h = ds['h'].array[\n                time_ind,\n                ml_min:(ml_max + 1),\n                lat_min_ind:(lat_max_ind + 1),\n                lon_min_ind:(lon_max_ind + 1)\n            ].data[0]\n\n        else:\n            root = 'https://portal.nccs.nasa.gov/datashare/gmao/geos-fp/das/Y{}/M{:02d}/D{:02d}'\n            base = f'GEOS.fp.asm.inst3_3d_asm_Nv.{corrected_DT.strftime(\"%Y%m%d\")}_{corrected_DT.hour:02}00.V01.nc4'\n            url = f'{root.format(corrected_DT.year, corrected_DT.month, corrected_DT.day)}/{base}'\n            f = '{}_raw{}'.format(*os.path.splitext(out))\n            if not os.path.exists(f):\n                logger.info('Fetching URL: %s', url)\n                session = requests_retry_session()\n                resp = session.get(url, stream=True)\n                assert resp.ok, f'Could not access url for datetime: {corrected_DT}'\n                with open(f, 'wb') as fh:\n                    shutil.copyfileobj(resp.raw, fh)\n            else:\n                logger.warning('Weather model already exists, skipping download')\n\n            with h5py.File(f, 'r') as ds:\n                q = ds['QV'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                p = ds['PL'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                t = ds['T'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n                h = ds['H'][0, :, lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)]\n            os.remove(f)\n\n        lats = np.arange(\n            (-90 + lat_min_ind * self._lat_res),\n            (-90 + (lat_max_ind + 1) * self._lat_res),\n            self._lat_res\n        )\n        lons = np.arange(\n            (-180 + lon_min_ind * self._lon_res),\n            (-180 + (lon_max_ind + 1) * self._lon_res),\n            self._lon_res\n        )\n\n        try:\n            # Note that lat/lon gets written twice for GMAO because they are the same as y/x\n            writeWeatherVars2NETCDF4(self, lats, lons, h, qv, p, t, outName=out)\n        except Exception:\n            logger.exception(\"Unable to save weathermodel to file\")\n\n\n    def load_weather(self, f=None):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n        self._load_model_level(f)\n\n\n    def _load_model_level(self, filename):\n'''\n        Get the variables from the GMAO link using OpenDAP\n        '''\n\n        # adding the import here should become absolute when transition to netcdf\n        from netCDF4 import Dataset\n        with Dataset(filename, mode='r') as f:\n            lons = np.array(f.variables['x'][:])\n            lats = np.array(f.variables['y'][:])\n            h = np.array(f.variables['H'][:])\n            q = np.array(f.variables['QV'][:])\n            p = np.array(f.variables['PL'][:])\n            t = np.array(f.variables['T'][:])\n\n        # restructure the 1-D lat/lon in regular 2D grid\n        _lons, _lats= np.meshgrid(lons, lats)\n\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        p = np.transpose(p)\n        q = np.transpose(q)\n        t = np.transpose(t)\n        h = np.transpose(h)\n\n        # check this\n        # data cube format should be lats,lons,heights\n        p = p.swapaxes(0, 1)\n        q = q.swapaxes(0, 1)\n        t = t.swapaxes(0, 1)\n        h = h.swapaxes(0, 1)\n\n        # For some reason z is opposite the others\n        p = np.flip(p, axis=2)\n        q = np.flip(q, axis=2)\n        t = np.flip(t, axis=2)\n        h = np.flip(h, axis=2)\n\n        # assign the regular-grid (lat/lon/h) variables\n\n        self._p = p\n        self._q = q\n        self._t = t\n        self._lats = _lats\n        self._lons = _lons\n        self._xs = _lons\n        self._ys = _lats\n        self._zs = h\n</code></pre>"},{"location":"reference/#RAiDER.models.gmao.GMAO.load_weather","title":"<code>load_weather(f=None)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/gmao.py</code> <pre><code>def load_weather(self, f=None):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n    self._load_model_level(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.hres","title":"<code>hres</code>","text":""},{"location":"reference/#RAiDER.models.hres.HRES","title":"<code>HRES</code>","text":"<p>             Bases: <code>ECMWF</code></p> <p>Implement ECMWF models</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hres.py</code> <pre><code>class HRES(ECMWF):\n'''\n    Implement ECMWF models\n    '''\n\n    def __init__(self, level_type='ml'):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        # model constants\n        self._k1 = 0.776   # [K/Pa]\n        self._k2 = 0.233   # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n\n        # 9 km horizontal grid spacing. This is only used for extending the download-buffer, i.e. not in subsequent processing.\n        self._lon_res = 9. / 111  # 0.08108115\n        self._lat_res = 9. / 111  # 0.08108115\n        self._x_res = 9. / 111  # 0.08108115\n        self._y_res = 9. / 111  # 0.08108115\n\n        self._humidityType = 'q'\n        # Default, pressure levels are 'pl'\n        self._expver = '1'\n        self._classname = 'od'\n        self._dataset = 'hres'\n        self._Name = 'HRES'\n        self._proj = CRS.from_epsg(4326)\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n        # Tuple of min/max years where data is available.\n        self._valid_range = (datetime.datetime(1983, 4, 20), \"Present\")\n        # Availability lag time in days\n        self._lag_time = datetime.timedelta(hours=6)\n\n        self.setLevelType('ml')\n\n    def update_a_b(self):\n        # Before 2013-06-26, there were only 91 model levels. The mapping coefficients below are extracted\n        # based on https://www.ecmwf.int/en/forecasts/documentation-and-support/91-model-levels\n        self._levels = 91\n        self._zlevels = np.flipud(LEVELS_91_HEIGHTS)\n        self._a = A_91_HRES\n        self._b = B_91_HRES\n\n    def load_weather(self, f=None):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n\n        if self._model_level_type == 'ml':\n            if (self._time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n                self.update_a_b()\n            self._load_model_level(f)\n        elif self._model_level_type == 'pl':\n            self._load_pressure_levels(f)\n\n    def _fetch(self,out):\n'''\n        Fetch a weather model from ECMWF\n        '''\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n        time = self._time\n\n        if (time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n            self.update_a_b()\n\n        # execute the search at ECMWF\n        self._download_ecmwf(lat_min, lat_max, self._lat_res, lon_min, lon_max, self._lon_res, time, out)\n</code></pre>"},{"location":"reference/#RAiDER.models.hres.HRES.load_weather","title":"<code>load_weather(f=None)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hres.py</code> <pre><code>def load_weather(self, f=None):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n\n    if self._model_level_type == 'ml':\n        if (self._time &lt; datetime.datetime(2013, 6, 26, 0, 0, 0)):\n            self.update_a_b()\n        self._load_model_level(f)\n    elif self._model_level_type == 'pl':\n        self._load_pressure_levels(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr","title":"<code>hrrr</code>","text":""},{"location":"reference/#RAiDER.models.hrrr.HRRR","title":"<code>HRRR</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>class HRRR(WeatherModel):\n    def __init__(self):\n        # initialize a weather model\n        super().__init__()\n\n        self._humidityType = 'q'\n        self._model_level_type = 'pl'  # Default, pressure levels are 'pl'\n        self._expver = '0001'\n        self._classname = 'hrrr'\n        self._dataset = 'hrrr'\n\n        self._time_res = TIME_RES[self._dataset.upper()]\n\n        # Tuple of min/max years where data is available.\n        self._valid_range = (datetime.datetime(2016, 7, 15), \"Present\")\n        self._lag_time = datetime.timedelta(hours=3)  # Availability lag time in days\n\n        # model constants\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        # 3 km horizontal grid spacing\n        self._lat_res = 3. / 111\n        self._lon_res = 3. / 111\n        self._x_res = 3.\n        self._y_res = 3.\n\n        self._Nproc = 1\n        self._Name = 'HRRR'\n        self._Npl = 0\n        self.files = None\n        self._bounds = None\n\n        # Projection\n        # NOTE: The HRRR projection will get read directly from the downloaded weather model file; however,\n        # we also define it here so that the projection can be used without downloading any data. This is\n        # used for consistency with the other weather models and allows for some nice features, such as\n        # buffering.\n\n        # See https://github.com/blaylockbk/pyBKB_v2/blob/master/demos/HRRR_earthRelative_vs_gridRelative_winds.ipynb and code lower down\n        # '262.5:38.5:38.5:38.5 237.280472:1799:3000.00 21.138123:1059:3000.00'\n        # 'lov:latin1:latin2:latd lon1:nx:dx lat1:ny:dy'\n        # LCC parameters\n        lon0 = 262.5\n        lat0 = 38.5\n        lat1 = 38.5\n        lat2 = 38.5\n        x0 = 0\n        y0 = 0\n        earth_radius = 6371229\n        self._proj = CRS(f'+proj=lcc +lat_1={lat1} +lat_2={lat2} +lat_0={lat0} '\\\n                 f'+lon_0={lon0} +x_0={x0} +y_0={y0} +a={earth_radius} '\\\n                 f'+b={earth_radius} +units=m +no_defs')\n        self._valid_bounds = HRRR_CONUS_COVERAGE_POLYGON\n        self.setLevelType('nat')\n\n\n    def __model_levels__(self):\n        self._levels  = 50\n        self._zlevels = np.flipud(LEVELS_50_HEIGHTS)\n\n\n    def __pressure_levels__(self):\n        raise NotImplementedError('Pressure levels do not go high enough for HRRR.')\n\n\n    def _fetch(self,  out):\n'''\n        Fetch weather model data from HRRR\n        '''\n        self._files = out\n        corrected_DT = round_date(self._time, datetime.timedelta(hours=self._time_res))\n        self.checkTime(corrected_DT)\n        if not corrected_DT == self._time:\n            logger.info('Rounded given datetime from  %s to %s', self._time, corrected_DT)\n\n        # HRRR uses 0-360 longitude, so we need to convert the bounds to that\n        bounds = self._ll_bounds.copy()\n        bounds[2:] = np.mod(bounds[2:], 360)\n\n        download_hrrr_file(bounds, corrected_DT, out, 'hrrr', self._model_level_type)\n\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''\n        Load a weather model into a python weatherModel object, from self.files if no\n        filename is passed.\n        '''\n        if f is None:\n            f = self.files[0] if isinstance(self.files, list) else self.files\n\n\n        _xs, _ys, _lons, _lats, qs, temps, pres, geo_hgt, proj = load_weather_hrrr(f)\n\n        # convert geopotential height to geometric height\n        self._get_heights(_lats, geo_hgt)\n\n        self._t = temps\n        self._q = qs\n        self._p = pres\n        self._xs = _xs\n        self._ys = _ys\n        self._lats = _lats\n        self._lons = _lons\n        self._proj = proj\n\n\n    def checkValidBounds(self: WeatherModel, ll_bounds: np.ndarray):\n'''\n        Checks whether the given bounding box is valid for the HRRR or HRRRAK\n        (i.e., intersects with the model domain at all)\n\n        Args:\n        ll_bounds : np.ndarray\n\n        Returns:\n            The weather model object\n        '''\n        S, N, W, E = ll_bounds\n        aoi = box(W, S, E, N)\n        if self._valid_bounds.contains(aoi):\n            Mod = self\n\n        elif aoi.intersects(self._valid_bounds):\n            Mod = self\n            logger.critical('The HRRR weather model extent does not completely cover your AOI!')\n\n        else:\n            Mod = HRRRAK()\n            # valid bounds are in 0-&gt;360 to account for dateline crossing\n            W, E = np.mod([W, E], 360)\n            aoi  = box(W, S, E, N)\n            if Mod._valid_bounds.contains(aoi):\n                pass\n            elif aoi.intersects(Mod._valid_bounds):\n                logger.critical('The HRRR-AK weather model extent does not completely cover your AOI!')\n\n            else:\n                raise ValueError('The requested location is unavailable for HRRR')\n\n        return Mod\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr.HRRR.checkValidBounds","title":"<code>checkValidBounds(ll_bounds)</code>","text":"<p>Checks whether the given bounding box is valid for the HRRR or HRRRAK (i.e., intersects with the model domain at all)</p> <p>Args: ll_bounds : np.ndarray</p> <p>Returns:</p> Type Description <p>The weather model object</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def checkValidBounds(self: WeatherModel, ll_bounds: np.ndarray):\n'''\n    Checks whether the given bounding box is valid for the HRRR or HRRRAK\n    (i.e., intersects with the model domain at all)\n\n    Args:\n    ll_bounds : np.ndarray\n\n    Returns:\n        The weather model object\n    '''\n    S, N, W, E = ll_bounds\n    aoi = box(W, S, E, N)\n    if self._valid_bounds.contains(aoi):\n        Mod = self\n\n    elif aoi.intersects(self._valid_bounds):\n        Mod = self\n        logger.critical('The HRRR weather model extent does not completely cover your AOI!')\n\n    else:\n        Mod = HRRRAK()\n        # valid bounds are in 0-&gt;360 to account for dateline crossing\n        W, E = np.mod([W, E], 360)\n        aoi  = box(W, S, E, N)\n        if Mod._valid_bounds.contains(aoi):\n            pass\n        elif aoi.intersects(Mod._valid_bounds):\n            logger.critical('The HRRR-AK weather model extent does not completely cover your AOI!')\n\n        else:\n            raise ValueError('The requested location is unavailable for HRRR')\n\n    return Mod\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr.HRRR.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Load a weather model into a python weatherModel object, from self.files if no filename is passed.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''\n    Load a weather model into a python weatherModel object, from self.files if no\n    filename is passed.\n    '''\n    if f is None:\n        f = self.files[0] if isinstance(self.files, list) else self.files\n\n\n    _xs, _ys, _lons, _lats, qs, temps, pres, geo_hgt, proj = load_weather_hrrr(f)\n\n    # convert geopotential height to geometric height\n    self._get_heights(_lats, geo_hgt)\n\n    self._t = temps\n    self._q = qs\n    self._p = pres\n    self._xs = _xs\n    self._ys = _ys\n    self._lats = _lats\n    self._lons = _lons\n    self._proj = proj\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr.download_hrrr_file","title":"<code>download_hrrr_file(ll_bounds, DATE, out, model='hrrr', product='nat', fxx=0, verbose=False)</code>","text":"<p>Download a HRRR weather model using Herbie</p> <p>Returns:</p> Type Description <p>None, writes data to a netcdf file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def download_hrrr_file(ll_bounds, DATE, out, model='hrrr', product='nat', fxx=0, verbose=False):\n'''\n    Download a HRRR weather model using Herbie\n\n    Args:\n        DATE (Python datetime)  - Datetime as a Python datetime. Herbie will automatically return the closest valid time,\n                                    which is currently hourly.\n        out (string)            - output location as a string\n        model (string)          - model can be \"hrrr\" or \"hrrrak\"\n        product (string)        - 'prs' for pressure levels, 'nat' for native levels\n        fxx (int)               - forecast time in hours. Can be up to 48 for 00/06/12/18\n        verbose (bool)          - True for extra printout of information\n\n    Returns:\n        None, writes data to a netcdf file\n    '''\n    H = Herbie(\n        DATE.strftime('%Y-%m-%d %H:%M'),\n        model=model,\n        product=product,\n        fxx=fxx,\n        overwrite=False,\n        verbose=True,\n        save_dir=Path(os.path.dirname(out)),\n    )\n\n\n    # Iterate through the list of datasets\n    try:\n        ds_list = H.xarray(\":(SPFH|PRES|TMP|HGT):\", verbose=verbose)\n    except ValueError as E:\n        logger.error (E)\n        raise ValueError\n\n    ds_out = None\n\n    for ds in ds_list:\n        if 'isobaricInhPa' in ds._coord_names:\n            ds_out = ds\n            coord = 'isobaricInhPa'\n            break\n        elif 'hybrid' in ds._coord_names:\n            ds_out = ds\n            coord = 'hybrid'\n            break\n\n    # subset the full file by AOI\n    x_min, x_max, y_min, y_max = get_bounds_indices(\n        ll_bounds,\n        ds_out.latitude.to_numpy(),\n        ds_out.longitude.to_numpy(),\n    )\n\n    # bookkeepping\n    ds_out = ds_out.rename({'gh': 'z', coord: 'levels'})\n    ny, nx = ds_out['longitude'].shape\n\n    # projection information\n    ds_out[\"proj\"] = int()\n    for k, v in CRS.from_user_input(ds.herbie.crs).to_cf().items():\n        ds_out.proj.attrs[k] = v\n    for var in ds_out.data_vars:\n        ds_out[var].attrs['grid_mapping'] = 'proj'\n\n\n    # pull the grid information\n    proj = CRS.from_cf(ds_out['proj'].attrs)\n    t = Transformer.from_crs(4326, proj, always_xy=True)\n    xl, yl = t.transform(ds_out['longitude'].values, ds_out['latitude'].values)\n    W, E, S, N = np.nanmin(xl), np.nanmax(xl), np.nanmin(yl), np.nanmax(yl)\n\n    grid_x = 3000 # meters\n    grid_y = 3000 # meters\n    xs = np.arange(W, E+grid_x/2, grid_x)\n    ys = np.arange(S, N+grid_y/2, grid_y)\n\n    ds_out['x'] = xs\n    ds_out['y'] = ys\n\n    ds_sub = ds_out.isel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n    ds_sub.to_netcdf(out, engine='netcdf4')\n\n    return\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr.get_bounds_indices","title":"<code>get_bounds_indices(SNWE, lats, lons)</code>","text":"<p>Convert SNWE lat/lon bounds to index bounds</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def get_bounds_indices(SNWE, lats, lons):\n'''\n    Convert SNWE lat/lon bounds to index bounds\n    '''\n    # Unpack the bounds and find the relevent indices\n    S, N, W, E = SNWE\n\n    # Need to account for crossing the international date line\n    if W &lt; E:\n        m1 = (S &lt;= lats) &amp; (N &gt;= lats) &amp; (W &lt;= lons) &amp; (E &gt;= lons)\n    else:\n        raise ValueError(\n            'Longitude is either flipped or you are crossing the international date line;' +\n            'if the latter please give me longitudes from 0-360'\n        )\n\n    if np.sum(m1) == 0:\n        lons = np.mod(lons, 360)\n        W, E = np.mod([W, E], 360)\n        m1 = (S &lt;= lats) &amp; (N &gt;= lats) &amp; (W &lt;= lons) &amp; (E &gt;= lons)\n        if np.sum(m1) == 0:\n            raise RuntimeError('Area of Interest has no overlap with the HRRR model available extent')\n\n    # Y extent\n    shp = lats.shape\n    m1_y = np.argwhere(np.sum(m1, axis=1) != 0)\n    y_min = max(m1_y[0][0], 0)\n    y_max = min(m1_y[-1][0], shp[0])\n    m1_y = None\n\n    # X extent\n    m1_x = np.argwhere(np.sum(m1, axis=0) != 0)\n    x_min = max(m1_x[0][0], 0)\n    x_max = min(m1_x[-1][0], shp[1])\n    m1_x = None\n    m1 = None\n\n    return x_min, x_max, y_min, y_max\n</code></pre>"},{"location":"reference/#RAiDER.models.hrrr.load_weather_hrrr","title":"<code>load_weather_hrrr(filename)</code>","text":"<p>Loads a weather model from a HRRR file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/hrrr.py</code> <pre><code>def load_weather_hrrr(filename):\n'''\n    Loads a weather model from a HRRR file\n    '''\n    # read data from the netcdf file\n    ds = xarray.open_dataset(filename, engine='netcdf4')\n    # Pull the relevant data from the file\n    pl = ds.levels.values\n    pres = ds['pres'].values.transpose(1, 2, 0)\n    xArr = ds['x'].values\n    yArr = ds['y'].values\n    lats = ds['latitude'].values\n    lons = ds['longitude'].values\n    temps = ds['t'].values.transpose(1, 2, 0)\n    qs = ds['q'].values.transpose(1, 2, 0)\n    geo_hgt = ds['z'].values.transpose(1, 2, 0)\n\n    proj = CRS.from_cf(ds['proj'].attrs)\n\n    lons[lons &gt; 180] -= 360\n\n    # data cube format should be lats,lons,heights\n    _xs = np.broadcast_to(xArr[np.newaxis, :, np.newaxis],\n                            geo_hgt.shape)\n    _ys = np.broadcast_to(yArr[:, np.newaxis, np.newaxis],\n                            geo_hgt.shape)\n\n    return _xs, _ys, lons, lats, qs, temps, pres, geo_hgt, proj\n</code></pre>"},{"location":"reference/#RAiDER.models.merra2","title":"<code>merra2</code>","text":""},{"location":"reference/#RAiDER.models.merra2.MERRA2","title":"<code>MERRA2</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/merra2.py</code> <pre><code>class MERRA2(WeatherModel):\n    # I took this from MERRA-2 model level weblink\n    # https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/\n    def __init__(self):\n\n        import calendar\n\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        self._humidityType = 'q'\n        self._model_level_type = 'ml'  # Default, pressure levels are 'pl'\n\n        self._classname = 'merra2'\n        self._dataset = 'merra2'\n\n        # Tuple of min/max years where data is available.\n        utcnow = dt.datetime.utcnow()\n        enddate = dt.datetime(utcnow.year, utcnow.month, 15) - dt.timedelta(days=60)\n        enddate = dt.datetime(enddate.year, enddate.month, calendar.monthrange(enddate.year, enddate.month)[1])\n        self._valid_range = (dt.datetime(1980, 1, 1), \"Present\")\n        lag_time = utcnow - enddate\n        self._lag_time = dt.timedelta(days=lag_time.days)  # Availability lag time in days\n        self._time_res = 1\n\n        # model constants\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        # horizontal grid spacing\n        self._lat_res = 0.5\n        self._lon_res = 0.625\n        self._x_res = 0.625\n        self._y_res = 0.5\n\n        self._Name = 'MERRA2'\n        self.files = None\n        self._bounds = None\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n\n        # Projection\n        self._proj = CRS.from_epsg(4326)\n\n    def _fetch(self, out):\n'''\n        Fetch weather model data from GMAO: note we only extract the lat/lon bounds for this weather model; fetching data is not needed here as we don't actually download any data using OpenDAP\n        '''\n        time = self._time \n\n        # check whether the file already exists\n        if os.path.exists(out):\n            return\n\n        # calculate the array indices for slicing the GMAO variable arrays\n        lat_min_ind = int((self._ll_bounds[0] - (-90.0)) / self._lat_res)\n        lat_max_ind = int((self._ll_bounds[1] - (-90.0)) / self._lat_res)\n        lon_min_ind = int((self._ll_bounds[2] - (-180.0)) / self._lon_res)\n        lon_max_ind = int((self._ll_bounds[3] - (-180.0)) / self._lon_res)\n\n        lats = np.arange(\n            (-90 + lat_min_ind * self._lat_res),\n            (-90 + (lat_max_ind + 1) * self._lat_res),\n            self._lat_res\n        )\n        lons = np.arange(\n            (-180 + lon_min_ind * self._lon_res),\n            (-180 + (lon_max_ind + 1) * self._lon_res),\n            self._lon_res\n        )\n\n        if time.year &lt; 1992:\n            url_sub = 100\n        elif time.year &lt; 2001:\n            url_sub = 200\n        elif time.year &lt; 2011:\n            url_sub = 300\n        else:\n            url_sub = 400\n\n        T0 = dt.datetime(time.year, time.month, time.day, 0, 0, 0)\n        DT = time - T0\n        time_ind = int(DT.total_seconds() / 3600.0 / 3.0)\n\n        ml_min = 0\n        ml_max = 71\n\n        # Earthdata credentials\n        earthdata_usr, earthdata_pwd = read_EarthData_loginInfo(EARTHDATA_RC)\n\n        # open the dataset and pull the data\n        url = 'https://goldsmr5.gesdisc.eosdis.nasa.gov:443/opendap/MERRA2/M2I3NVASM.5.12.4/' + time.strftime('%Y/%m') + '/MERRA2_' + str(url_sub) + '.inst3_3d_asm_Nv.' + time.strftime('%Y%m%d') + '.nc4'\n        session = pydap.cas.urs.setup_session(earthdata_usr, earthdata_pwd, check_url=url)\n        ds = pydap.client.open_url(url, session=session)\n\n        ############# The MERRA-2 server changes the pydap data retrieval format frequently between these two formats; so better to retain both of them rather than only using either one of them #############\n        try:\n            q = ds['QV'].data[0][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            p = ds['PL'].data[0][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            t = ds['T'].data[0][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            h = ds['H'].data[0][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n        except IndexError:\n            q = ds['QV'].data[time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            p = ds['PL'].data[time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            t = ds['T'].data[time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            h = ds['H'].data[time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n        except AttributeError:\n            q = ds['QV'][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            p = ds['PL'][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            t = ds['T'][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n            h = ds['H'][time_ind, ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)][0]\n        except BaseException:\n            logger.exception(\"MERRA-2: Unable to read weathermodel data\")\n        ########################################################################################################################\n\n        try:\n            writeWeatherVars2NETCDF4(self, lats, lons, h, q, p, t, outName=out)\n        except Exception as e:\n            logger.debug(e)\n            logger.exception(\"MERRA-2: Unable to save weathermodel to file\")\n            raise RuntimeError('MERRA-2 failed with the following error: {}'.format(e))\n\n    def load_weather(self,  f=None, *args, **kwargs):\n'''\n        Consistent class method to be implemented across all weather model types.\n        As a result of calling this method, all of the variables (x, y, z, p, q,\n        t, wet_refractivity, hydrostatic refractivity, e) should be fully\n        populated.\n        '''\n        f = self.files[0] if f is None else f\n        self._load_model_level(f)\n\n    def _load_model_level(self, filename):\n'''\n        Get the variables from the GMAO link using OpenDAP\n        '''\n\n        # adding the import here should become absolute when transition to netcdf\n        from netCDF4 import Dataset\n        with Dataset(filename, mode='r') as f:\n            lons = np.array(f.variables['x'][:])\n            lats = np.array(f.variables['y'][:])\n            h = np.array(f.variables['H'][:])\n            q = np.array(f.variables['QV'][:])\n            p = np.array(f.variables['PL'][:])\n            t = np.array(f.variables['T'][:])\n\n        # restructure the 3-D lat/lon/h in regular grid\n        _lons = np.broadcast_to(lons[np.newaxis, np.newaxis, :], t.shape)\n        _lats = np.broadcast_to(lats[np.newaxis, :, np.newaxis], t.shape)\n\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        p = np.transpose(p)\n        q = np.transpose(q)\n        t = np.transpose(t)\n        h = np.transpose(h)\n        _lats = np.transpose(_lats)\n        _lons = np.transpose(_lons)\n\n        # check this\n        # data cube format should be lats,lons,heights\n        p = p.swapaxes(0, 1)\n        q = q.swapaxes(0, 1)\n        t = t.swapaxes(0, 1)\n        h = h.swapaxes(0, 1)\n        _lats = _lats.swapaxes(0, 1)\n        _lons = _lons.swapaxes(0, 1)\n\n        # For some reason z is opposite the others\n        p = np.flip(p, axis=2)\n        q = np.flip(q, axis=2)\n        t = np.flip(t, axis=2)\n        h = np.flip(h, axis=2)\n        _lats = np.flip(_lats, axis=2)\n        _lons = np.flip(_lons, axis=2)\n\n        # assign the regular-grid (lat/lon/h) variables\n\n        self._p = p\n        self._q = q\n        self._t = t\n        self._lats = _lats\n        self._lons = _lons\n        self._xs = _lons\n        self._ys = _lats\n        self._zs = h\n</code></pre>"},{"location":"reference/#RAiDER.models.merra2.MERRA2.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Consistent class method to be implemented across all weather model types. As a result of calling this method, all of the variables (x, y, z, p, q, t, wet_refractivity, hydrostatic refractivity, e) should be fully populated.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/merra2.py</code> <pre><code>def load_weather(self,  f=None, *args, **kwargs):\n'''\n    Consistent class method to be implemented across all weather model types.\n    As a result of calling this method, all of the variables (x, y, z, p, q,\n    t, wet_refractivity, hydrostatic refractivity, e) should be fully\n    populated.\n    '''\n    f = self.files[0] if f is None else f\n    self._load_model_level(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.model_levels","title":"<code>model_levels</code>","text":"<p>Pre-defined model levels and a, b constants for the different weather models</p> <p>NOTE: The fixed heights used here are from ECMWF's geometric altitudes (https://confluence.ecmwf.int/display/UDOC/L137+model+level+definitions), where \"geopotential altitude is calculated from a mathematical model that adjusts the altitude to include the variation of gravity with height, while geometric altitude is the standard direct vertical distance above mean sea level (MSL).\" - Wikipedia.org, https://en.wikipedia.org/wiki/International_Standard_Atmosphere</p>"},{"location":"reference/#RAiDER.models.ncmr","title":"<code>ncmr</code>","text":"<p>Created on Wed Sep  9 10:26:44 2020 @author: prashant Modified by Yang Lei, GPS/Caltech</p>"},{"location":"reference/#RAiDER.models.ncmr.NCMR","title":"<code>NCMR</code>","text":"<p>             Bases: <code>WeatherModel</code></p> <p>Implement NCMRWF NCUM (named as NCMR) model in future</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/ncmr.py</code> <pre><code>class NCMR(WeatherModel):\n'''\n    Implement NCMRWF NCUM (named as NCMR) model in future\n    '''\n\n    def __init__(self):\n        # initialize a weather model\n        WeatherModel.__init__(self)\n\n        self._humidityType = 'q'                     # q for specific humidity and rh for relative humidity\n        self._model_level_type = 'ml'                # Default, pressure levels are 'pl', and model levels are \"ml\"\n        self._classname = 'ncmr'                     # name of the custom weather model\n        self._dataset = 'ncmr'                       # same name as above\n        self._Name = 'NCMR'                          # name of the new weather model (in Capital)\n        self._time_res = TIME_RES[self._dataset.upper()]\n\n        # Tuple of min/max years where data is available.\n        self._valid_range = (datetime.datetime(2015, 12, 1), \"Present\")\n        # Availability lag time in days/hours\n        self._lag_time = datetime.timedelta(hours=6)\n\n        # model constants\n        self._k1 = 0.776   # [K/Pa]\n        self._k2 = 0.233   # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        # horizontal grid spacing\n        self._lon_res = .17578125                  # grid spacing in longitude\n        self._lat_res = .11718750                  # grid spacing in latitude\n\n        self._x_res = .17578125                  # same as longitude\n        self._y_res = .11718750                  # same as latitude\n\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n\n        self._bounds = None\n\n        # Projection\n        self._proj = CRS.from_epsg(4326)\n\n    def _fetch(self, out):\n'''\n        Fetch weather model data from NCMR: note we only extract the lat/lon bounds for this weather model;\n        fetching data is not needed here as we don't actually download data , data exist in same system\n        '''\n        time = self._time\n\n        # Auxillary function:\n'''\n        download data of the NCMR model and save it in desired location\n        '''\n        self._files = self._download_ncmr_file(out, time, self._ll_bounds)\n\n    def load_weather(self, f=None, *args, **kwargs):\n'''\n        Load NCMR model variables from existing file\n        '''\n        f = self.files[0] if f is None else f\n\n        # bounding box plus a buffer\n        lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n        self._bounds = (lat_min, lat_max, lon_min, lon_max)\n\n        self._makeDataCubes(f)\n\n    def _download_ncmr_file(self, out, date_time, bounding_box):\n'''\n        Download weather model data (whole globe) from NCMR weblink, crop it to the region of interest, and save the cropped data as a standard .nc file of RAiDER (e.g. \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\");\n        Temporarily download data from NCMR ftp 'https://ftp.ncmrwf.gov.in/pub/outgoing/SAC/NCUM_OSF/' and copied in weather_models folder\n        '''\n\n        from netCDF4 import Dataset\n\n        ############# Use these lines and modify the link when actually downloading NCMR data from a weblink #############\n        url, username, password = read_NCMR_loginInfo()\n        filename = os.path.basename(out)\n        url = f'ftp://{username}:{password}@{url}/TEST/{filename}'\n        filepath = f'{out[:-3]}_raw.nc'\n        if not os.path.exists(filepath):\n            logger.info('Fetching URL: %s', url)\n            local_filename, headers = urllib.request.urlretrieve(url, filepath, show_progress)\n        else:\n            logger.warning('Weather model already exists, skipping download')\n        ########################################################################################################################\n\n        ############# For debugging: use pre-downloaded files; Remove/comment out it when actually downloading NCMR data from a weblink #############\n#        filepath = os.path.dirname(out) + '/NCUM_ana_mdllev_20180701_00z.nc'\n        ########################################################################################################################\n\n        # calculate the array indices for slicing the GMAO variable arrays\n        lat_min_ind = int((self._bounds[0] - (-89.94141)) / self._lat_res)\n        lat_max_ind = int((self._bounds[1] - (-89.94141)) / self._lat_res)\n        if (self._bounds[2] &lt; 0.0):\n            lon_min_ind = int((self._bounds[2] + 360.0 - (0.087890625)) / self._lon_res)\n        else:\n            lon_min_ind = int((self._bounds[2] - (0.087890625)) / self._lon_res)\n        if (self._bounds[3] &lt; 0.0):\n            lon_max_ind = int((self._bounds[3] + 360.0 - (0.087890625)) / self._lon_res)\n        else:\n            lon_max_ind = int((self._bounds[3] - (0.087890625)) / self._lon_res)\n\n        ml_min = 0\n        ml_max = 70\n\n        with Dataset(filepath, 'r', maskandscale=True) as f:\n            lats = f.variables['latitude'][lat_min_ind:(lat_max_ind + 1)].copy()\n            if (self._bounds[2] * self._bounds[3] &lt; 0):\n                lons1 = f.variables['longitude'][lon_min_ind:].copy()\n                lons2 = f.variables['longitude'][0:(lon_max_ind + 1)].copy()\n                lons = np.append(lons1, lons2)\n            else:\n                lons = f.variables['longitude'][lon_min_ind:(lon_max_ind + 1)].copy()\n            if (self._bounds[2] * self._bounds[3] &lt; 0):\n                t1 = f.variables['air_temperature'][ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:].copy()\n                t2 = f.variables['air_temperature'][ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), 0:(lon_max_ind + 1)].copy()\n                t = np.append(t1, t2, axis=2)\n            else:\n                t = f.variables['air_temperature'][ml_min:(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)].copy()\n\n            # Skipping first pressure levels (below 20 meter)\n            if (self._bounds[2] * self._bounds[3] &lt; 0):\n                q1 = f.variables['specific_humidity'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:].copy()\n                q2 = f.variables['specific_humidity'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), 0:(lon_max_ind + 1)].copy()\n                q = np.append(q1, q2, axis=2)\n            else:\n                q = f.variables['specific_humidity'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)].copy()\n            if (self._bounds[2] * self._bounds[3] &lt; 0):\n                p1 = f.variables['air_pressure'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:].copy()\n                p2 = f.variables['air_pressure'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), 0:(lon_max_ind + 1)].copy()\n                p = np.append(p1, p2, axis=2)\n            else:\n                p = f.variables['air_pressure'][(ml_min + 1):(ml_max + 1), lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)].copy()\n\n            level_hgt = f.variables['level_height'][(ml_min + 1):(ml_max + 1)].copy()\n            if (self._bounds[2] * self._bounds[3] &lt; 0):\n                surface_alt1 = f.variables['surface_altitude'][lat_min_ind:(lat_max_ind + 1), lon_min_ind:].copy()\n                surface_alt2 = f.variables['surface_altitude'][lat_min_ind:(lat_max_ind + 1), 0:(lon_max_ind + 1)].copy()\n                surface_alt = np.append(surface_alt1, surface_alt2, axis=1)\n            else:\n                surface_alt = f.variables['surface_altitude'][lat_min_ind:(lat_max_ind + 1), lon_min_ind:(lon_max_ind + 1)].copy()\n\n            hgt = np.zeros([len(level_hgt), len(surface_alt[:, 1]), len(surface_alt[1, :])])\n            for i in range(len(level_hgt)):\n                hgt[i, :, :] = surface_alt[:, :] + level_hgt[i]\n\n            lons[lons &gt; 180] -= 360\n\n        ############# For debugging: comment it out when using pre-downloaded raw data files and don't want to remove them for test; Uncomment it when actually downloading NCMR data from a weblink #############\n        os.remove(filepath)\n        ########################################################################################################################\n\n        try:\n            writeWeatherVars2NETCDF4(self, lats, lons, hgt, q, p, t, outName=out)\n        except Exception:\n            logger.exception(\"Unable to save weathermodel to file\")\n\n    def _makeDataCubes(self, filename):\n'''\n        Get the variables from the saved .nc file (named as \"NCMR_YYYY_MM_DD_THH_MM_SS.nc\")\n        '''\n        from netCDF4 import Dataset\n\n        # adding the import here should become absolute when transition to netcdf\n        with Dataset(filename, mode='r') as f:\n            lons = np.array(f.variables['x'][:])\n            lats = np.array(f.variables['y'][:])\n            hgt = np.array(f.variables['H'][:])\n            q = np.array(f.variables['QV'][:])\n            p = np.array(f.variables['PL'][:])\n            t = np.array(f.variables['T'][:])\n\n        # re-assign lons, lats to match heights\n        _lons = np.broadcast_to(lons[np.newaxis, np.newaxis, :],\n                                t.shape)\n        _lats = np.broadcast_to(lats[np.newaxis, :, np.newaxis],\n                                t.shape)\n\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        _lats = np.transpose(_lats)\n        _lons = np.transpose(_lons)\n        t = np.transpose(t)\n        q = np.transpose(q)\n        p = np.transpose(p)\n        hgt = np.transpose(hgt)\n\n        # data cube format should be lats,lons,heights\n        p = p.swapaxes(0, 1)\n        q = q.swapaxes(0, 1)\n        t = t.swapaxes(0, 1)\n        hgt = hgt.swapaxes(0, 1)\n        _lats = _lats.swapaxes(0, 1)\n        _lons = _lons.swapaxes(0, 1)\n\n        # assign the regular-grid variables\n        self._p = p\n        self._q = q\n        self._t = t\n        self._lats = _lats\n        self._lons = _lons\n        self._xs = _lons\n        self._ys = _lats\n        self._zs = hgt\n</code></pre>"},{"location":"reference/#RAiDER.models.ncmr.NCMR.load_weather","title":"<code>load_weather(f=None, *args, **kwargs)</code>","text":"<p>Load NCMR model variables from existing file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/ncmr.py</code> <pre><code>def load_weather(self, f=None, *args, **kwargs):\n'''\n    Load NCMR model variables from existing file\n    '''\n    f = self.files[0] if f is None else f\n\n    # bounding box plus a buffer\n    lat_min, lat_max, lon_min, lon_max = self._ll_bounds\n    self._bounds = (lat_min, lat_max, lon_min, lon_max)\n\n    self._makeDataCubes(f)\n</code></pre>"},{"location":"reference/#RAiDER.models.plotWeather","title":"<code>plotWeather</code>","text":"<p>This set of functions is designed to for plotting WeatherModel class objects. It is not designed to be used on its own apart from this class.</p>"},{"location":"reference/#RAiDER.models.plotWeather.plot_pqt","title":"<code>plot_pqt(weatherObj, savefig=True, z1=500, z2=15000)</code>","text":"<p>Create a plot with pressure, temp, and humidity at two heights</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/plotWeather.py</code> <pre><code>def plot_pqt(weatherObj, savefig=True, z1=500, z2=15000):\n'''\n    Create a plot with pressure, temp, and humidity at two heights\n    '''\n\n    # Get the interpolator\n\n    intFcn_p = Interpolator((weatherObj._xs, weatherObj._ys, weatherObj._zs), weatherObj._p.swapaxes(0, 1))\n    intFcn_e = Interpolator((weatherObj._xs, weatherObj._ys, weatherObj._zs), weatherObj._e.swapaxes(0, 1))\n    intFcn_t = Interpolator((weatherObj._xs, weatherObj._ys, weatherObj._zs), weatherObj._t.swapaxes(0, 1))\n\n    # get the points needed\n    XY = np.meshgrid(weatherObj._xs, weatherObj._ys)\n    x = XY[0]\n    y = XY[1]\n    z1a = np.zeros(x.shape) + z1\n    z2a = np.zeros(x.shape) + z2\n    pts1 = np.stack((x.flatten(), y.flatten(), z1a.flatten()), axis=1)\n    pts2 = np.stack((x.flatten(), y.flatten(), z2a.flatten()), axis=1)\n\n    p1 = intFcn_p(pts1)\n    e1 = intFcn_e(pts1)\n    t1 = intFcn_t(pts1)\n    p2 = intFcn_p(pts2)\n    e2 = intFcn_e(pts2)\n    t2 = intFcn_t(pts2)\n\n    # Now get the data to plot\n    plots = [p1 / 1e2, e1 / 1e2, t1 - 273.15, p2 / 1e2, e2 / 1e2, t2 - 273.15]\n    # titles = ('P (hPa)', 'E (hPa)'.format(z1), 'T (C)', '', '', '')\n    titles = ('P (hPa)', 'E (hPa)', 'T (C)', '', '', '')\n\n    # setup the plot\n    f = plt.figure(figsize=(18, 14))\n    f.suptitle(f'{weatherObj._Name} Pressure/Humidity/Temperature at height {z1}m and {z2}m (values should drop as elevation increases)')\n\n    xind = int(np.floor(weatherObj._xs.shape[0] / 2))\n    yind = int(np.floor(weatherObj._ys.shape[0] / 2))\n\n    # loop over each plot\n    for ind, plot, title in zip(range(len(plots)), plots, titles):\n        sp = f.add_subplot(3, 3, ind + 1)\n        im = sp.imshow(np.reshape(plot, x.shape),\n                       cmap='viridis',\n                       extent=[np.nanmin(x), np.nanmax(x), np.nanmin(y), np.nanmax(y)],\n                       origin='lower')\n        sp.plot(x[yind, xind], y[yind, xind], 'ko')\n        divider = mal(sp)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n        plt.colorbar(im, cax=cax)\n        sp.set_title(title)\n        if ind == 0:\n            sp.set_ylabel('{} m\\n'.format(z1))\n        if ind == 3:\n            sp.set_ylabel('{} m\\n'.format(z2))\n\n    # add plots that show each variable with height\n    zdata = weatherObj._zs[:] / 1000\n    sp = f.add_subplot(3, 3, 7)\n    sp.plot(weatherObj._p[yind, xind, :] / 1e2, zdata)\n    sp.set_ylabel('Height (km)')\n    sp.set_xlabel('Pressure (hPa)')\n\n    sp = f.add_subplot(3, 3, 8)\n    sp.plot(weatherObj._e[yind, xind, :] / 100, zdata)\n    sp.set_xlabel('E (hPa)')\n\n    sp = f.add_subplot(3, 3, 9)\n    sp.plot(weatherObj._t[yind, xind, :] - 273.15, zdata)\n    sp.set_xlabel('Temp (C)')\n\n    plt.subplots_adjust(top=0.95, bottom=0.1, left=0.1, right=0.95, hspace=0.2,\n                        wspace=0.3)\n\n    if savefig:\n        wd   = os.path.dirname(os.path.dirname(weatherObj._out_name))\n        f    = f'{weatherObj._Name}_weather_hgt{z1}_and_{z2}m.pdf'\n        plt.savefig(os.path.join(wd, f))\n    return f\n</code></pre>"},{"location":"reference/#RAiDER.models.plotWeather.plot_wh","title":"<code>plot_wh(weatherObj, savefig=True, z1=500, z2=15000)</code>","text":"<p>Create a plot with wet refractivity and hydrostatic refractivity, at two different heights</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/plotWeather.py</code> <pre><code>def plot_wh(weatherObj, savefig=True, z1=500, z2=15000):\n'''\n    Create a plot with wet refractivity and hydrostatic refractivity,\n    at two different heights\n    '''\n\n    # Get the interpolator\n    intFcn_w = Interpolator((weatherObj._xs, weatherObj._ys, weatherObj._zs), weatherObj._wet_refractivity.swapaxes(0, 1))\n    intFcn_h = Interpolator((weatherObj._xs, weatherObj._ys, weatherObj._zs), weatherObj._hydrostatic_refractivity.swapaxes(0, 1))\n\n    # get the points needed\n    XY = np.meshgrid(weatherObj._xs, weatherObj._ys)\n    x = XY[0]\n    y = XY[1]\n    z1a = np.zeros(x.shape) + z1\n    z2a = np.zeros(x.shape) + z2\n    pts1 = np.stack((x.flatten(), y.flatten(), z1a.flatten()), axis=1)\n    pts2 = np.stack((x.flatten(), y.flatten(), z2a.flatten()), axis=1)\n\n    w1 = intFcn_w(pts1)\n    h1 = intFcn_h(pts1)\n    w2 = intFcn_w(pts2)\n    h2 = intFcn_h(pts2)\n\n    # Now get the data to plot\n    plots = [w1, h1, w2, h2]\n\n    # titles\n    titles = ('Wet refractivity {}'.format(z1),\n              'Hydrostatic refractivity {}'.format(z1),\n              '{}'.format(z2),\n              '{}'.format(z2))\n\n    # setup the plot\n    f = plt.figure(figsize=(14, 10))\n    f.suptitle(f'{weatherObj._Name} Wet and Hydrostatic refractivity at height {z1}m and {z2}m')\n\n    # loop over each plot\n    for ind, plot, title in zip(range(len(plots)), plots, titles):\n        sp = f.add_subplot(2, 2, ind + 1)\n        im = sp.imshow(np.reshape(plot, x.shape), cmap='viridis',\n                       extent=[np.nanmin(x), np.nanmax(x), np.nanmin(y), np.nanmax(y)], origin='lower')\n        divider = mal(sp)\n        cax = divider.append_axes(\"right\", size=\"4%\", pad=0.05)\n        plt.colorbar(im, cax=cax)\n        sp.set_title(title)\n        if ind == 0:\n            sp.set_ylabel('{} m\\n'.format(z1))\n        if ind == 2:\n            sp.set_ylabel('{} m\\n'.format(z2))\n\n    if savefig:\n        wd   = os.path.dirname(os.path.dirname(weatherObj._out_name))\n        f    = f'{weatherObj._Name}_refractivity_hgt{z1}_and_{z2}m.pdf'\n        plt.savefig(os.path.join(wd, f))\n    return f\n</code></pre>"},{"location":"reference/#RAiDER.models.template","title":"<code>template</code>","text":""},{"location":"reference/#RAiDER.models.template.customModelReader","title":"<code>customModelReader</code>","text":"<p>             Bases: <code>WeatherModel</code></p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/template.py</code> <pre><code>class customModelReader(WeatherModel):\n    def __init__(self):\n        WeatherModel.__init__(self)\n        self._humidityType = 'q'  # can be \"q\" (specific humidity) or \"rh\" (relative humidity)\n        self._model_level_type = 'pl'  # Default, pressure levels are \"pl\", and model levels are \"ml\"\n        self._classname = 'abcd'  # name of the custom weather model\n        self._dataset = 'abcd'  # same name as above\n\n        # Tuple of min/max years where data is available.\n        #  valid range of the dataset. Users need to specify the start date and end date (can be \"present\")\n        self._valid_range = (datetime.datetime(2016, 7, 15), \"Present\")\n        #  Availability lag time. Can be specified in hours \"hours=3\" or in days \"days=3\"\n        self._lag_time = datetime.timedelta(hours=3)\n        # Availabile time resolution; i.e. minimum rate model is available in hours. 1 is hourly\n        self._time_res = 1\n\n        # model constants (these three constants are borrowed from ECMWF model and currently\n        # set to be default for all other models, which may need to be double checked.)\n        self._k1 = 0.776  # [K/Pa]\n        self._k2 = 0.233  # [K/Pa]\n        self._k3 = 3.75e3  # [K^2/Pa]\n\n        # horizontal grid spacing\n        self._lat_res = 3. / 111  # grid spacing in latitude\n        self._lon_res = 3. / 111  # grid spacing in longitude\n        self._x_res = 3.  # x-direction grid spacing in the native weather model projection\n        #  (if the projection is in lat/lon, it is the same as \"self._lon_res\")\n        self._y_res = 3.  # y-direction grid spacing in the weather model native projection\n        #  (if the projection is in lat/lon, it is the same as \"self._lat_res\")\n\n        # zlevels specify fixed heights at which to interpolate the weather model variables\n        self._zlevels = np.flipud(LEVELS_137_HEIGHTS)\n\n        self._Name = 'ABCD'  # name of the custom weather model (better to be capitalized)\n\n        # Projections in RAiDER are defined using pyproj (python wrapper around Proj)\n        # If the projection is defined with EPSG code, one can use \"self._proj = CRS.from_epsg(4326)\"\n        # to replace the following lines to get \"self._proj\".\n        # Below we show the example of HRRR model with the parameters of its Lambert Conformal Conic projection\n        lon0 = 262.5\n        lat0 = 38.5\n        lat1 = 38.5\n        lat2 = 38.5\n        x0 = 0\n        y0 = 0\n        earth_radius = 6371229\n        p1 = CRS('+proj=lcc +lat_1={lat1} +lat_2={lat2} +lat_0={lat0} +lon_0={lon0} +x_0={x0} +y_0={y0} +a={a} +b={a} +units=m +no_defs'.format(lat1=lat1, lat2=lat2, lat0=lat0, lon0=lon0, x0=x0, y0=y0, a=earth_radius))\n        self._proj = p1\n\n    def _fetch(self, out):\n'''\n        Fetch weather model data from the custom weather model \"ABCD\"\n        Inputs (no need to change in the custom weather model reader):\n        lats - latitude\n        lons - longitude\n        time - datatime object (year,month,day,hour,minute,second)\n        out - name of downloaded dataset file from the custom weather model server\n        Nextra - buffer of latitude/longitude for determining the bounding box\n        '''\n        # Auxilliary function:\n        # download dataset of the custom weather model \"ABCD\" from a server and then save it to a file named out.\n        # This function needs to be writen by the users. For download from the weather model server, the weather model\n        # name, time and bounding box may be needed to retrieve the dataset; for cases where no file is actually\n        # downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP, this function can be omitted leaving the data\n        # retrieval to the following \"load_weather\" function.\n        self._files = self._download_abcd_file(out, 'abcd', self._time, self._ll_bounds)\n\n    def load_weather(self, filename):\n'''\n        Load weather model variables from the downloaded file named filename\n        Inputs:\n        filename - filename of the downloaded weather model file\n        '''\n\n        # Auxilliary function:\n        # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file\n        # This function needs to be writen by the users. For downloaded file from the weather model server,\n        # this function extracts the individual variables from the saved file named filename;\n        # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP,\n        # this function retrieves the individual variables directly from the weblink of the weather model.\n        lats, lons, xs, ys, t, q, p, hgt = self._makeDataCubes(filename)\n\n        # extra steps that may be needed to calculate topographic height and pressure level if not provided\n        # directly by the weather model through the above auxilliary function \"self._makeDataCubes\"\n\n        # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be\n        # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to\n        # geopotential, pressure level and geopotential height; otherwise commented out\n        z, p, hgt = self._calculategeoh(z, p)  # TODO: z is undefined\n\n        # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"),\n        # one can use the following line to convert to geopotential height; otherwise, commented out\n        hgt = z / self._g0\n\n        # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to\n        # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out\n        self._get_heights(lats, hgt)\n\n        # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out\n        self._zs = hgt\n\n        ###########\n\n        ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ########\n\n        # _t: temperture\n        # _q: either relative or specific humidity\n        # _p: must be pressure level\n        # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons)\n        # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats)\n        # _zs: must be topographic height\n        # _lats: latitude\n        # _lons: longitude\n        self._t = t\n        self._q = q\n        self._p = p\n        self._xs = xs\n        self._ys = ys\n        self._lats = lats\n        self._lons = lons\n\n        ###########\n\n    def _download_abcd_file(self, out, model_name, date_time, bounding_box):\n'''\n        Auxilliary function:\n        Download weather model data from a server\n        Inputs:\n        out - filename for saving the retrieved data file\n        model_name - name of the custom weather model\n        date_time - datatime object (year,month,day,hour,minute,second)\n        bounding_box - lat/lon bounding box for the region of interest\n        Output:\n        out - returned filename from input\n        '''\n        pass\n\n    def _makeDataCubes(self, filename):\n'''\n        Auxilliary function:\n        Read 3-D data cubes from downloaded file or directly from weather model weblink (in which case, there is no\n        need to download and save any file; rather, the weblink needs to be hardcoded in the custom reader, e.g. GMAO)\n        Input:\n        filename - filename of the downloaded weather model file from the server\n        Outputs:\n        lats - latitude (3-D data cube)\n        lons - longitude (3-D data cube)\n        xs - x-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _xs = _lons)\n        ys - y-direction grid dimension of the native weather model coordinates (3-D data cube; if in lat/lon, _ys = _lats)\n        t - temperature (3-D data cube)\n        q - humidity (3-D data cube; could be relative humidity or specific humidity)\n        p - pressure level (3-D data cube; could be pressure level (preferred) or surface pressure)\n        hgt - height (3-D data cube; could be geopotential height or topographic height (preferred))\n        '''\n        pass\n</code></pre>"},{"location":"reference/#RAiDER.models.template.customModelReader.load_weather","title":"<code>load_weather(filename)</code>","text":"<p>Load weather model variables from the downloaded file named filename Inputs: filename - filename of the downloaded weather model file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/template.py</code> <pre><code>def load_weather(self, filename):\n'''\n    Load weather model variables from the downloaded file named filename\n    Inputs:\n    filename - filename of the downloaded weather model file\n    '''\n\n    # Auxilliary function:\n    # read individual variables (in 3-D cube format with exactly the same dimension) from downloaded file\n    # This function needs to be writen by the users. For downloaded file from the weather model server,\n    # this function extracts the individual variables from the saved file named filename;\n    # for cases where no file is actually downloaded, e.g. the GMAO and MERRA-2 models using OpenDAP,\n    # this function retrieves the individual variables directly from the weblink of the weather model.\n    lats, lons, xs, ys, t, q, p, hgt = self._makeDataCubes(filename)\n\n    # extra steps that may be needed to calculate topographic height and pressure level if not provided\n    # directly by the weather model through the above auxilliary function \"self._makeDataCubes\"\n\n    # if surface pressure (in logarithm) is provided as \"p\" along with the surface geopotential \"z\" (needs to be\n    # added to the auxilliary function \"self._makeDataCubes\"), one can use the following line to convert to\n    # geopotential, pressure level and geopotential height; otherwise commented out\n    z, p, hgt = self._calculategeoh(z, p)  # TODO: z is undefined\n\n    # if the geopotential is provided as \"z\" (needs to be added to the auxilliary function \"self._makeDataCubes\"),\n    # one can use the following line to convert to geopotential height; otherwise, commented out\n    hgt = z / self._g0\n\n    # if geopotential height is provided/calculated as \"hgt\", one can use the following line to convert to\n    # topographic height, which is then automatically assigned to \"self._zs\"; otherwise commented out\n    self._get_heights(lats, hgt)\n\n    # if topographic height is provided as \"hgt\", use the following line directly; otherwise commented out\n    self._zs = hgt\n\n    ###########\n\n    ######### output of the weather model reader for delay calculations (all in 3-D data cubes) ########\n\n    # _t: temperture\n    # _q: either relative or specific humidity\n    # _p: must be pressure level\n    # _xs: x-direction grid dimension of the native weather model coordinates (if in lat/lon, _xs = _lons)\n    # _ys: y-direction grid dimension of the native weather model coordinates (if in lat/lon, _ys = _lats)\n    # _zs: must be topographic height\n    # _lats: latitude\n    # _lons: longitude\n    self._t = t\n    self._q = q\n    self._p = p\n    self._xs = xs\n    self._ys = ys\n    self._lats = lats\n    self._lons = lons\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel","title":"<code>weatherModel</code>","text":""},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel","title":"<code>WeatherModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Implement a generic weather model for getting estimated SAR delays</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>class WeatherModel(ABC):\n'''\n    Implement a generic weather model for getting estimated SAR delays\n    '''\n\n    def __init__(self):\n        # Initialize model-specific constants/parameters\n        self._k1 = None\n        self._k2 = None\n        self._k3 = None\n        self._humidityType = 'q'\n        self._a = []\n        self._b = []\n\n        self.files = None\n\n        self._time_res = None # time resolution of the weather model in hours\n\n        self._lon_res = None\n        self._lat_res = None\n        self._x_res = None\n        self._y_res = None\n\n        self._classname = None\n        self._dataset = None\n        self._name    = None\n        self._wmLoc   = None\n\n        self._model_level_type = 'ml'\n\n        self._valid_range = (\n            datetime.date(1900, 1, 1),\n        )  # Tuple of min/max years where data is available.\n        self._lag_time = datetime.timedelta(days=30)  # Availability lag time in days\n        self._time = None\n        self._bbox = None\n\n        # Define fixed constants\n        self._R_v = 461.524\n        self._R_d = 287.06  # in our original code this was 287.053\n        self._g0 = _g0  # gravity constant\n        self._zmin = _ZMIN  # minimum integration height\n        self._zmax = _ZREF  # max integration height\n        self._proj = None\n\n        # setup data structures\n        self._levels = []\n        self._xs = np.empty((1, 1, 1))  # Use generic x/y/z instead of lon/lat/height\n        self._ys = np.empty((1, 1, 1))\n        self._zs = np.empty((1, 1, 1))\n\n        self._lats = None\n        self._lons = None\n        self._ll_bounds = None\n        self._valid_bounds =  box(-180, -90, 180, 90) # Shapely box with WSEN bounds\n\n        self._p = None\n        self._q = None\n        self._rh = None\n        self._t = None\n        self._e = None\n        self._wet_refractivity = None\n        self._hydrostatic_refractivity = None\n        self._wet_ztd = None\n        self._hydrostatic_ztd = None\n\n\n    def __str__(self):\n        string = '\\n'\n        string += '======Weather Model class object=====\\n'\n        string += 'Weather model time: {}\\n'.format(self._time)\n        string += 'Latitude resolution: {}\\n'.format(self._lat_res)\n        string += 'Longitude resolution: {}\\n'.format(self._lon_res)\n        string += 'Native projection: {}\\n'.format(self._proj)\n        string += 'ZMIN: {}\\n'.format(self._zmin)\n        string += 'ZMAX: {}\\n'.format(self._zmax)\n        string += 'k1 = {}\\n'.format(self._k1)\n        string += 'k2 = {}\\n'.format(self._k2)\n        string += 'k3 = {}\\n'.format(self._k3)\n        string += 'Humidity type = {}\\n'.format(self._humidityType)\n        string += '=====================================\\n'\n        string += 'Class name: {}\\n'.format(self._classname)\n        string += 'Dataset: {}\\n'.format(self._dataset)\n        string += '=====================================\\n'\n        string += 'A: {}\\n'.format(self._a)\n        string += 'B: {}\\n'.format(self._b)\n        if self._p is not None:\n            string += 'Number of points in Lon/Lat = {}/{}\\n'.format(*self._p.shape[:2])\n            string += 'Total number of grid points (3D): {}\\n'.format(np.prod(self._p.shape))\n        if self._xs.size == 0:\n            string += 'Minimum/Maximum y: {: 4.2f}/{: 4.2f}\\n'\\\n                      .format(robmin(self._ys), robmax(self._ys))\n            string += 'Minimum/Maximum x: {: 4.2f}/{: 4.2f}\\n'\\\n                      .format(robmin(self._xs), robmax(self._xs))\n            string += 'Minimum/Maximum zs/heights: {: 10.2f}/{: 10.2f}\\n'\\\n                      .format(robmin(self._zs), robmax(self._zs))\n        string += '=====================================\\n'\n        return str(string)\n\n\n    def Model(self):\n        return self._Name\n\n\n    def dtime(self):\n        return self._time_res\n\n\n    def getLLRes(self):\n        return np.max([self._lat_res, self._lon_res])\n\n\n    def fetch(self, out, time):\n'''\n        Checks the input datetime against the valid date range for the model and then\n        calls the model _fetch routine\n\n        Args:\n        ----------\n        out -\n        ll_bounds - 4 x 1 array, SNWE\n        time = UTC datetime\n        '''\n        self.checkTime(time)\n\n        # write the error raised by the weather model API to the log\n        try:\n            self._fetch(out)\n            err = False\n\n        except Exception as E:\n            err = E\n\n        return err\n\n\n    @abstractmethod\n    def _fetch(self, out):\n'''\n        Placeholder method. Should be implemented in each weather model type class\n        '''\n        pass\n\n\n    def getTime(self):\n        return self._time\n\n\n    def setTime(self, time, fmt='%Y-%m-%dT%H:%M:%S'):\n''' Set the time for a weather model '''\n        if isinstance(time, str):\n            self._time = datetime.datetime.strptime(time, fmt)\n        elif isinstance(time, datetime.datetime):\n            self._time = time\n        else:\n            raise ValueError('\"time\" must be a string or a datetime object')\n\n\n    def get_latlon_bounds(self):\n        return self._ll_bounds\n\n\n    def set_latlon_bounds(self, ll_bounds, Nextra=2, output_spacing=None):\n'''\n        Need to correct lat/lon bounds because not all of the weather models have valid\n        data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2,\n        need to adjust the longitude higher end with an extra buffer; for other models,\n        the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be\n        rounded to the above regions (either in the downloading-file API or subsetting-\n        data API) without problems.\n        '''\n        ex_buffer_lon_max = 0.0\n\n        if self._Name in 'HRRR HRRR-AK HRES'.split():\n            Nextra = 6 # have a bigger buffer\n\n        else:\n            ex_buffer_lon_max = self._lon_res\n\n        # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary\n        S, N, W, E = ll_bounds\n\n        # Adjust bounds if they get near the poles or IDL\n        pixlat, pixlon = Nextra*self._lat_res, Nextra*self._lon_res\n\n        S= np.max([S - pixlat, -90.0 + pixlat])\n        N= np.min([N + pixlat, 90.0 - pixlat])\n        W= np.max([W - (pixlon + ex_buffer_lon_max), -180.0 + (pixlon+ex_buffer_lon_max)])\n        E= np.min([E + (pixlon + ex_buffer_lon_max), 180.0 - pixlon - ex_buffer_lon_max])\n        if output_spacing is not None:\n            S, N, W, E  = clip_bbox([S,N,W,E], output_spacing)\n\n        self._ll_bounds = np.array([S, N, W, E])\n\n\n    def get_wmLoc(self):\n\"\"\" Get the path to the direct with the weather model files \"\"\"\n        if self._wmLoc is None:\n            wmLoc = os.path.join(os.getcwd(), 'weather_files')\n        else:\n            wmLoc = self._wmLoc\n        return wmLoc\n\n\n    def set_wmLoc(self, weather_model_directory:str):\n\"\"\" Set the path to the directory with the weather model files \"\"\"\n        self._wmLoc = weather_model_directory\n\n\n    def load(\n        self,\n        *args,\n        _zlevels=None,\n        **kwargs\n    ):\n'''\n        Calls the load_weather method. Each model class should define a load_weather\n        method appropriate for that class. 'args' should be one or more filenames.\n        '''\n        # If the weather file has already been processed, do nothing\n        outLoc = self.get_wmLoc()\n        path_wm_raw    = make_raw_weather_data_filename(outLoc, self.Model(), self.getTime())\n        self._out_name = self.out_file(outLoc)\n\n        if os.path.exists(self._out_name):\n            return self._out_name\n        else:\n            # Load the weather just for the query points\n            self.load_weather(f=path_wm_raw, *args, **kwargs)\n\n            # Process the weather model data\n            self._find_e()\n            self._uniform_in_z(_zlevels=_zlevels)\n            self._checkForNans()\n            self._get_wet_refractivity()\n            self._get_hydro_refractivity()\n            self._adjust_grid(self.get_latlon_bounds())\n\n            # Compute Zenith delays at the weather model grid nodes\n            self._getZTD()\n            return None\n\n\n    @abstractmethod\n    def load_weather(self, *args, **kwargs):\n'''\n        Placeholder method. Should be implemented in each weather model type class\n        '''\n        pass\n\n\n    def plot(self, plotType='pqt', savefig=True):\n'''\n        Plotting method. Valid plot types are 'pqt'\n        '''\n        if plotType == 'pqt':\n            plot = plots.plot_pqt(self, savefig)\n        elif plotType == 'wh':\n            plot = plots.plot_wh(self, savefig)\n        else:\n            raise RuntimeError('WeatherModel.plot: No plotType named {}'.format(plotType))\n        return plot\n\n\n    def checkTime(self, time):\n'''\n        Checks the time against the lag time and valid date range for the given model type\n        '''\n        end_time = self._valid_range[1]\n        end_time = end_time if isinstance(end_time, str) else end_time.date()\n\n        logger.info(\n            'Weather model %s is available from %s to %s',\n            self.Model(), self._valid_range[0].date(), end_time\n        )\n\n        msg = f\"Weather model {self.Model()} is not available at: {time}\"\n\n        if time &lt; self._valid_range[0]:\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n        if self._valid_range[1] is not None:\n            if self._valid_range[1] == 'Present':\n                pass\n            elif self._valid_range[1] &lt; time:\n                logger.error(msg)\n                raise RuntimeError(msg)\n\n        if time &gt; datetime.datetime.utcnow() - self._lag_time:\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n\n    def setLevelType(self, levelType):\n'''Set the level type to model levels or pressure levels'''\n        if levelType in 'ml pl nat prs'.split():\n            self._model_level_type = levelType\n        else:\n            raise RuntimeError(f'Level type {levelType} is not recognized')\n\n        if levelType in 'ml nat'.split():\n            self.__model_levels__()\n        else:\n            self.__pressure_levels__()\n\n\n    def _convertmb2Pa(self, pres):\n'''\n        Convert pressure in millibars to Pascals\n        '''\n        return 100 * pres\n\n\n    def _get_heights(self, lats, geo_hgt, geo_ht_fill=np.nan):\n'''\n        Transform geo heights to WGS84 ellipsoidal heights\n        '''\n        geo_ht_fix = np.where(geo_hgt != geo_ht_fill, geo_hgt, np.nan)\n        lats_full  = np.broadcast_to(lats[...,np.newaxis], geo_ht_fix.shape)\n        self._zs   = util.geo_to_ht(lats_full, geo_ht_fix)\n\n\n    def _find_e(self):\n\"\"\"Check the type of e-calculation needed\"\"\"\n        if self._humidityType == 'rh':\n            self._find_e_from_rh()\n        elif self._humidityType == 'q':\n            self._find_e_from_q()\n        else:\n            raise RuntimeError('Not a valid humidity type')\n        self._rh = None\n        self._q = None\n\n\n    def _find_e_from_q(self):\n\"\"\"Calculate e, partial pressure of water vapor.\"\"\"\n        svp = find_svp(self._t)\n        # We have q = w/(w + 1), so w = q/(1 - q)\n        w = self._q / (1 - self._q)\n        self._e = w * self._R_v * (self._p - svp) / self._R_d\n\n\n    def _find_e_from_rh(self):\n\"\"\"Calculate partial pressure of water vapor.\"\"\"\n        svp = find_svp(self._t)\n        self._e = self._rh / 100 * svp\n\n\n    def _get_wet_refractivity(self):\n'''\n        Calculate the wet delay from pressure, temperature, and e\n        '''\n        self._wet_refractivity = self._k2 * self._e / self._t + self._k3 * self._e / self._t**2\n\n\n    def _get_hydro_refractivity(self):\n'''\n        Calculate the hydrostatic delay from pressure and temperature\n        '''\n        self._hydrostatic_refractivity = self._k1 * self._p / self._t\n\n\n    def getWetRefractivity(self):\n        return self._wet_refractivity\n\n\n    def getHydroRefractivity(self):\n        return self._hydrostatic_refractivity\n\n\n    def _adjust_grid(self, ll_bounds=None):\n'''\n        This function pads the weather grid with a level at self._zmin, if\n        it does not already go that low.\n        &lt;&lt;The functionality below has been removed.&gt;&gt;\n        &lt;&lt;It also removes levels that are above self._zmax, since they are not needed.&gt;&gt;\n        '''\n\n        if self._zmin &lt; np.nanmin(self._zs):\n            # first add in a new layer at zmin\n            self._zs = np.insert(self._zs, 0, self._zmin)\n\n            self._p = util.padLower(self._p)\n            self._t = util.padLower(self._t)\n            self._e = util.padLower(self._e)\n            self._wet_refractivity = util.padLower(self._wet_refractivity)\n            self._hydrostatic_refractivity = util.padLower(self._hydrostatic_refractivity)\n            if ll_bounds is not None:\n                self._trimExtent(ll_bounds)\n\n\n    def _getZTD(self):\n'''\n        Compute the full slant tropospheric delay for each weather model grid node, using the reference\n        height zref\n        '''\n        wet = self.getWetRefractivity()\n        hydro = self.getHydroRefractivity()\n\n        # Get the integrated ZTD\n        wet_total, hydro_total = np.zeros(wet.shape), np.zeros(hydro.shape)\n        for level in range(wet.shape[2]):\n            wet_total[..., level] = 1e-6 * np.trapz(\n                wet[..., level:], x=self._zs[level:], axis=2\n            )\n            hydro_total[..., level] = 1e-6 * np.trapz(\n                hydro[..., level:], x=self._zs[level:], axis=2\n            )\n        self._hydrostatic_ztd = hydro_total\n        self._wet_ztd = wet_total\n\n\n    def _getExtent(self, lats, lons):\n'''\n        get the bounding box around a set of lats/lons\n        '''\n        if (lats.size == 1) &amp; (lons.size == 1):\n            return [lats - self._lat_res, lats + self._lat_res, lons - self._lon_res, lons + self._lon_res]\n        elif (lats.size &gt; 1) &amp; (lons.size &gt; 1):\n            return [np.nanmin(lats), np.nanmax(lats), np.nanmin(lons), np.nanmax(lons)]\n        elif lats.size == 1:\n            return [lats - self._lat_res, lats + self._lat_res, np.nanmin(lons), np.nanmax(lons)]\n        elif lons.size == 1:\n            return [np.nanmin(lats), np.nanmax(lats), lons - self._lon_res, lons + self._lon_res]\n        else:\n            raise RuntimeError('Not a valid lat/lon shape')\n\n\n    @property\n    def bbox(self) -&gt; list:\n\"\"\"\n        Obtains the bounding box of the weather model in lat/lon CRS.\n\n        Returns:\n        -------\n        list\n            xmin, ymin, xmax, ymax\n\n        Raises\n        ------\n        ValueError\n           When `self.files` is None.\n        \"\"\"\n\n        if self._bbox is None:\n            path_weather_model = self.out_file(self.get_wmLoc())\n            if not os.path.exists(path_weather_model):\n                raise ValueError('Need to save cropped weather model as netcdf')\n\n            with xarray.load_dataset(path_weather_model) as ds:\n                try:\n                    xmin, xmax = ds.x.min(), ds.x.max()\n                    ymin, ymax = ds.y.min(), ds.y.max()\n                except:\n                    xmin, xmax = ds.longitude.min(), ds.longitude.max()\n                    ymin, ymax = ds.latitude.min(), ds.latitude.max()\n\n            wm_proj    = self._proj\n            xs, ys     = [xmin, xmin, xmax, xmax], [ymin, ymax, ymin, ymax]\n            lons, lats = transform_coords(wm_proj, CRS(4326), xs, ys)\n            ## projected weather models may not be aligned N/S\n            ## should only matter for warning messages\n            W, E = np.min(lons), np.max(lons)\n            # S, N = np.sort([lats[np.argmin(lons)], lats[np.argmax(lons)]])\n            S, N = np.min(lats), np.max(lats)\n            self._bbox = W, S, E, N\n\n        return self._bbox\n\n\n    def checkValidBounds(\n            self: weatherModel,\n            ll_bounds: np.ndarray,\n                         ):\n'''\n        Checks whether the given bounding box is valid for the model\n        (i.e., intersects with the model domain at all)\n\n        Args:\n        ll_bounds : np.ndarray\n\n        Returns:\n            The weather model object\n        '''\n        S, N, W, E = ll_bounds\n        if box(W, S, E, N).intersects(self._valid_bounds):\n            Mod = self\n\n        else:\n            raise ValueError(f'The requested location is unavailable for {self._Name}')\n\n        return Mod\n\n\n    def checkContainment(self: weatherModel,\n                         ll_bounds,\n                         buffer_deg: float = 1e-5) -&gt; bool:\n\"\"\"\"\n        Checks containment of weather model bbox of outLats and outLons\n        provided.\n\n        Args:\n        ----------\n        weather_model : weatherModel\n        ll_bounds: an array of floats (SNWE) demarcating bbox of targets\n        buffer_deg : float\n            For x-translates for extents that lie outside of world bounding box,\n            this ensures that translates have some overlap. The default is 1e-5\n            or ~11.1 meters.\n\n        Returns:\n        -------\n        bool\n           True if weather model contains bounding box of OutLats and outLons\n           and False otherwise.\n        \"\"\"\n        ymin_input, ymax_input, xmin_input, xmax_input = ll_bounds\n        input_box   = box(xmin_input, ymin_input, xmax_input, ymax_input)\n        xmin, ymin, xmax, ymax = self.bbox\n        weather_model_box = box(xmin, ymin, xmax, ymax)\n        world_box  = box(-180, -90, 180, 90)\n\n        # Logger\n        input_box_str = [f'{x:1.2f}' for x in [xmin_input, ymin_input,\n                                               xmax_input, ymax_input]]\n        weath_box_str = [f'{x:1.2f}' for x in [xmin, ymin, xmax, ymax]]\n\n        weath_box_str = ', '.join(weath_box_str)\n        input_box_str = ', '.join(input_box_str)\n\n        logger.info(f'Extent of the weather model is (xmin, ymin, xmax, ymax):'\n                    f'{weath_box_str}')\n        logger.info(f'Extent of the input is (xmin, ymin, xmax, ymax): '\n                    f'{input_box_str}')\n\n        # If the bounding box goes beyond the normal world extents\n        # Look at two x-translates, buffer them, and take their union.\n        if not world_box.contains(weather_model_box):\n            logger.info('Considering x-translates of weather model +/-360 '\n                        'as bounding box outside of -180, -90, 180, 90')\n            translates = [weather_model_box.buffer(buffer_deg),\n                          translate(weather_model_box,\n                                    xoff=360).buffer(buffer_deg),\n                          translate(weather_model_box,\n                                    xoff=-360).buffer(buffer_deg)\n                          ]\n            weather_model_box = unary_union(translates)\n\n        return weather_model_box.contains(input_box)\n\n\n    def _isOutside(self, extent1, extent2):\n'''\n        Determine whether any of extent1  lies outside extent2\n        extent1/2 should be a list containing [lower_lat, upper_lat, left_lon, right_lon]\n        '''\n        t1 = extent1[0] &lt; extent2[0]\n        t2 = extent1[1] &gt; extent2[1]\n        t3 = extent1[2] &lt; extent2[2]\n        t4 = extent1[3] &gt; extent2[3]\n        if np.any([t1, t2, t3, t4]):\n            return True\n        return False\n\n\n    def _trimExtent(self, extent):\n'''\n        get the bounding box around a set of lats/lons\n        '''\n        lat = self._lats.copy()\n        lon = self._lons.copy()\n        lat[np.isnan(lat)] = np.nanmean(lat)\n        lon[np.isnan(lon)] = np.nanmean(lon)\n        mask = (lat &gt;= extent[0]) &amp; (lat &lt;= extent[1]) &amp; \\\n               (lon &gt;= extent[2]) &amp; (lon &lt;= extent[3])\n        ma1 = np.sum(mask, axis=1).astype('bool')\n        ma2 = np.sum(mask, axis=0).astype('bool')\n        if np.sum(ma1) == 0 and np.sum(ma2) == 0:\n            # Don't need to remove any points\n            return\n\n        # indices of the part of the grid to keep\n        ny, nx, nz = self._p.shape\n        index1 = max(np.arange(len(ma1))[ma1][0] - 2, 0)\n        index2 = min(np.arange(len(ma1))[ma1][-1] + 2, ny)\n        index3 = max(np.arange(len(ma2))[ma2][0] - 2, 0)\n        index4 = min(np.arange(len(ma2))[ma2][-1] + 2, nx)\n\n        # subset around points of interest\n        self._lons = self._lons[index1:index2, index3:index4]\n        self._lats = self._lats[index1:index2, index3:index4]\n        self._xs = self._xs[index3:index4]\n        self._ys = self._ys[index1:index2]\n        self._p = self._p[index1:index2, index3:index4, ...]\n        self._t = self._t[index1:index2, index3:index4, ...]\n        self._e = self._e[index1:index2, index3:index4, ...]\n\n        self._wet_refractivity = self._wet_refractivity[index1:index2, index3:index4, ...]\n        self._hydrostatic_refractivity = self._hydrostatic_refractivity[index1:index2, index3:index4, :]\n\n\n    def _calculategeoh(self, z, lnsp):\n'''\n        Function to calculate pressure, geopotential, and geopotential height\n        from the surface pressure and model levels provided by a weather model.\n        The model levels are numbered from the highest eleveation to the lowest.\n        Inputs:\n            self - weather model object with parameters a, b defined\n            z    - 3-D array of surface heights for the location(s) of interest\n            lnsp - log of the surface pressure\n        Outputs:\n            geopotential - The geopotential in units of height times acceleration\n            pressurelvs  - The pressure at each of the model levels for each of\n                           the input points\n            geoheight    - The geopotential heights\n        '''\n        return calcgeoh(lnsp, self._t, self._q, z, self._a, self._b, self._R_d, self._levels)\n\n\n    def getProjection(self):\n'''\n        Returns: the native weather projection, which should be a pyproj object\n        '''\n        return self._proj\n\n\n    def getPoints(self):\n        return self._xs.copy(), self._ys.copy(), self._zs.copy()\n\n\n    def _uniform_in_z(self, _zlevels=None):\n'''\n        Interpolate all variables to a regular grid in z\n        '''\n        nx, ny = self._p.shape[:2]\n\n        # new regular z-spacing\n        if _zlevels is None:\n            try:\n                _zlevels = self._zlevels\n            except BaseException:\n                _zlevels = np.nanmean(self._zs, axis=(0, 1))\n\n        new_zs = np.tile(_zlevels, (nx, ny, 1))\n\n        # re-assign values to the uniform z\n        self._t = interpolate_along_axis(\n            self._zs, self._t, new_zs, axis=2, fill_value=np.nan\n        ).astype(np.float32)\n        self._p = interpolate_along_axis(\n            self._zs, self._p, new_zs, axis=2, fill_value=np.nan\n        ).astype(np.float32)\n        self._e = interpolate_along_axis(\n            self._zs, self._e, new_zs, axis=2, fill_value=np.nan\n        ).astype(np.float32)\n\n        self._zs = _zlevels\n        self._xs = np.unique(self._xs)\n        self._ys = np.unique(self._ys)\n\n\n    def _checkForNans(self):\n'''\n        Fill in NaN-values\n        '''\n        self._p = fillna3D(self._p)\n        self._t = fillna3D(self._t, fill_value=1e16) # to avoid division by zero later on\n        self._e = fillna3D(self._e)\n\n\n    def out_file(self, outLoc):\n        f = make_weather_model_filename(\n            self._Name,\n            self._time,\n            self._ll_bounds,\n        )\n        return os.path.join(outLoc, f)\n\n\n    def filename(self, time=None, outLoc='weather_files'):\n'''\n        Create a filename to store the weather model\n        '''\n        os.makedirs(outLoc, exist_ok=True)\n\n        if time is None:\n            if self._time is None:\n                raise ValueError('Time must be specified before the file can be written')\n            else:\n                time = self._time\n\n        f = make_raw_weather_data_filename(\n            outLoc,\n            self._Name,\n            time,\n        )\n\n        self.files = [f]\n        return f\n\n\n    def write(self):\n'''\n        By calling the abstract/modular netcdf writer\n        (RAiDER.utilFcns.write2NETCDF4core), write the weather model data\n        and refractivity to an NETCDF4 file that can be accessed by external programs.\n        '''\n        # Generate the filename\n        f = self._out_name\n\n        attrs_dict = {\n                \"Conventions\": 'CF-1.6',\n                \"datetime\": datetime.datetime.strftime(self._time, \"%Y_%m_%dT%H_%M_%S\"),\n                'date_created': datetime.datetime.now().strftime(\"%Y_%m_%dT%H_%M_%S\"),\n                'title': 'Weather model data and delay calculations',\n\n            }\n\n        dimension_dict = {\n            'x': ('x', self._xs),\n            'y': ('y', self._ys),\n            'z': ('z', self._zs),\n            'latitude': (('y', 'x'), self._lats),\n            'longitude': (('y', 'x'), self._lons),\n            'datetime': self._time,\n        }\n\n        dataset_dict = {\n            't': (('z', 'y', 'x'), self._t.swapaxes(0, 2).swapaxes(1, 2)),\n            'p': (('z', 'y', 'x'), self._p.swapaxes(0, 2).swapaxes(1, 2)),\n            'e': (('z', 'y', 'x'), self._e.swapaxes(0, 2).swapaxes(1, 2)),\n            'wet': (('z', 'y', 'x'), self._wet_refractivity.swapaxes(0, 2).swapaxes(1, 2)),\n            'hydro': (('z', 'y', 'x'), self._hydrostatic_refractivity.swapaxes(0, 2).swapaxes(1, 2)),\n            'wet_total': (('z', 'y', 'x'), self._wet_ztd.swapaxes(0, 2).swapaxes(1, 2)),\n            'hydro_total': (('z', 'y', 'x'), self._hydrostatic_ztd.swapaxes(0, 2).swapaxes(1, 2)),\n        }\n\n        ds = xarray.Dataset(data_vars=dataset_dict, coords=dimension_dict, attrs=attrs_dict)\n\n        # Define units\n        ds['t'].attrs['units'] = 'K'\n        ds['e'].attrs['units'] = 'Pa'\n        ds['p'].attrs['units'] = 'Pa'\n        ds['wet'].attrs['units'] = 'dimentionless'\n        ds['hydro'].attrs['units'] = 'dimentionless'\n        ds['wet_total'].attrs['units'] = 'm'\n        ds['hydro_total'].attrs['units'] = 'm'\n\n        # Define standard names\n        ds['t'].attrs['standard_name'] = 'temperature'\n        ds['e'].attrs['standard_name'] = 'humidity'\n        ds['p'].attrs['standard_name'] = 'pressure'\n        ds['wet'].attrs['standard_name'] = 'wet_refractivity'\n        ds['hydro'].attrs['standard_name'] = 'hydrostatic_refractivity'\n        ds['wet_total'].attrs['standard_name'] = 'total_wet_refractivity'\n        ds['hydro_total'].attrs['standard_name'] = 'total_hydrostatic_refractivity'\n\n        # projection information\n        ds[\"proj\"] = int()\n        for k, v in self._proj.to_cf().items():\n            ds.proj.attrs[k] = v\n        for var in ds.data_vars:\n            ds[var].attrs['grid_mapping'] = 'proj'\n\n        # write to file and return the filename\n        ds.to_netcdf(f)\n        return f\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.bbox","title":"<code>bbox: list</code>  <code>property</code>","text":"<p>Obtains the bounding box of the weather model in lat/lon CRS.</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.bbox--returns","title":"Returns:","text":"<p>list     xmin, ymin, xmax, ymax</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.bbox--raises","title":"Raises","text":"<p>ValueError    When <code>self.files</code> is None.</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkContainment","title":"<code>checkContainment(ll_bounds, buffer_deg=1e-05)</code>","text":"<p>\" Checks containment of weather model bbox of outLats and outLons provided.</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkContainment--args","title":"Args:","text":"<p>weather_model : weatherModel ll_bounds: an array of floats (SNWE) demarcating bbox of targets buffer_deg : float     For x-translates for extents that lie outside of world bounding box,     this ensures that translates have some overlap. The default is 1e-5     or ~11.1 meters.</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkContainment--returns","title":"Returns:","text":"<p>bool    True if weather model contains bounding box of OutLats and outLons    and False otherwise.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def checkContainment(self: weatherModel,\n                     ll_bounds,\n                     buffer_deg: float = 1e-5) -&gt; bool:\n\"\"\"\"\n    Checks containment of weather model bbox of outLats and outLons\n    provided.\n\n    Args:\n    ----------\n    weather_model : weatherModel\n    ll_bounds: an array of floats (SNWE) demarcating bbox of targets\n    buffer_deg : float\n        For x-translates for extents that lie outside of world bounding box,\n        this ensures that translates have some overlap. The default is 1e-5\n        or ~11.1 meters.\n\n    Returns:\n    -------\n    bool\n       True if weather model contains bounding box of OutLats and outLons\n       and False otherwise.\n    \"\"\"\n    ymin_input, ymax_input, xmin_input, xmax_input = ll_bounds\n    input_box   = box(xmin_input, ymin_input, xmax_input, ymax_input)\n    xmin, ymin, xmax, ymax = self.bbox\n    weather_model_box = box(xmin, ymin, xmax, ymax)\n    world_box  = box(-180, -90, 180, 90)\n\n    # Logger\n    input_box_str = [f'{x:1.2f}' for x in [xmin_input, ymin_input,\n                                           xmax_input, ymax_input]]\n    weath_box_str = [f'{x:1.2f}' for x in [xmin, ymin, xmax, ymax]]\n\n    weath_box_str = ', '.join(weath_box_str)\n    input_box_str = ', '.join(input_box_str)\n\n    logger.info(f'Extent of the weather model is (xmin, ymin, xmax, ymax):'\n                f'{weath_box_str}')\n    logger.info(f'Extent of the input is (xmin, ymin, xmax, ymax): '\n                f'{input_box_str}')\n\n    # If the bounding box goes beyond the normal world extents\n    # Look at two x-translates, buffer them, and take their union.\n    if not world_box.contains(weather_model_box):\n        logger.info('Considering x-translates of weather model +/-360 '\n                    'as bounding box outside of -180, -90, 180, 90')\n        translates = [weather_model_box.buffer(buffer_deg),\n                      translate(weather_model_box,\n                                xoff=360).buffer(buffer_deg),\n                      translate(weather_model_box,\n                                xoff=-360).buffer(buffer_deg)\n                      ]\n        weather_model_box = unary_union(translates)\n\n    return weather_model_box.contains(input_box)\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkTime","title":"<code>checkTime(time)</code>","text":"<p>Checks the time against the lag time and valid date range for the given model type</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def checkTime(self, time):\n'''\n    Checks the time against the lag time and valid date range for the given model type\n    '''\n    end_time = self._valid_range[1]\n    end_time = end_time if isinstance(end_time, str) else end_time.date()\n\n    logger.info(\n        'Weather model %s is available from %s to %s',\n        self.Model(), self._valid_range[0].date(), end_time\n    )\n\n    msg = f\"Weather model {self.Model()} is not available at: {time}\"\n\n    if time &lt; self._valid_range[0]:\n        logger.error(msg)\n        raise RuntimeError(msg)\n\n    if self._valid_range[1] is not None:\n        if self._valid_range[1] == 'Present':\n            pass\n        elif self._valid_range[1] &lt; time:\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n    if time &gt; datetime.datetime.utcnow() - self._lag_time:\n        logger.error(msg)\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.checkValidBounds","title":"<code>checkValidBounds(ll_bounds)</code>","text":"<p>Checks whether the given bounding box is valid for the model (i.e., intersects with the model domain at all)</p> <p>Args: ll_bounds : np.ndarray</p> <p>Returns:</p> Type Description <p>The weather model object</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def checkValidBounds(\n        self: weatherModel,\n        ll_bounds: np.ndarray,\n                     ):\n'''\n    Checks whether the given bounding box is valid for the model\n    (i.e., intersects with the model domain at all)\n\n    Args:\n    ll_bounds : np.ndarray\n\n    Returns:\n        The weather model object\n    '''\n    S, N, W, E = ll_bounds\n    if box(W, S, E, N).intersects(self._valid_bounds):\n        Mod = self\n\n    else:\n        raise ValueError(f'The requested location is unavailable for {self._Name}')\n\n    return Mod\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.fetch","title":"<code>fetch(out, time)</code>","text":"<p>Checks the input datetime against the valid date range for the model and then calls the model _fetch routine</p>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.fetch--args","title":"Args:","text":"<p>out - ll_bounds - 4 x 1 array, SNWE time = UTC datetime</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def fetch(self, out, time):\n'''\n    Checks the input datetime against the valid date range for the model and then\n    calls the model _fetch routine\n\n    Args:\n    ----------\n    out -\n    ll_bounds - 4 x 1 array, SNWE\n    time = UTC datetime\n    '''\n    self.checkTime(time)\n\n    # write the error raised by the weather model API to the log\n    try:\n        self._fetch(out)\n        err = False\n\n    except Exception as E:\n        err = E\n\n    return err\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.filename","title":"<code>filename(time=None, outLoc='weather_files')</code>","text":"<p>Create a filename to store the weather model</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def filename(self, time=None, outLoc='weather_files'):\n'''\n    Create a filename to store the weather model\n    '''\n    os.makedirs(outLoc, exist_ok=True)\n\n    if time is None:\n        if self._time is None:\n            raise ValueError('Time must be specified before the file can be written')\n        else:\n            time = self._time\n\n    f = make_raw_weather_data_filename(\n        outLoc,\n        self._Name,\n        time,\n    )\n\n    self.files = [f]\n    return f\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.getProjection","title":"<code>getProjection()</code>","text":"<p>Returns: the native weather projection, which should be a pyproj object</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def getProjection(self):\n'''\n    Returns: the native weather projection, which should be a pyproj object\n    '''\n    return self._proj\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.get_wmLoc","title":"<code>get_wmLoc()</code>","text":"<p>Get the path to the direct with the weather model files</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def get_wmLoc(self):\n\"\"\" Get the path to the direct with the weather model files \"\"\"\n    if self._wmLoc is None:\n        wmLoc = os.path.join(os.getcwd(), 'weather_files')\n    else:\n        wmLoc = self._wmLoc\n    return wmLoc\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.load","title":"<code>load(*args, _zlevels=None, **kwargs)</code>","text":"<p>Calls the load_weather method. Each model class should define a load_weather method appropriate for that class. 'args' should be one or more filenames.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def load(\n    self,\n    *args,\n    _zlevels=None,\n    **kwargs\n):\n'''\n    Calls the load_weather method. Each model class should define a load_weather\n    method appropriate for that class. 'args' should be one or more filenames.\n    '''\n    # If the weather file has already been processed, do nothing\n    outLoc = self.get_wmLoc()\n    path_wm_raw    = make_raw_weather_data_filename(outLoc, self.Model(), self.getTime())\n    self._out_name = self.out_file(outLoc)\n\n    if os.path.exists(self._out_name):\n        return self._out_name\n    else:\n        # Load the weather just for the query points\n        self.load_weather(f=path_wm_raw, *args, **kwargs)\n\n        # Process the weather model data\n        self._find_e()\n        self._uniform_in_z(_zlevels=_zlevels)\n        self._checkForNans()\n        self._get_wet_refractivity()\n        self._get_hydro_refractivity()\n        self._adjust_grid(self.get_latlon_bounds())\n\n        # Compute Zenith delays at the weather model grid nodes\n        self._getZTD()\n        return None\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.load_weather","title":"<code>load_weather(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder method. Should be implemented in each weather model type class</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>@abstractmethod\ndef load_weather(self, *args, **kwargs):\n'''\n    Placeholder method. Should be implemented in each weather model type class\n    '''\n    pass\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.plot","title":"<code>plot(plotType='pqt', savefig=True)</code>","text":"<p>Plotting method. Valid plot types are 'pqt'</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def plot(self, plotType='pqt', savefig=True):\n'''\n    Plotting method. Valid plot types are 'pqt'\n    '''\n    if plotType == 'pqt':\n        plot = plots.plot_pqt(self, savefig)\n    elif plotType == 'wh':\n        plot = plots.plot_wh(self, savefig)\n    else:\n        raise RuntimeError('WeatherModel.plot: No plotType named {}'.format(plotType))\n    return plot\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.setLevelType","title":"<code>setLevelType(levelType)</code>","text":"<p>Set the level type to model levels or pressure levels</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def setLevelType(self, levelType):\n'''Set the level type to model levels or pressure levels'''\n    if levelType in 'ml pl nat prs'.split():\n        self._model_level_type = levelType\n    else:\n        raise RuntimeError(f'Level type {levelType} is not recognized')\n\n    if levelType in 'ml nat'.split():\n        self.__model_levels__()\n    else:\n        self.__pressure_levels__()\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.setTime","title":"<code>setTime(time, fmt='%Y-%m-%dT%H:%M:%S')</code>","text":"<p>Set the time for a weather model</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def setTime(self, time, fmt='%Y-%m-%dT%H:%M:%S'):\n''' Set the time for a weather model '''\n    if isinstance(time, str):\n        self._time = datetime.datetime.strptime(time, fmt)\n    elif isinstance(time, datetime.datetime):\n        self._time = time\n    else:\n        raise ValueError('\"time\" must be a string or a datetime object')\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.set_latlon_bounds","title":"<code>set_latlon_bounds(ll_bounds, Nextra=2, output_spacing=None)</code>","text":"<p>Need to correct lat/lon bounds because not all of the weather models have valid data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2, need to adjust the longitude higher end with an extra buffer; for other models, the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be rounded to the above regions (either in the downloading-file API or subsetting- data API) without problems.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def set_latlon_bounds(self, ll_bounds, Nextra=2, output_spacing=None):\n'''\n    Need to correct lat/lon bounds because not all of the weather models have valid\n    data exactly bounded by -90/90 (lats) and -180/180 (lons); for GMAO and MERRA2,\n    need to adjust the longitude higher end with an extra buffer; for other models,\n    the exact bounds are close to -90/90 (lats) and -180/180 (lons) and thus can be\n    rounded to the above regions (either in the downloading-file API or subsetting-\n    data API) without problems.\n    '''\n    ex_buffer_lon_max = 0.0\n\n    if self._Name in 'HRRR HRRR-AK HRES'.split():\n        Nextra = 6 # have a bigger buffer\n\n    else:\n        ex_buffer_lon_max = self._lon_res\n\n    # At boundary lats and lons, need to modify Nextra buffer so that the lats and lons do not exceed the boundary\n    S, N, W, E = ll_bounds\n\n    # Adjust bounds if they get near the poles or IDL\n    pixlat, pixlon = Nextra*self._lat_res, Nextra*self._lon_res\n\n    S= np.max([S - pixlat, -90.0 + pixlat])\n    N= np.min([N + pixlat, 90.0 - pixlat])\n    W= np.max([W - (pixlon + ex_buffer_lon_max), -180.0 + (pixlon+ex_buffer_lon_max)])\n    E= np.min([E + (pixlon + ex_buffer_lon_max), 180.0 - pixlon - ex_buffer_lon_max])\n    if output_spacing is not None:\n        S, N, W, E  = clip_bbox([S,N,W,E], output_spacing)\n\n    self._ll_bounds = np.array([S, N, W, E])\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.set_wmLoc","title":"<code>set_wmLoc(weather_model_directory)</code>","text":"<p>Set the path to the directory with the weather model files</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def set_wmLoc(self, weather_model_directory:str):\n\"\"\" Set the path to the directory with the weather model files \"\"\"\n    self._wmLoc = weather_model_directory\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.WeatherModel.write","title":"<code>write()</code>","text":"<p>By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the weather model data and refractivity to an NETCDF4 file that can be accessed by external programs.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def write(self):\n'''\n    By calling the abstract/modular netcdf writer\n    (RAiDER.utilFcns.write2NETCDF4core), write the weather model data\n    and refractivity to an NETCDF4 file that can be accessed by external programs.\n    '''\n    # Generate the filename\n    f = self._out_name\n\n    attrs_dict = {\n            \"Conventions\": 'CF-1.6',\n            \"datetime\": datetime.datetime.strftime(self._time, \"%Y_%m_%dT%H_%M_%S\"),\n            'date_created': datetime.datetime.now().strftime(\"%Y_%m_%dT%H_%M_%S\"),\n            'title': 'Weather model data and delay calculations',\n\n        }\n\n    dimension_dict = {\n        'x': ('x', self._xs),\n        'y': ('y', self._ys),\n        'z': ('z', self._zs),\n        'latitude': (('y', 'x'), self._lats),\n        'longitude': (('y', 'x'), self._lons),\n        'datetime': self._time,\n    }\n\n    dataset_dict = {\n        't': (('z', 'y', 'x'), self._t.swapaxes(0, 2).swapaxes(1, 2)),\n        'p': (('z', 'y', 'x'), self._p.swapaxes(0, 2).swapaxes(1, 2)),\n        'e': (('z', 'y', 'x'), self._e.swapaxes(0, 2).swapaxes(1, 2)),\n        'wet': (('z', 'y', 'x'), self._wet_refractivity.swapaxes(0, 2).swapaxes(1, 2)),\n        'hydro': (('z', 'y', 'x'), self._hydrostatic_refractivity.swapaxes(0, 2).swapaxes(1, 2)),\n        'wet_total': (('z', 'y', 'x'), self._wet_ztd.swapaxes(0, 2).swapaxes(1, 2)),\n        'hydro_total': (('z', 'y', 'x'), self._hydrostatic_ztd.swapaxes(0, 2).swapaxes(1, 2)),\n    }\n\n    ds = xarray.Dataset(data_vars=dataset_dict, coords=dimension_dict, attrs=attrs_dict)\n\n    # Define units\n    ds['t'].attrs['units'] = 'K'\n    ds['e'].attrs['units'] = 'Pa'\n    ds['p'].attrs['units'] = 'Pa'\n    ds['wet'].attrs['units'] = 'dimentionless'\n    ds['hydro'].attrs['units'] = 'dimentionless'\n    ds['wet_total'].attrs['units'] = 'm'\n    ds['hydro_total'].attrs['units'] = 'm'\n\n    # Define standard names\n    ds['t'].attrs['standard_name'] = 'temperature'\n    ds['e'].attrs['standard_name'] = 'humidity'\n    ds['p'].attrs['standard_name'] = 'pressure'\n    ds['wet'].attrs['standard_name'] = 'wet_refractivity'\n    ds['hydro'].attrs['standard_name'] = 'hydrostatic_refractivity'\n    ds['wet_total'].attrs['standard_name'] = 'total_wet_refractivity'\n    ds['hydro_total'].attrs['standard_name'] = 'total_hydrostatic_refractivity'\n\n    # projection information\n    ds[\"proj\"] = int()\n    for k, v in self._proj.to_cf().items():\n        ds.proj.attrs[k] = v\n    for var in ds.data_vars:\n        ds[var].attrs['grid_mapping'] = 'proj'\n\n    # write to file and return the filename\n    ds.to_netcdf(f)\n    return f\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.checkContainment_raw","title":"<code>checkContainment_raw(path_wm_raw, ll_bounds, buffer_deg=1e-05)</code>","text":"<p>\" Checks if existing raw weather model contains requested ll_bounds</p>"},{"location":"reference/#RAiDER.models.weatherModel.checkContainment_raw--args","title":"Args:","text":"<p>path_wm_raw : path to downloaded, uncropped weather model file ll_bounds: an array of floats (SNWE) demarcating bbox of targets buffer_deg : float     For x-translates for extents that lie outside of world bounding box,     this ensures that translates have some overlap. The default is 1e-5     or ~11.1 meters.</p>"},{"location":"reference/#RAiDER.models.weatherModel.checkContainment_raw--returns","title":"Returns:","text":"<p>bool     True if weather model contains bounding box of OutLats and outLons     and False otherwise.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def checkContainment_raw(path_wm_raw,\n                        ll_bounds,\n                        buffer_deg: float = 1e-5) -&gt; bool:\n\"\"\"\"\n    Checks if existing raw weather model contains\n    requested ll_bounds\n\n    Args:\n    ----------\n    path_wm_raw : path to downloaded, uncropped weather model file\n    ll_bounds: an array of floats (SNWE) demarcating bbox of targets\n    buffer_deg : float\n        For x-translates for extents that lie outside of world bounding box,\n        this ensures that translates have some overlap. The default is 1e-5\n        or ~11.1 meters.\n\n    Returns:\n    -------\n    bool\n        True if weather model contains bounding box of OutLats and outLons\n        and False otherwise.\n    \"\"\"\n    import xarray as xr\n    ymin_input, ymax_input, xmin_input, xmax_input = ll_bounds\n    input_box   = box(xmin_input, ymin_input, xmax_input, ymax_input)\n\n    with xr.open_dataset(path_wm_raw) as ds:\n        try:\n            ymin, ymax = ds.latitude.min(), ds.latitude.max()\n            xmin, xmax = ds.longitude.min(), ds.longitude.max()\n        except:\n            ymin, ymax = ds.y.min(), ds.y.max()\n            xmin, xmax = ds.x.min(), ds.x.max()\n\n        xmin, xmax = np.mod(np.array([xmin, xmax])+180, 360) - 180\n        weather_model_box = box(xmin, ymin, xmax, ymax)\n\n    world_box  = box(-180, -90, 180, 90)\n\n    # Logger\n    input_box_str = [f'{x:1.2f}' for x in [xmin_input, ymin_input,\n                                            xmax_input, ymax_input]]\n    weath_box_str = [f'{x:1.2f}' for x in [xmin, ymin, xmax, ymax]]\n\n    weath_box_str = ', '.join(weath_box_str)\n    input_box_str = ', '.join(input_box_str)\n\n\n    # If the bounding box goes beyond the normal world extents\n    # Look at two x-translates, buffer them, and take their union.\n    if not world_box.contains(weather_model_box):\n        logger.info('Considering x-translates of weather model +/-360 '\n                    'as bounding box outside of -180, -90, 180, 90')\n        translates = [weather_model_box.buffer(buffer_deg),\n                        translate(weather_model_box,\n                                xoff=360).buffer(buffer_deg),\n                        translate(weather_model_box,\n                                xoff=-360).buffer(buffer_deg)\n                        ]\n        weather_model_box = unary_union(translates)\n\n    return weather_model_box.contains(input_box)\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.find_svp","title":"<code>find_svp(t)</code>","text":"<p>Calculate standard vapor presure. Should be model-specific</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def find_svp(t):\n\"\"\"\n    Calculate standard vapor presure. Should be model-specific\n    \"\"\"\n    # From TRAIN:\n    # Could not find the wrf used equation as they appear to be\n    # mixed with latent heat etc. Istead I used the equations used\n    # in ERA-I (see IFS documentation part 2: Data assimilation\n    # (CY25R1)). Calculate saturated water vapour pressure (svp) for\n    # water (svpw) using Buck 1881 and for ice (swpi) from Alduchow\n    # and Eskridge (1996) euation AERKi\n\n    # TODO: figure out the sources of all these magic numbers and move\n    # them somewhere more visible.\n    # TODO: (Jeremy) - Need to fix/get the equation for the other\n    # weather model types. Right now this will be used for all models,\n    # except WRF, which is yet to be implemented in my new structure.\n    t1 = 273.15  # O Celsius\n    t2 = 250.15  # -23 Celsius\n\n    tref = t - t1\n    wgt = (t - t2) / (t1 - t2)\n    svpw = (6.1121 * np.exp((17.502 * tref) / (240.97 + tref)))\n    svpi = (6.1121 * np.exp((22.587 * tref) / (273.86 + tref)))\n\n    svp = svpi + (svpw - svpi) * wgt**2\n    ix_bound1 = t &gt; t1\n    svp[ix_bound1] = svpw[ix_bound1]\n    ix_bound2 = t &lt; t2\n    svp[ix_bound2] = svpi[ix_bound2]\n\n    svp = svp * 100\n    return svp.astype(np.float32)\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.get_mapping","title":"<code>get_mapping(proj)</code>","text":"<p>Get CF-complient projection information from a proj</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def get_mapping(proj):\n'''Get CF-complient projection information from a proj'''\n    # In case of WGS-84 lat/lon, keep it simple\n    if proj.to_epsg()==4326:\n        return 'WGS84'\n    else:\n        return proj.to_wkt()\n</code></pre>"},{"location":"reference/#RAiDER.models.weatherModel.make_raw_weather_data_filename","title":"<code>make_raw_weather_data_filename(outLoc, name, time)</code>","text":"<p>Filename generator for the raw downloaded weather model data</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/weatherModel.py</code> <pre><code>def make_raw_weather_data_filename(outLoc, name, time):\n''' Filename generator for the raw downloaded weather model data '''\n    f = os.path.join(\n        outLoc,\n        '{}_{}.{}'.format(\n            name,\n            datetime.datetime.strftime(time, '%Y_%m_%d_T%H_%M_%S'),\n            'nc'\n        )\n    )\n    return f\n</code></pre>"},{"location":"reference/#RAiDER.models.wrf","title":"<code>wrf</code>","text":""},{"location":"reference/#RAiDER.models.wrf.UnitTypeError","title":"<code>UnitTypeError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Define a unit type exception for easily formatting error messages for units</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/wrf.py</code> <pre><code>class UnitTypeError(Exception):\n'''\n    Define a unit type exception for easily formatting\n    error messages for units\n    '''\n\n    def __init___(self, varName, unittype):\n        msg = \"Unknown units for {}: '{}'\".format(varName, unittype)\n        Exception.__init__(self, msg)\n</code></pre>"},{"location":"reference/#RAiDER.models.wrf.WRF","title":"<code>WRF</code>","text":"<p>             Bases: <code>WeatherModel</code></p> <p>WRF class definition, based on the WeatherModel base class.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/wrf.py</code> <pre><code>class WRF(WeatherModel):\n'''\n    WRF class definition, based on the WeatherModel base class.\n    '''\n    # TODO: finish implementing\n\n    def __init__(self):\n        WeatherModel.__init__(self)\n\n        self._k1 = 0.776  # K/Pa\n        self._k2 = 0.233  # K/Pa\n        self._k3 = 3.75e3  # K^2/Pa\n\n\n        # Currently WRF is using RH instead of Q to get E\n        self._humidityType = 'rh'\n        self._Name = 'WRF'\n        self._time_res = TIME_RES[self._Name]\n\n    def _fetch(self):\n        pass\n\n    def load_weather(self, file1, file2, *args, **kwargs):\n'''\n        Consistent class method to be implemented across all weather model types\n        '''\n        try:\n            lons, lats = self._get_wm_nodes(file1)\n            self._read_netcdf(file2)\n        except KeyError:\n            self._get_wm_nodes(file2)\n            self._read_netcdf(file1)\n\n        # WRF doesn't give us the coordinates of the points in the native projection,\n        # only the coordinates in lat/long. Ray transformed these to the native\n        # projection, then used an average to enforce a regular grid. It does matter\n        # for the interpolation whether the grid is regular.\n        lla = CRS.from_epsg(4326)\n        t = Transformer.from_proj(lla, self._proj)\n        xs, ys = t.transform(lons.flatten(), lats.flatten())\n        xs = xs.reshape(lons.shape)\n        ys = ys.reshape(lats.shape)\n\n        # Expected accuracy here is to two decimal places (five significant digits)\n        xs = np.mean(xs, axis=0)\n        ys = np.mean(ys, axis=1)\n\n        _xs = np.broadcast_to(xs[np.newaxis, np.newaxis, :],\n                              self._p.shape)\n        _ys = np.broadcast_to(ys[np.newaxis, :, np.newaxis],\n                              self._p.shape)\n        # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n        self._p = np.transpose(self._p)\n        self._t = np.transpose(self._t)\n        self._rh = np.transpose(self._rh)\n        self._ys = np.transpose(_ys)\n        self._xs = np.transpose(_xs)\n        self._zs = np.transpose(self._zs)\n\n        # TODO: Not sure if WRF provides this\n        self._levels = list(range(self._zs.shape[2]))\n\n    def _get_wm_nodes(self, nodeFile):\n        with netcdf.netcdf_file(nodeFile, 'r', maskandscale=True) as outf:\n            lats = outf.variables['XLAT'][0].copy()  # Takes only the first date!\n            lons = outf.variables['XLONG'][0].copy()\n\n        lons[lons &gt; 180] -= 360\n\n        return lons, lats\n\n    def _read_netcdf(self, weatherFile, defNul=None):\n\"\"\"\n        Read weather variables from a netCDF file\n        \"\"\"\n        if defNul is None:\n            defNul = np.nan\n\n        # TODO: it'd be cool to use some kind of units package\n        # TODO: extract partial pressure directly (q?)\n        with netcdf.netcdf_file(weatherFile, 'r', maskandscale=True) as f:\n            spvar = f.variables['P_PL']\n            temp = f.variables['T_PL']\n            humid = f.variables['RH_PL']\n            geohvar = f.variables['GHT_PL']\n\n            lon0 = f.STAND_LON.copy()\n            lat0 = f.MOAD_CEN_LAT.copy()\n            lat1 = f.TRUELAT1.copy()\n            lat2 = f.TRUELAT2.copy()\n\n            checkUnits(spvar.units.decode('utf-8'), 'pressure')\n            checkUnits(temp.units.decode('utf-8'), 'temperature')\n            checkUnits(humid.units.decode('utf-8'), 'relative humidity')\n            checkUnits(geohvar.units.decode('utf-8'), 'geopotential')\n\n            # _FillValue is not always set, but when it is we want to read it\n            tNull = getNullValue(temp)\n            hNull = getNullValue(humid)\n            gNull = getNullValue(geohvar)\n            pNull = getNullValue(spvar)\n\n            sp = spvar[0].copy()\n            temps = temp[0].copy()\n            humids = humid[0].copy()\n            geoh = geohvar[0].copy()\n\n            spvar = None\n            temp = None\n            humid = None\n            geohvar = None\n\n        # Projection\n        # See http://www.pkrc.net/wrf-lambert.html\n        earthRadius = 6370e3  # &lt;- note Ray had a bug here\n        p1 = CRS(proj='lcc', lat_1=lat1,\n                 lat_2=lat2, lat_0=lat0,\n                 lon_0=lon0, a=earthRadius, b=earthRadius,\n                 towgs84=(0, 0, 0), no_defs=True)\n        self._proj = p1\n\n        temps[temps == tNull] = np.nan\n        sp[sp == pNull] = np.nan\n        humids[humids == hNull] = np.nan\n        geoh[geoh == gNull] = np.nan\n\n        self._t = temps\n        self._rh = humids\n\n        # Zs are problematic because any z below the topography is nan.\n        # For a temporary fix, I will assign any nan value to equal the\n        # nanmean of that level.\n        zmeans = np.nanmean(geoh, axis=(1, 2))\n        nz, ny, nx = geoh.shape\n        Zmeans = np.tile(zmeans, (nx, ny, 1))\n        Zmeans = Zmeans.T\n        ix = np.isnan(geoh)\n        geoh[ix] = Zmeans[ix]\n        self._zs = geoh\n\n        if len(sp.shape) == 1:\n            self._p = np.broadcast_to(\n                sp[:, np.newaxis, np.newaxis], self._zs.shape)\n        else:\n            self._p = sp\n</code></pre>"},{"location":"reference/#RAiDER.models.wrf.WRF.load_weather","title":"<code>load_weather(file1, file2, *args, **kwargs)</code>","text":"<p>Consistent class method to be implemented across all weather model types</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/wrf.py</code> <pre><code>def load_weather(self, file1, file2, *args, **kwargs):\n'''\n    Consistent class method to be implemented across all weather model types\n    '''\n    try:\n        lons, lats = self._get_wm_nodes(file1)\n        self._read_netcdf(file2)\n    except KeyError:\n        self._get_wm_nodes(file2)\n        self._read_netcdf(file1)\n\n    # WRF doesn't give us the coordinates of the points in the native projection,\n    # only the coordinates in lat/long. Ray transformed these to the native\n    # projection, then used an average to enforce a regular grid. It does matter\n    # for the interpolation whether the grid is regular.\n    lla = CRS.from_epsg(4326)\n    t = Transformer.from_proj(lla, self._proj)\n    xs, ys = t.transform(lons.flatten(), lats.flatten())\n    xs = xs.reshape(lons.shape)\n    ys = ys.reshape(lats.shape)\n\n    # Expected accuracy here is to two decimal places (five significant digits)\n    xs = np.mean(xs, axis=0)\n    ys = np.mean(ys, axis=1)\n\n    _xs = np.broadcast_to(xs[np.newaxis, np.newaxis, :],\n                          self._p.shape)\n    _ys = np.broadcast_to(ys[np.newaxis, :, np.newaxis],\n                          self._p.shape)\n    # Re-structure everything from (heights, lats, lons) to (lons, lats, heights)\n    self._p = np.transpose(self._p)\n    self._t = np.transpose(self._t)\n    self._rh = np.transpose(self._rh)\n    self._ys = np.transpose(_ys)\n    self._xs = np.transpose(_xs)\n    self._zs = np.transpose(self._zs)\n\n    # TODO: Not sure if WRF provides this\n    self._levels = list(range(self._zs.shape[2]))\n</code></pre>"},{"location":"reference/#RAiDER.models.wrf.checkUnits","title":"<code>checkUnits(unitCheck, varName)</code>","text":"<p>Implement a check that the units are as expected</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/wrf.py</code> <pre><code>def checkUnits(unitCheck, varName):\n'''\n    Implement a check that the units are as expected\n    '''\n    unitDict = {'pressure': 'Pa', 'temperature': 'K', 'relative humidity': '%', 'geopotential': 'm'}\n    if unitCheck != unitDict[varName]:\n        raise UnitTypeError(varName, unitCheck)\n</code></pre>"},{"location":"reference/#RAiDER.models.wrf.getNullValue","title":"<code>getNullValue(var)</code>","text":"<p>Get the null (or fill) value if it exists, otherwise set the null value to defNullValue</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/models/wrf.py</code> <pre><code>def getNullValue(var):\n'''\n    Get the null (or fill) value if it exists, otherwise set the null value to defNullValue\n    '''\n    # NetCDF files have the ability to record their nodata value, but in the\n    # particular NetCDF files that I'm reading, this field is left\n    # unspecified and a nodata value of -999 is used. The solution I'm using\n    # is to check if nodata is specified, and otherwise assume it's -999.\n    _default_fill_value = -999\n\n    try:\n        var_fill = var._FillValue\n    except AttributeError:\n        var_fill = _default_fill_value\n\n    return var_fill\n</code></pre>"},{"location":"reference/#RAiDER.processWM","title":"<code>processWM</code>","text":""},{"location":"reference/#RAiDER.processWM.prepareWeatherModel","title":"<code>prepareWeatherModel(weather_model, time, ll_bounds, download_only=False, makePlots=False, force_download=False)</code>","text":"<p>Parse inputs to download and prepare a weather model grid for interpolation</p> <p>Parameters:</p> Name Type Description Default <code>weather_model</code> <p>WeatherModel   - instantiated weather model object</p> required <code>time</code> <p>datetime                - Python datetime to request. Will be rounded to nearest available time</p> required <code>ll_bounds</code> <p>list/array        - SNWE bounds target area to ensure weather model contains them</p> required <code>download_only</code> <code>bool</code> <p>bool           - False if preprocessing weather model data</p> <code>False</code> <code>makePlots</code> <code>bool</code> <p>bool               - whether to write debug plots</p> <code>False</code> <code>force_download</code> <code>bool</code> <p>bool          - True if you want to download even when the weather model exists</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>filename of the netcdf file to which the weather model has been written</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/processWM.py</code> <pre><code>def prepareWeatherModel(\n        weather_model,\n        time,\n        ll_bounds,\n        download_only: bool=False,\n        makePlots: bool=False,\n        force_download: bool=False,\n    ) -&gt; str:\n\"\"\"Parse inputs to download and prepare a weather model grid for interpolation\n\n    Args:\n        weather_model: WeatherModel   - instantiated weather model object\n        time: datetime                - Python datetime to request. Will be rounded to nearest available time\n        ll_bounds: list/array        - SNWE bounds target area to ensure weather model contains them\n        download_only: bool           - False if preprocessing weather model data\n        makePlots: bool               - whether to write debug plots\n        force_download: bool          - True if you want to download even when the weather model exists\n\n    Returns:\n        str: filename of the netcdf file to which the weather model has been written\n    \"\"\"\n    ## set the bounding box from the in the case that it hasn't been set\n    if weather_model.get_latlon_bounds() is None:\n        weather_model.set_latlon_bounds(ll_bounds)\n\n    # Ensure the file output location exists\n    wmLoc     = weather_model.get_wmLoc()\n    weather_model.setTime(time)\n\n    # get the path to the less processed weather model file\n    path_wm_raw = make_raw_weather_data_filename(wmLoc, weather_model.Model(), time)\n\n    # get the path to the more processed (cropped) weather model file\n    path_wm_crop = weather_model.out_file(wmLoc)\n\n    # check whether weather model files exists and/or or should be downloaded\n    if os.path.exists(path_wm_crop) and not force_download:\n        logger.warning(\n            'Processed weather model already exists, please remove it (\"%s\") if you want '\n            'to download a new one.', path_wm_crop)\n\n    # check whether the raw weather model covers this area\n    elif os.path.exists(path_wm_raw) and \\\n        checkContainment_raw(path_wm_raw, ll_bounds) and not force_download:\n        logger.warning(\n            'Raw weather model already exists, please remove it (\"%s\") if you want '\n            'to download a new one.', path_wm_raw)\n\n    # if no weather model files supplied, check the standard location\n    else:\n        E = weather_model.fetch(path_wm_raw, time)\n        if E:\n            logger.warning (E)\n            raise RuntimeError\n\n    # If only downloading, exit now\n    if download_only:\n        logger.warning(\n            'download_only flag selected. No further processing will happen.'\n        )\n        return None\n\n    # Otherwise, load the weather model data\n    f = weather_model.load()\n\n    if f is not None:\n        logger.warning(\n            'The processed weather model file already exists,'\n            ' so I will use that.'\n        )\n\n        containment = weather_model.checkContainment(ll_bounds)\n        if not containment and weather_model.Model() in 'GMAO ERA5 ERA5T HRES'.split():\n            msg = 'The weather model passed does not cover all of the input ' \\\n                'points; you may need to download a larger area.'\n            logger.error(msg)\n            raise RuntimeError(msg)\n        return f\n\n    # Logging some basic info\n    logger.debug(\n        'Number of weather model nodes: %s',\n            np.prod(weather_model.getWetRefractivity().shape)\n            )\n    shape = weather_model.getWetRefractivity().shape\n    logger.debug(f'Shape of weather model: {shape}')\n    logger.debug(\n        'Bounds of the weather model: %.2f/%.2f/%.2f/%.2f (SNWE)',\n        np.nanmin(weather_model._ys), np.nanmax(weather_model._ys),\n        np.nanmin(weather_model._xs), np.nanmax(weather_model._xs)\n    )\n    logger.debug('Weather model: %s', weather_model.Model())\n    logger.debug(\n        'Mean value of the wet refractivity: %f',\n        np.nanmean(weather_model.getWetRefractivity())\n    )\n    logger.debug(\n        'Mean value of the hydrostatic refractivity: %f',\n        np.nanmean(weather_model.getHydroRefractivity())\n    )\n    logger.debug(weather_model)\n\n    if makePlots:\n        weather_model.plot('wh', True)\n        weather_model.plot('pqt', True)\n        plt.close('all')\n\n    try:\n        f = weather_model.write()\n        containment = weather_model.checkContainment(ll_bounds)\n\n    except Exception as e:\n        logger.exception(\"Unable to save weathermodel to file\")\n        logger.exception(e)\n        raise RuntimeError(\"Unable to save weathermodel to file\")\n\n    finally:\n        wm = weather_model.Model()\n        del weather_model\n\n    if not containment and wm in 'GMAO ERA5 ERA5T HRES'.split():\n        msg = 'The weather model passed does not cover all of the input ' \\\n            'points; you may need to download a larger area.'\n        logger.error(msg)\n        raise RuntimeError(msg)\n    else:\n        return f\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing","title":"<code>s1_azimuth_timing</code>","text":""},{"location":"reference/#RAiDER.s1_azimuth_timing.get_azimuth_time_grid","title":"<code>get_azimuth_time_grid(lon_mesh, lat_mesh, hgt_mesh, orb)</code>","text":"<p>Source: https://github.com/dbekaert/RAiDER/blob/dev/tools/RAiDER/losreader.py#L601C1-L674C22</p> <p>lon_mesh, lat_mesh, hgt_mesh are coordinate arrays (this routine makes a mesh to comute azimuth timing grid)</p> <p>Technically, this is \"sensor neutral\" since it uses an orb object.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/s1_azimuth_timing.py</code> <pre><code>def get_azimuth_time_grid(lon_mesh: np.ndarray,\n                          lat_mesh: np.ndarray,\n                          hgt_mesh:  np.ndarray,\n                          orb: isce.core.Orbit) -&gt; np.ndarray:\n'''\n    Source: https://github.com/dbekaert/RAiDER/blob/dev/tools/RAiDER/losreader.py#L601C1-L674C22\n\n    lon_mesh, lat_mesh, hgt_mesh are coordinate arrays (this routine makes a mesh to comute azimuth timing grid)\n\n    Technically, this is \"sensor neutral\" since it uses an orb object.\n    '''\n\n    num_iteration = 100\n    residual_threshold = 1.0e-7\n\n    elp = isce.core.Ellipsoid()\n    dop = isce.core.LUT2d()\n    look = isce.core.LookSide.Right\n\n    m, n, p = hgt_mesh.shape\n    az_arr = np.full((m, n, p),\n                     np.datetime64('NaT'),\n                     # source: https://stackoverflow.com/a/27469108\n                     dtype='datetime64[ms]')\n\n    for ind_0 in range(m):\n        for ind_1 in range(n):\n            for ind_2 in range(p):\n\n                hgt_pt, lat_pt, lon_pt = (hgt_mesh[ind_0, ind_1, ind_2],\n                                          lat_mesh[ind_0, ind_1, ind_2],\n                                          lon_mesh[ind_0, ind_1, ind_2])\n\n                input_vec = np.array([np.deg2rad(lon_pt),\n                                      np.deg2rad(lat_pt),\n                                      hgt_pt])\n\n                aztime, sr = isce.geometry.geo2rdr(\n                    input_vec, elp, orb, dop, 0.06, look,\n                    threshold=residual_threshold,\n                    maxiter=num_iteration,\n                    delta_range=10.0)\n\n                rng_seconds = sr / isce.core.speed_of_light\n                aztime = aztime + rng_seconds\n                aztime_isce = orb.reference_epoch + isce.core.TimeDelta(aztime)\n                aztime_np = np.datetime64(aztime_isce.isoformat())\n                az_arr[ind_0, ind_1, ind_2] = aztime_np\n    return az_arr\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_inverse_weights_for_dates","title":"<code>get_inverse_weights_for_dates(azimuth_time_array, dates, inverse_regularizer=1e-09, temporal_window_hours=None)</code>","text":"<p>Obtains weights according to inverse weighting with respect to the absolute difference between azimuth timing array and dates. The output will be a list with length equal to that of dates and whose entries are arrays each whose shape matches the azimuth_timing_array.</p> <p>Note: we do not do any checking of the dates provided so the inferred <code>temporal_window_hours</code> may be incorrect.</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_inverse_weights_for_dates--parameters","title":"Parameters","text":"<p>azimuth_time_array : np.ndarray     Array of type <code>np.datetime64[ms]</code> dates : list[datetime.datetime]     List of datetimes inverse_regularizer : float, optional     If a <code>time</code> in the azimuth time arr equals one of the given dates, then the regularlizer ensures that the value     <code>1 / (|date - time| + inverse_regularizer) = weight</code> is not infinity, by default 1e-9 temporal_window_hours : float, optional     Values outside of this are masked from inverse weighted.     If None, then window is minimum abs difference of dates (inferring the temporal resolution), by default None     No check of equi-spaced dates are done so not specifying temporal window hours requires dates to be derived     from valid model time steps</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_inverse_weights_for_dates--returns","title":"Returns","text":"<p>list[np.ndarray]     Weighting per pixel with respect to each date</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/s1_azimuth_timing.py</code> <pre><code>def get_inverse_weights_for_dates(azimuth_time_array: np.ndarray,\n                                  dates: list[datetime.datetime],\n                                  inverse_regularizer: float = 1e-9,\n                                  temporal_window_hours: float = None) -&gt; list[np.ndarray]:\n\"\"\"Obtains weights according to inverse weighting with respect to the absolute difference between\n    azimuth timing array and dates. The output will be a list with length equal to that of dates and\n    whose entries are arrays each whose shape matches the azimuth_timing_array.\n\n    Note: we do not do any checking of the dates provided so the inferred `temporal_window_hours` may be incorrect.\n\n    Parameters\n    ----------\n    azimuth_time_array : np.ndarray\n        Array of type `np.datetime64[ms]`\n    dates : list[datetime.datetime]\n        List of datetimes\n    inverse_regularizer : float, optional\n        If a `time` in the azimuth time arr equals one of the given dates, then the regularlizer ensures that the value\n        `1 / (|date - time| + inverse_regularizer) = weight` is not infinity, by default 1e-9\n    temporal_window_hours : float, optional\n        Values outside of this are masked from inverse weighted.\n        If None, then window is minimum abs difference of dates (inferring the temporal resolution), by default None\n        No check of equi-spaced dates are done so not specifying temporal window hours requires dates to be derived\n        from valid model time steps\n\n    Returns\n    -------\n    list[np.ndarray]\n        Weighting per pixel with respect to each date\n    \"\"\"\n    if not all([isinstance(date, datetime.datetime) for date in dates]):\n        raise TypeError('dates must be all datetimes')\n    if temporal_window_hours is None:\n        temporal_window_seconds = min([abs((date - dates[0]).total_seconds()) for date in dates[1:]])\n    else:\n        temporal_window_seconds = temporal_window_hours * 60 * 60\n\n    # Get absolute differences\n    dates_np = list(map(np.datetime64, dates))\n    abs_diff = [np.abs(azimuth_time_array - date) / np.timedelta64(1, 's') for date in dates_np]\n\n    # Get inverse weighting with mask determined by window\n    wgts = [1. / (diff + inverse_regularizer) for diff in abs_diff]\n    masks = [(diff &lt;= temporal_window_seconds).astype(int) for diff in abs_diff]\n\n    if all([mask.sum() == 0 for mask in masks]):\n        raise ValueError('No dates provided are within temporal window')\n\n    # Normalize so that sum of weights is 1\n    wgts_masked = [wgt * mask for wgt, mask in zip(wgts, masks)]\n    wgts_sum = np.sum(np.stack(wgts_masked, axis=-1), axis=-1)\n    wgts_norm = [wgt / wgts_sum for wgt in wgts_masked]\n    return wgts_norm\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_n_closest_datetimes","title":"<code>get_n_closest_datetimes(ref_time, n_target_times, time_step_hours)</code>","text":"<p>Gets n closes times relative to the <code>round_to_hour_delta</code> and the <code>ref_time</code>. Specifically, if one is interetsted in getting 3 closest times to say 0, 6, 12, 18 UTC times of a ref time <code>dt</code>, then:</p> <pre><code>dt = datetime.datetime(2023, 1, 1, 11, 0, 0)\nget_n_closest_datetimes(dt, 3, 6)\n</code></pre> <p>gives the desired answer of</p> <pre><code>[datetime.datetime(2023, 1, 1, 12, 0, 0),\n datetime.datetime(2023, 1, 1, 6, 0, 0),\n datetime.datetime(2023, 1, 1, 18, 0, 0)]\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_n_closest_datetimes--parameters","title":"Parameters","text":"<p>ref_time : datetime.datetime     Time to round from n_times : int     Number of times to get time_step_hours : int     If 1, then rounds ref_time to nearest hour(s). If 2, then rounds to     nearest 0, 2, 4, etc. times. Must be divisible by 24 otherwise is     not consistent across all days.</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_n_closest_datetimes--returns","title":"Returns","text":"<p>list[datetime.datetime]     List of closest dates ordered by absolute proximity</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/s1_azimuth_timing.py</code> <pre><code>def get_n_closest_datetimes(ref_time: datetime.datetime,\n                            n_target_times: int,\n                            time_step_hours: int) -&gt; list[datetime.datetime]:\n\"\"\"Gets n closes times relative to the `round_to_hour_delta` and the\n    `ref_time`. Specifically, if one is interetsted in getting 3 closest times\n    to say 0, 6, 12, 18 UTC times of a ref time `dt`, then:\n    ```\n    dt = datetime.datetime(2023, 1, 1, 11, 0, 0)\n    get_n_closest_datetimes(dt, 3, 6)\n    ```\n    gives the desired answer of\n    ```\n    [datetime.datetime(2023, 1, 1, 12, 0, 0),\n     datetime.datetime(2023, 1, 1, 6, 0, 0),\n     datetime.datetime(2023, 1, 1, 18, 0, 0)]\n    ```\n\n    Parameters\n    ----------\n    ref_time : datetime.datetime\n        Time to round from\n    n_times : int\n        Number of times to get\n    time_step_hours : int\n        If 1, then rounds ref_time to nearest hour(s). If 2, then rounds to\n        nearest 0, 2, 4, etc. times. Must be divisible by 24 otherwise is\n        not consistent across all days.\n\n    Returns\n    -------\n    list[datetime.datetime]\n        List of closest dates ordered by absolute proximity\n    \"\"\"\n    iterations = int(np.ceil(n_target_times / 2))\n    closest_times = []\n\n    if (24 % time_step_hours) != 0:\n        raise ValueError('The time step does not evenly divide 24 hours;'\n                         'Time step has period &gt; 1 day and depends when model '\n                         'starts')\n\n    ts = pd.Timestamp(ref_time)\n    for k in range(iterations):\n        ts_0 = pd.Timestamp(ref_time) - pd.Timedelta(hours=(time_step_hours * k))\n        ts_1 = pd.Timestamp(ref_time) + pd.Timedelta(hours=(time_step_hours * k))\n\n        t_ceil = ts_0.floor(f'{time_step_hours}H')\n        t_floor = ts_1.ceil(f'{time_step_hours}H')\n\n        closest_times.extend([t_ceil, t_floor])\n    closest_times = sorted(closest_times, key=lambda ts_rounded: abs(ts - ts_rounded))\n    closest_times = [t.to_pydatetime() for t in closest_times]\n    closest_times = closest_times[:n_target_times]\n    return closest_times\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_s1_azimuth_time_grid","title":"<code>get_s1_azimuth_time_grid(lon, lat, hgt, dt)</code>","text":"<p>Based on the lon, lat, hgt (3d cube) - obtains an associated s1 orbit file to calculate the azimuth timing across the cube. Requires datetime of acq associated to cube.</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_s1_azimuth_time_grid--parameters","title":"Parameters","text":"<p>lon : np.ndarray     1 dimensional coordinate array or 3d mesh of coordinates lat : np.ndarray     1 dimensional coordinate array or 3d mesh of coordinates hgt : np.ndarray     1 dimensional coordinate array or 3d mesh of coordinates dt : datetime.datetime</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_s1_azimuth_time_grid--returns","title":"Returns","text":"<p>np.ndarray     Cube whose coordinates are hgt x lat x lon with each pixel</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/s1_azimuth_timing.py</code> <pre><code>def get_s1_azimuth_time_grid(lon: np.ndarray,\n                             lat: np.ndarray,\n                             hgt:  np.ndarray,\n                             dt: datetime.datetime) -&gt; np.ndarray:\n\"\"\"Based on the lon, lat, hgt (3d cube) - obtains an associated s1 orbit\n    file to calculate the azimuth timing across the cube. Requires datetime of acq\n    associated to cube.\n\n    Parameters\n    ----------\n    lon : np.ndarray\n        1 dimensional coordinate array or 3d mesh of coordinates\n    lat : np.ndarray\n        1 dimensional coordinate array or 3d mesh of coordinates\n    hgt : np.ndarray\n        1 dimensional coordinate array or 3d mesh of coordinates\n    dt : datetime.datetime\n\n    Returns\n    -------\n    np.ndarray\n        Cube whose coordinates are hgt x lat x lon with each pixel\n    \"\"\"\n    dims = [len(c.shape) for c in [lon, lat, hgt]]\n    if not all([dim == dims[0] for dim in dims]):\n        raise ValueError('All coordinates have same dimension (either 1 or 3 dimensional)')\n    if not all([dim in [1, 3] for dim in dims]):\n        raise ValueError('Coordinates must be 1d or 3d coordinate arrays')\n\n    if dims[0] == 1:\n        hgt_mesh, lat_mesh, lon_mesh = np.meshgrid(hgt, lat, lon,\n                                                   # indexing keyword argument\n                                                   # Ensures output dimensions\n                                                   # align with order the inputs\n                                                   # height x latitude x longitude\n                                                   indexing='ij')\n    else:\n        hgt_mesh = hgt\n        lat_mesh = lat\n        lon_mesh = lon\n\n    try:\n        lon_m = np.mean(lon)\n        lat_m = np.mean(lat)\n        slc_ids = get_slc_id_from_point_and_time(lon_m, lat_m, dt)\n    except ValueError:\n        warnings.warn('No slc id found for the given datetime and grid; returning empty grid')\n        m, n, p = hgt_mesh.shape\n        az_arr = np.full((m, n, p),\n                         np.datetime64('NaT'),\n                         dtype='datetime64[ms]')\n        return az_arr\n    orb_files = list(map(lambda slc_id: hyp3lib.get_orb.downloadSentinelOrbitFile(slc_id)[0], slc_ids))\n    orb = get_isce_orbit(orb_files, dt, pad=600)\n\n    az_arr = get_azimuth_time_grid(lon_mesh, lat_mesh, hgt_mesh, orb)\n    return az_arr\n</code></pre>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_slc_id_from_point_and_time","title":"<code>get_slc_id_from_point_and_time(lon, lat, dt, buffer_seconds=600, buffer_deg=2)</code>","text":"<p>Obtains a (non-unique) SLC id from the lon/lat and datetime of inputs. The buffere ensures that an SLC id is within the queried start/end times. Note an S1 scene takes roughly 30 seconds to acquire.</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_slc_id_from_point_and_time--parameters","title":"Parameters","text":"<p>lon : float lat : float dt : datetime.datetime buffer_seconds : int, optional     Do not recommend adjusting this, by default 600, to ensure enough padding for multiple orbit files</p>"},{"location":"reference/#RAiDER.s1_azimuth_timing.get_slc_id_from_point_and_time--returns","title":"Returns","text":"<p>list     All slc_ids returned by asf_search</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/s1_azimuth_timing.py</code> <pre><code>def get_slc_id_from_point_and_time(lon: float,\n                                   lat: float,\n                                   dt: datetime.datetime,\n                                   buffer_seconds: int = 600,\n                                   buffer_deg: float = 2) -&gt; list:\n\"\"\"Obtains a (non-unique) SLC id from the lon/lat and datetime of inputs. The buffere ensures that\n    an SLC id is within the queried start/end times. Note an S1 scene takes roughly 30 seconds to acquire.\n\n    Parameters\n    ----------\n    lon : float\n    lat : float\n    dt : datetime.datetime\n    buffer_seconds : int, optional\n        Do not recommend adjusting this, by default 600, to ensure enough padding for multiple orbit files\n\n    Returns\n    -------\n    list\n        All slc_ids returned by asf_search\n    \"\"\"\n    point = Point(lon, lat)\n    time_delta = datetime.timedelta(seconds=buffer_seconds)\n    start = dt - time_delta\n    end = dt + time_delta\n\n    # Requires buffer of degrees to get several SLCs and ensure we get correct\n    # orbit files\n    slc_ids = _asf_query(point, start, end, buffer_degrees=buffer_deg)\n    if not slc_ids:\n        raise ValueError('No results found for input lon/lat and datetime')\n\n    return slc_ids\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns","title":"<code>utilFcns</code>","text":"<p>Geodesy-related utility functions.</p>"},{"location":"reference/#RAiDER.utilFcns.calcgeoh","title":"<code>calcgeoh(lnsp, t, q, z, a, b, R_d, num_levels)</code>","text":"<p>Calculate pressure, geopotential, and geopotential height from the surface pressure and model levels provided by a weather model. The model levels are numbered from the highest eleveation to the lowest. Args:</p> <pre><code>lnsp: ndarray         - [y, x] array of log surface pressure\nt: ndarray            - [z, y, x] cube of temperatures\nq: ndarray            - [z, y, x] cube of specific humidity\ngeopotential: ndarray - [z, y, x] cube of geopotential values\na: ndarray            - [z] vector of a values\nb: ndarray            - [z] vector of b values\nnum_levels: int       - integer number of model levels\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.calcgeoh--returns","title":"Returns:","text":"<pre><code>geopotential - The geopotential in units of height times acceleration\npressurelvs  - The pressure at each of the model levels for each of\n               the input points\ngeoheight    - The geopotential heights\n</code></pre> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def calcgeoh(lnsp, t, q, z, a, b, R_d, num_levels):\n'''\n    Calculate pressure, geopotential, and geopotential height\n    from the surface pressure and model levels provided by a weather model.\n    The model levels are numbered from the highest eleveation to the lowest.\n    Args:\n    ----------\n        lnsp: ndarray         - [y, x] array of log surface pressure\n        t: ndarray            - [z, y, x] cube of temperatures\n        q: ndarray            - [z, y, x] cube of specific humidity\n        geopotential: ndarray - [z, y, x] cube of geopotential values\n        a: ndarray            - [z] vector of a values\n        b: ndarray            - [z] vector of b values\n        num_levels: int       - integer number of model levels\n    Returns:\n    -------\n        geopotential - The geopotential in units of height times acceleration\n        pressurelvs  - The pressure at each of the model levels for each of\n                       the input points\n        geoheight    - The geopotential heights\n    '''\n    geopotential = np.zeros_like(t)\n    pressurelvs = np.zeros_like(geopotential)\n    geoheight = np.zeros_like(geopotential)\n\n    # log surface pressure\n    # Note that we integrate from the ground up, so from the largest model level to 0\n    sp = np.exp(lnsp)\n\n    if len(a) != num_levels + 1 or len(b) != num_levels + 1:\n        raise ValueError(\n            'I have here a model with {} levels, but parameters a '.format(num_levels) +\n            'and b have lengths {} and {} respectively. Of '.format(len(a), len(b)) +\n            'course, these three numbers should be equal.')\n\n    # Integrate up into the atmosphere from *lowest level*\n    z_h = 0  # initial value\n    for lev, t_level, q_level in zip(\n            range(num_levels, 0, -1), t[::-1], q[::-1]):\n\n        # lev is the level number 1-60, we need a corresponding index\n        # into ts and qs\n        # ilevel = num_levels - lev # &lt;&lt; this was Ray's original, but is a typo\n        # because indexing like that results in pressure and height arrays that\n        # are in the opposite orientation to the t/q arrays.\n        ilevel = lev - 1\n\n        # compute moist temperature\n        t_level = t_level * (1 + 0.609133 * q_level)\n\n        # compute the pressures (on half-levels)\n        Ph_lev = a[lev - 1] + (b[lev - 1] * sp)\n        Ph_levplusone = a[lev] + (b[lev] * sp)\n\n        pressurelvs[ilevel] = Ph_lev  # + Ph_levplusone) / 2  # average pressure at half-levels above and below\n\n        if lev == 1:\n            dlogP = np.log(Ph_levplusone / 0.1)\n            alpha = np.log(2)\n        else:\n            dlogP = np.log(Ph_levplusone) - np.log(Ph_lev)\n            alpha = 1 - ((Ph_lev / (Ph_levplusone - Ph_lev)) * dlogP)\n\n        TRd = t_level * R_d\n\n        # z_f is the geopotential of this full level\n        # integrate from previous (lower) half-level z_h to the full level\n        z_f = z_h + TRd * alpha + z\n\n        # Geopotential (add in surface geopotential)\n        geopotential[ilevel] = z_f\n        geoheight[ilevel] = geopotential[ilevel] / g0\n\n        # z_h is the geopotential of 'half-levels'\n        # integrate z_h to next half level\n        z_h += TRd * dlogP\n\n    return geopotential, pressurelvs, geoheight\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.checkShapes","title":"<code>checkShapes(los, lats, lons, hts)</code>","text":"<p>Make sure that by the time the code reaches here, we have a consistent set of line-of-sight and position data.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def checkShapes(los, lats, lons, hts):\n'''\n    Make sure that by the time the code reaches here, we have a\n    consistent set of line-of-sight and position data.\n    '''\n    from RAiDER.losreader import Zenith\n    test1 = hts.shape == lats.shape == lons.shape\n    try:\n        test2 = los.shape[:-1] == hts.shape\n    except AttributeError:\n        test2 = los is Zenith\n\n    if not test1 and test2:\n        raise ValueError(\n            'I need lats, lons, heights, and los to all be the same shape. ' +\n            'lats had shape {}, lons had shape {}, '.format(lats.shape, lons.shape) +\n            'heights had shape {}, and los was not Zenith'.format(hts.shape))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.clip_bbox","title":"<code>clip_bbox(bbox, spacing)</code>","text":"<p>Clip box to multiple of spacing</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def clip_bbox(bbox, spacing):\n\"\"\"\n    Clip box to multiple of spacing\n    \"\"\"\n    return [np.floor(bbox[0] / spacing) * spacing,\n            np.ceil(bbox[1] / spacing) * spacing,\n            np.floor(bbox[2] / spacing) * spacing,\n            np.ceil(bbox[3] / spacing) * spacing]\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.convertLons","title":"<code>convertLons(inLons)</code>","text":"<p>Convert lons from 0-360 to -180-180</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def convertLons(inLons):\n'''Convert lons from 0-360 to -180-180'''\n    mask = inLons &gt; 180\n    outLons = inLons\n    outLons[mask] = outLons[mask] - 360\n    return outLons\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.cosd","title":"<code>cosd(x)</code>","text":"<p>Return the cosine of x when x is in degrees.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def cosd(x):\n\"\"\"Return the cosine of x when x is in degrees.\"\"\"\n    return np.cos(np.radians(x))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.ecef2enu","title":"<code>ecef2enu(xyz, lat, lon, height)</code>","text":"<p>Convert ECEF xyz to ENU</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def ecef2enu(xyz, lat, lon, height):\n'''Convert ECEF xyz to ENU'''\n    x, y, z = xyz[..., 0], xyz[..., 1], xyz[..., 2]\n\n    t = cosd(lon) * x + sind(lon) * y\n\n    e = -sind(lon) * x + cosd(lon) * y\n    n = -sind(lat) * t + cosd(lat) * z\n    u = cosd(lat) * t + sind(lat) * z\n    return np.stack((e, n, u), axis=-1)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.enu2ecef","title":"<code>enu2ecef(east, north, up, lat0, lon0, h0)</code>","text":""},{"location":"reference/#RAiDER.utilFcns.enu2ecef--args","title":"Args:","text":"<p>e1 : float     target east ENU coordinate (meters) n1 : float     target north ENU coordinate (meters) u1 : float     target up ENU coordinate (meters) Results</p> <p>u : float v : float w : float</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def enu2ecef(\n    east: ndarray,\n    north: ndarray,\n    up: ndarray,\n    lat0: ndarray,\n    lon0: ndarray,\n    h0: ndarray,\n):\n\"\"\"\n    Args:\n    ----------\n    e1 : float\n        target east ENU coordinate (meters)\n    n1 : float\n        target north ENU coordinate (meters)\n    u1 : float\n        target up ENU coordinate (meters)\n    Results\n    -------\n    u : float\n    v : float\n    w : float\n    \"\"\"\n    t = cosd(lat0) * up - sind(lat0) * north\n    w = sind(lat0) * up + cosd(lat0) * north\n\n    u = cosd(lon0) * t - sind(lon0) * east\n    v = sind(lon0) * t + cosd(lon0) * east\n\n    return np.stack((u, v, w), axis=-1)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.floorish","title":"<code>floorish(val, frac)</code>","text":"<p>Round a value to the lower fractional part</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def floorish(val, frac):\n'''Round a value to the lower fractional part'''\n    return val - (val % frac)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.geo_to_ht","title":"<code>geo_to_ht(lats, hts)</code>","text":"<p>Convert geopotential height to ellipsoidal heights referenced to WGS84.</p> <p>Note that this formula technically computes height above geoid (geometric height) but the geoid is actually a perfect sphere; Thus returned heights are above a reference ellipsoid, which most assume to be a sphere (e.g., ECMWF - see https://confluence.ecmwf.int/display/CKB/ERA5%3A+compute+pressure+and+geopotential+on+model+levels%2C+geopotential+height+and+geometric+height#ERA5:computepressureandgeopotentialonmodellevels,geopotentialheightandgeometricheight-Geopotentialheight - \"Geometric Height\" and also https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Earthmodel). However, by calculating the ellipsoid here we directly reference to WGS84.</p> <p>Compare to MetPy: (https://unidata.github.io/MetPy/latest/api/generated/metpy.calc.geopotential_to_height.html)</p>"},{"location":"reference/#RAiDER.utilFcns.geo_to_ht--h-geopotential-re-g0-re-geopotential","title":"h = (geopotential * Re) / (g0 * Re - geopotential)","text":""},{"location":"reference/#RAiDER.utilFcns.geo_to_ht--assumes-a-sphere-instead-of-an-ellipsoid","title":"Assumes a sphere instead of an ellipsoid","text":"<p>Returns:</p> Name Type Description <code>ndarray</code> <p>geometric heights. These are approximate ellipsoidal heights referenced to WGS84</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def geo_to_ht(lats, hts):\n\"\"\"\n    Convert geopotential height to ellipsoidal heights referenced to WGS84.\n\n    Note that this formula technically computes height above geoid (geometric height)\n    but the geoid is actually a perfect sphere;\n    Thus returned heights are above a reference ellipsoid, which most assume to be\n    a sphere (e.g., ECMWF - see https://confluence.ecmwf.int/display/CKB/ERA5%3A+compute+pressure+and+geopotential+on+model+levels%2C+geopotential+height+and+geometric+height#ERA5:computepressureandgeopotentialonmodellevels,geopotentialheightandgeometricheight-Geopotentialheight\n    - \"Geometric Height\" and also https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Earthmodel).\n    However, by calculating the ellipsoid here we directly reference to WGS84.\n\n    Compare to MetPy:\n    (https://unidata.github.io/MetPy/latest/api/generated/metpy.calc.geopotential_to_height.html)\n    # h = (geopotential * Re) / (g0 * Re - geopotential)\n    # Assumes a sphere instead of an ellipsoid\n\n    Args:\n        lats    - latitude of points of interest\n        hts     - geopotential height at points of interest\n\n    Returns:\n        ndarray: geometric heights. These are approximate ellipsoidal heights referenced to WGS84\n    \"\"\"\n    g_ll = _get_g_ll(lats) # gravity function of latitude\n    Re = get_Re(lats) # Earth radius function of latitude\n\n    # Calculate Geometric Height, h\n    h = (hts * Re) / (g_ll / g0 * Re - hts)\n\n    return h\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.getChunkSize","title":"<code>getChunkSize(in_shape)</code>","text":"<p>Create a reasonable chunk size</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def getChunkSize(in_shape):\n'''Create a reasonable chunk size'''\n    if mp is None:\n        raise ImportError('RAiDER.utilFcns: getChunkSize - multiprocessing is not available')\n    minChunkSize = 100\n    maxChunkSize = 1000\n    cpu_count = mp.cpu_count()\n    chunkSize = tuple(\n        max(\n            min(maxChunkSize, s // cpu_count),\n            min(s, minChunkSize)\n        ) for s in in_shape\n    )\n    return chunkSize\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.getTimeFromFile","title":"<code>getTimeFromFile(filename)</code>","text":"<p>Parse a filename to get a date-time</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def getTimeFromFile(filename):\n'''\n    Parse a filename to get a date-time\n    '''\n    fmt = '%Y_%m_%d_T%H_%M_%S'\n    p = re.compile(r'\\d{4}_\\d{2}_\\d{2}_T\\d{2}_\\d{2}_\\d{2}')\n    try:\n        out = p.search(filename).group()\n        return datetime.strptime(out, fmt)\n    except BaseException:  # TODO: Which error(s)?\n        raise RuntimeError('The filename for {} does not include a datetime in the correct format'.format(filename))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.get_Re","title":"<code>get_Re(lats)</code>","text":"<p>Returns earth radius as a function of latitude for WGS84</p> <p>Returns:</p> Type Description <p>ndarray of earth radius at each latitude</p> <p>Example:</p> <p>import numpy as np from RAiDER.utilFcns import get_Re output = get_Re(np.array([0, 30, 45, 60, 90])) output  array([6378137., 6372770.5219805, 6367417.56705189, 6362078.07851428, 6356752.]) assert output[0] == 6378137 # (Rmax) assert output[-1] == 6356752 # (Rmin)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def get_Re(lats):\n'''\n    Returns earth radius as a function of latitude for WGS84\n\n    Args:\n        lats    - ndarray of geodetic latitudes in degrees\n\n    Returns:\n        ndarray of earth radius at each latitude\n\n    Example:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from RAiDER.utilFcns import get_Re\n    &gt;&gt;&gt; output = get_Re(np.array([0, 30, 45, 60, 90]))\n    &gt;&gt;&gt; output\n     array([6378137., 6372770.5219805, 6367417.56705189, 6362078.07851428, 6356752.])\n    &gt;&gt;&gt; assert output[0] == 6378137 # (Rmax)\n    &gt;&gt;&gt; assert output[-1] == 6356752 # (Rmin)\n    '''\n    return np.sqrt(1 / (((cosd(lats)**2) / Rmax**2) + ((sind(lats)**2) / Rmin**2)))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.get_dt","title":"<code>get_dt(t1, t2)</code>","text":"<p>Helper function for getting the absolute difference in seconds between two python datetimes</p> <p>Returns:</p> Type Description <p>Absolute difference in seconds between the two inputs</p> <p>Examples:</p> <p>import datetime from RAiDER.utilFcns import get_dt get_dt(datetime.datetime(2020,1,1,5,0,0), datetime.datetime(2020,1,1,0,0,0))  18000.0</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def get_dt(t1,t2):\n'''\n    Helper function for getting the absolute difference in seconds between\n    two python datetimes\n\n    Args:\n        t1, t2  - Python datetimes\n\n    Returns:\n        Absolute difference in seconds between the two inputs\n\n    Examples:\n    &gt;&gt;&gt; import datetime\n    &gt;&gt;&gt; from RAiDER.utilFcns import get_dt\n    &gt;&gt;&gt; get_dt(datetime.datetime(2020,1,1,5,0,0), datetime.datetime(2020,1,1,0,0,0))\n     18000.0\n    '''\n    return np.abs((t1 - t2).total_seconds())\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.get_file_and_band","title":"<code>get_file_and_band(filestr)</code>","text":"<p>Support file;bandnum as input for filename strings</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def get_file_and_band(filestr):\n\"\"\"\n    Support file;bandnum as input for filename strings\n    \"\"\"\n    parts = filestr.split(\";\")\n\n    # Defaults to first band if no bandnum is provided\n    if len(parts) == 1:\n        return filestr.strip(), 1\n    elif len(parts) == 2:\n        return parts[0].strip(), int(parts[1].strip())\n    else:\n        raise ValueError(\n            f\"Cannot interpret {filestr} as valid filename\"\n        )\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.get_nearest_wmtimes","title":"<code>get_nearest_wmtimes(t0, time_delta)</code>","text":"<p>\" Get the nearest two available times to the requested time given a time step</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>list of datetimes representing the one or two closest</p> <p>available times to the requested time</p> <p>Example:</p> <p>import datetime from RAiDER.utilFcns import get_nearest_wmtimes t0 = datetime.datetime(2020,1,1,11,35,0) get_nearest_wmtimes(t0, 3)  (datetime.datetime(2020, 1, 1, 9, 0), datetime.datetime(2020, 1, 1, 12, 0))</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def get_nearest_wmtimes(t0, time_delta):\n\"\"\"\"\n    Get the nearest two available times to the requested time given a time step\n\n    Args:\n        t0         - user-requested Python datetime\n        time_delta  - time interval of weather model\n\n    Returns:\n        tuple: list of datetimes representing the one or two closest\n        available times to the requested time\n\n    Example:\n    &gt;&gt;&gt; import datetime\n    &gt;&gt;&gt; from RAiDER.utilFcns import get_nearest_wmtimes\n    &gt;&gt;&gt; t0 = datetime.datetime(2020,1,1,11,35,0)\n    &gt;&gt;&gt; get_nearest_wmtimes(t0, 3)\n     (datetime.datetime(2020, 1, 1, 9, 0), datetime.datetime(2020, 1, 1, 12, 0))\n    \"\"\"\n    # get the closest time available\n    tclose = round_time(t0, roundTo = time_delta * 60 *60)\n\n    # Just calculate both options and take the closest\n    t2_1 = tclose + timedelta(hours=time_delta)\n    t2_2 = tclose - timedelta(hours=time_delta)\n    t2 = [t2_1 if get_dt(t2_1, t0) &lt; get_dt(t2_2, t0) else t2_2][0]\n\n    # If you're within 5 minutes just take the closest time\n    if get_dt(tclose, t0) &lt; _THRESHOLD_SECONDS:\n        return [tclose]\n    else:\n        if t2 &gt; tclose:\n            return [tclose, t2]\n        else:\n            return [t2, tclose]\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.nodataToNan","title":"<code>nodataToNan(inarr, listofvals)</code>","text":"<p>Setting values to nan as needed</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def nodataToNan(inarr, listofvals):\n\"\"\"\n    Setting values to nan as needed\n    \"\"\"\n    inarr = inarr.astype(float) # nans cannot be integers (i.e. in DEM)\n    for val in listofvals:\n        if val is not None:\n            inarr[inarr == val] = np.nan\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.padLower","title":"<code>padLower(invar)</code>","text":"<p>add a layer of data below the lowest current z-level at height zmin</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def padLower(invar):\n'''\n    add a layer of data below the lowest current z-level at height zmin\n    '''\n    new_var = _least_nonzero(invar)\n    return np.concatenate((new_var[:, :, np.newaxis], invar), axis=2)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.projectDelays","title":"<code>projectDelays(delay, inc)</code>","text":"<p>Project zenith delays to LOS</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def projectDelays(delay, inc):\n'''Project zenith delays to LOS'''\n    return delay / cosd(inc)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.requests_retry_session","title":"<code>requests_retry_session(retries=10, session=None)</code>","text":"<p>https://www.peterbe.com/plog/best-practice-with-retries-with-requests</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def requests_retry_session(retries=10, session=None):\n\"\"\" https://www.peterbe.com/plog/best-practice-with-retries-with-requests \"\"\"\n    import requests\n    from requests.adapters import HTTPAdapter\n    from requests.packages.urllib3.util.retry import Retry\n    # add a retry strategy; https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/\n    session = session or requests.Session()\n    retry = Retry(total=retries, read=retries, connect=retries,\n                  backoff_factor=0.3, status_forcelist=list(range(429, 505)))\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.rio_extents","title":"<code>rio_extents(profile)</code>","text":"<p>Get a bounding box in SNWE from a rasterio profile</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def rio_extents(profile):\n\"\"\" Get a bounding box in SNWE from a rasterio profile \"\"\"\n    gt = profile[\"transform\"].to_gdal()\n    xSize = profile[\"width\"]\n    ySize = profile[\"height\"]\n\n    if profile[\"crs\"] is None or not gt:\n        raise AttributeError('Profile does not contain geotransform information')\n    W, E = gt[0], gt[0] + (xSize - 1) * gt[1] + (ySize - 1) * gt[2]\n    N, S = gt[3], gt[3] + (xSize - 1) * gt[4] + (ySize - 1) * gt[5]\n    return S, N, W, E\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.rio_open","title":"<code>rio_open(fname, returnProj=False, userNDV=None, band=None)</code>","text":"<p>Reads a rasterio-compatible raster file and returns the data and profile</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def rio_open(fname, returnProj=False, userNDV=None, band=None):\n'''\n    Reads a rasterio-compatible raster file and returns the data and profile\n    '''\n    if rasterio is None:\n        raise ImportError('RAiDER.utilFcns: rio_open - rasterio is not installed')\n\n    if os.path.exists(fname + '.vrt'):\n        fname = fname + '.vrt'\n\n    with rasterio.open(fname) as src:\n        profile = src.profile\n\n        # For all bands\n        nodata = src.nodatavals\n\n        # If user requests a band\n        if band is not None:\n            ndv = nodata[band - 1]\n            data = src.read(band).squeeze()\n            nodataToNan(data, [userNDV, nodata[band - 1]])\n\n        else:\n            data = src.read().squeeze()\n            if data.ndim &gt; 2:\n                for bnd in range(data.shape[0]):\n                    val = data[bnd, ...]\n                    nodataToNan(val, [userNDV, nodata[bnd]])\n            else:\n                nodataToNan(data, list(nodata) + [userNDV])\n\n\n        if data.ndim &gt; 2:\n            dlist = []\n            for k in range(data.shape[0]):\n                dlist.append(data[k,...].copy())\n            data = dlist\n\n    if not returnProj:\n        return data\n\n    else:\n        return data, profile\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.rio_profile","title":"<code>rio_profile(fname)</code>","text":"<p>Reads the profile of a rasterio file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def rio_profile(fname):\n'''\n    Reads the profile of a rasterio file\n    '''\n    if rasterio is None:\n        raise ImportError('RAiDER.utilFcns: rio_profile - rasterio is not installed')\n\n    ## need to access subdataset directly\n    if os.path.basename(fname).startswith('S1-GUNW'):\n        fname = os.path.join(f'NETCDF:\"{fname}\":science/grids/data/unwrappedPhase')\n        with rasterio.open(fname) as src:\n            profile = src.profile\n\n    elif os.path.exists(fname + '.vrt'):\n        fname = fname + '.vrt'\n\n    with rasterio.open(fname) as src:\n        profile = src.profile\n        # if 'S1-GUNW' in fname:\n            # profile['length'] = profile['width']\n            # profile['width']  = profile['height']\n\n    if profile[\"crs\"] is None:\n        raise AttributeError(\n            f\"{fname} does not contain geotransform information\"\n        )\n    return profile\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.rio_stats","title":"<code>rio_stats(fname, band=1)</code>","text":"<p>Read a rasterio-compatible file and pull the metadata.</p> <p>Returns:</p> Type Description <p>stats   - a list of stats for the specified band</p> <p>proj    - CRS/projection information for the file</p> <p>gt      - geotransform for the data</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def rio_stats(fname, band=1):\n'''\n    Read a rasterio-compatible file and pull the metadata.\n\n    Args:\n        fname   - filename to be loaded\n        band    - band number to use for getting statistics\n\n    Returns:\n        stats   - a list of stats for the specified band\n        proj    - CRS/projection information for the file\n        gt      - geotransform for the data\n    '''\n    if rasterio is None:\n        raise ImportError('RAiDER.utilFcns: rio_stats - rasterio is not installed')\n\n    if os.path.basename(fname).startswith('S1-GUNW'):\n        fname = os.path.join(f'NETCDF:\"{fname}\":science/grids/data/unwrappedPhase')\n\n    if os.path.exists(fname + '.vrt'):\n        fname = fname + '.vrt'\n\n    # Turn off PAM to avoid creating .aux.xml files\n    with rasterio.Env(GDAL_PAM_ENABLED=\"NO\"):\n        with rasterio.open(fname) as src:\n            gt    = src.transform.to_gdal()\n            proj  = src.crs\n            stats = src.statistics(band)\n\n    return stats, proj, gt\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.robmax","title":"<code>robmax(a)</code>","text":"<p>Get the minimum of an array, accounting for empty lists</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def robmax(a):\n'''\n    Get the minimum of an array, accounting for empty lists\n    '''\n    try:\n        return np.nanmax(a)\n    except ValueError:\n        return 'N/A'\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.robmin","title":"<code>robmin(a)</code>","text":"<p>Get the minimum of an array, accounting for empty lists</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def robmin(a):\n'''\n    Get the minimum of an array, accounting for empty lists\n    '''\n    try:\n        return np.nanmin(a)\n    except ValueError:\n        return 'N/A'\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.round_time","title":"<code>round_time(dt, roundTo=60)</code>","text":"<p>Round a datetime object to any time lapse in seconds dt: datetime.datetime object roundTo: Closest number of seconds to round to, default 1 minute. Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def round_time(dt, roundTo=60):\n'''\n    Round a datetime object to any time lapse in seconds\n    dt: datetime.datetime object\n    roundTo: Closest number of seconds to round to, default 1 minute.\n    Source: https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object/10854034#10854034\n    '''\n    seconds = (dt.replace(tzinfo=None) - dt.min).seconds\n    rounding = (seconds + roundTo / 2) // roundTo * roundTo\n    return dt + timedelta(0, rounding - seconds, -dt.microsecond)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.show_progress","title":"<code>show_progress(block_num, block_size, total_size)</code>","text":"<p>Show download progress</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def show_progress(block_num, block_size, total_size):\n'''Show download progress'''\n    if progressbar is None:\n        raise ImportError('RAiDER.utilFcns: show_progress - progressbar is not available')\n\n    global pbar\n    if pbar is None:\n        pbar = progressbar.ProgressBar(maxval=total_size)\n        pbar.start()\n\n    downloaded = block_num * block_size\n    if downloaded &lt; total_size:\n        pbar.update(downloaded)\n    else:\n        pbar.finish()\n        pbar = None\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.sind","title":"<code>sind(x)</code>","text":"<p>Return the sine of x when x is in degrees.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def sind(x):\n\"\"\"Return the sine of x when x is in degrees.\"\"\"\n    return np.sin(np.radians(x))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.transform_bbox","title":"<code>transform_bbox(snwe_in, dest_crs=4326, src_crs=4326, margin=100.0)</code>","text":"<p>Transform bbox to lat/lon or another CRS for use with rest of workflow Returns: SNWE</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def transform_bbox(snwe_in, dest_crs=4326, src_crs=4326, margin=100.):\n\"\"\"\n    Transform bbox to lat/lon or another CRS for use with rest of workflow\n    Returns: SNWE\n    \"\"\"\n    # TODO - Handle dateline crossing\n    if isinstance(src_crs, int):\n        src_crs = CRS.from_epsg(src_crs)\n    elif isinstance(src_crs, str):\n        src_crs = CRS(src_crs)\n\n    # Handle margin for input bbox in degrees\n    if src_crs.axis_info[0].unit_name == \"degree\":\n        margin = margin / 1.0e5\n\n    if isinstance(dest_crs, int):\n        dest_crs = CRS.from_epsg(dest_crs)\n    elif isinstance(dest_crs, str):\n        dest_crs = CRS(dest_crs)\n\n    # If dest_crs is same as src_crs\n    if dest_crs == src_crs:\n        return snwe_in\n\n    T = Transformer.from_crs(src_crs, dest_crs, always_xy=True)\n    xs = np.linspace(snwe_in[2]-margin, snwe_in[3]+margin, num=11)\n    ys = np.linspace(snwe_in[0]-margin, snwe_in[1]+margin, num=11)\n    X, Y = np.meshgrid(xs, ys)\n\n    # Transform to lat/lon\n    xx, yy = T.transform(X, Y)\n\n    # query_area convention\n    snwe = [np.nanmin(yy), np.nanmax(yy),\n            np.nanmin(xx), np.nanmax(xx)]\n    return snwe\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.transform_coords","title":"<code>transform_coords(proj1, proj2, x, y)</code>","text":"<p>Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj). e.g. x, y = transform_coords(4326, 4087, lon, lat)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def transform_coords(proj1, proj2, x, y):\n\"\"\"\n    Transform coordinates from proj1 to proj2 (can be EPSG or crs from proj).\n    e.g. x, y = transform_coords(4326, 4087, lon, lat)\n    \"\"\"\n    transformer = Transformer.from_crs(proj1, proj2, always_xy=True)\n    return transformer.transform(x, y)\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.write2NETCDF4core","title":"<code>write2NETCDF4core(nc_outfile, dimension_dict, dataset_dict, tran, mapping_name='WGS84')</code>","text":"<p>The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file that can be accessed by external programs.</p> <p>The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def write2NETCDF4core(nc_outfile, dimension_dict, dataset_dict, tran, mapping_name='WGS84'):\n'''\n    The abstract/modular netcdf writer that can be called by a wrapper function to write data to a NETCDF4 file\n    that can be accessed by external programs.\n\n    The point of doing this is to alleviate some of the memory load of keeping\n    the full model in memory and make it easier to scale up the program.\n    '''\n    from osgeo import osr\n\n    if mapping_name == 'WGS84':\n        epsg = 4326\n        crs = CRS.from_epsg(epsg)\n\n        grid_mapping = 'WGS84'  # need to set this as an attribute for the image variables\n    else:\n        crs = CRS.from_wkt(mapping_name)\n        grid_mapping = 'CRS'\n\n    datatype = np.dtype('S1')\n    dimensions = ()\n    var = nc_outfile.createVariable(\n        grid_mapping,\n        datatype,\n        dimensions,\n        fill_value=None\n    )\n\n    # variable made, now add attributes\n    for k, v in crs.to_cf().items():\n        var.setncattr(k, v)\n\n    var.setncattr('GeoTransform', ' '.join(str(x) for x in tran))  # note this has pixel size in it - set  explicitly above\n\n    for dim in dimension_dict:\n        nc_outfile.createDimension(dim, dimension_dict[dim]['length'])\n        varname = dimension_dict[dim]['varname']\n        datatype = dimension_dict[dim]['datatype']\n        dimensions = dimension_dict[dim]['dimensions']\n        FillValue = dimension_dict[dim]['FillValue']\n        var = nc_outfile.createVariable(varname, datatype, dimensions, fill_value=FillValue)\n        var.setncattr('standard_name', dimension_dict[dim]['standard_name'])\n        var.setncattr('description', dimension_dict[dim]['description'])\n        var.setncattr('units', dimension_dict[dim]['units'])\n        var[:] = dimension_dict[dim]['dataset'].astype(dimension_dict[dim]['datatype'])\n\n    for data in dataset_dict:\n        varname = dataset_dict[data]['varname']\n        datatype = dataset_dict[data]['datatype']\n        dimensions = dataset_dict[data]['dimensions']\n        FillValue = dataset_dict[data]['FillValue']\n        ChunkSize = dataset_dict[data]['ChunkSize']\n        var = nc_outfile.createVariable(\n            varname,\n            datatype,\n            dimensions,\n            fill_value=FillValue,\n            zlib=True,\n            complevel=2,\n            shuffle=True,\n            chunksizes=ChunkSize\n        )\n        # Override with correct name here\n        var.setncattr('grid_mapping', grid_mapping) # dataset_dict[data]['grid_mapping'])\n        var.setncattr('standard_name', dataset_dict[data]['standard_name'])\n        var.setncattr('description', dataset_dict[data]['description'])\n        if 'units' in dataset_dict[data]:\n            var.setncattr('units', dataset_dict[data]['units'])\n\n        ndmask = np.isnan(dataset_dict[data]['dataset'])\n        dataset_dict[data]['dataset'][ndmask] = FillValue\n\n        var[:] = dataset_dict[data]['dataset'].astype(datatype)\n\n    return nc_outfile\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.writeArrayToFile","title":"<code>writeArrayToFile(lats, lons, array, filename, noDataValue=-9999)</code>","text":"<p>Write a single-dim array of values to a file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def writeArrayToFile(lats, lons, array, filename, noDataValue=-9999):\n'''\n    Write a single-dim array of values to a file\n    '''\n    array[np.isnan(array)] = noDataValue\n    with open(filename, 'w') as f:\n        f.write('Lat,Lon,Hgt_m\\n')\n        for lat, lon, height in zip(lats, lons, array):\n            f.write('{},{},{}\\n'.format(lat, lon, height))\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.writeArrayToRaster","title":"<code>writeArrayToRaster(array, filename, noDataValue=0.0, fmt='ENVI', proj=None, gt=None)</code>","text":"<p>write a numpy array to a GDAL-readable raster</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def writeArrayToRaster(array, filename, noDataValue=0., fmt='ENVI', proj=None, gt=None):\n'''\n    write a numpy array to a GDAL-readable raster\n    '''\n    array_shp = np.shape(array)\n    if array.ndim != 2:\n        raise RuntimeError('writeArrayToRaster: cannot write an array of shape {} to a raster image'.format(array_shp))\n\n    # Data type\n    if \"complex\" in str(array.dtype):\n        dtype = np.complex64\n    elif \"float\" in str(array.dtype):\n        dtype = np.float32\n    else:\n        dtype = np.uint8\n\n    # Geotransform\n    trans = None\n    if gt is not None:\n        trans = rasterio.Affine.from_gdal(*gt)\n\n    ## cant write netcdfs with rasterio in a simple way\n    if fmt == 'nc':\n        fmt = 'GTiff'\n        filename = filename.replace('.nc', '.GTiff')\n\n    with rasterio.open(filename, mode=\"w\", count=1,\n                       width=array_shp[1], height=array_shp[0],\n                       dtype=dtype, crs=proj, nodata=noDataValue,\n                       driver=fmt, transform=trans) as dst:\n        dst.write(array, 1)\n    logger.info('Wrote: %s', filename)\n    return\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.writeDelays","title":"<code>writeDelays(aoi, wetDelay, hydroDelay, wetFilename, hydroFilename=None, outformat=None, ndv=0.0)</code>","text":"<p>Write the delay numpy arrays to files in the format specified</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def writeDelays(aoi, wetDelay, hydroDelay,\n                wetFilename, hydroFilename=None,\n                outformat=None, ndv=0.):\n\"\"\" Write the delay numpy arrays to files in the format specified \"\"\"\n    if pd is None:\n        raise ImportError('pandas is required to write GNSS delays to a file')\n\n    # Need to consistently handle noDataValues\n    wetDelay[np.isnan(wetDelay)] = ndv\n    hydroDelay[np.isnan(hydroDelay)] = ndv\n\n    # Do different things, depending on the type of input\n    if aoi.type() == 'station_file':\n        #TODO: why is this a try/except?\n        try:\n            df = pd.read_csv(aoi._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n        except ValueError:\n            df = pd.read_csv(aoi._filename).drop_duplicates(subset=[\"Lat\", \"Lon\"])\n\n        df['wetDelay'] = wetDelay\n        df['hydroDelay'] = hydroDelay\n        df['totalDelay'] = wetDelay + hydroDelay\n        df.to_csv(wetFilename, index=False)\n        logger.info('Wrote delays to: %s', wetFilename)\n\n    else:\n        proj = aoi.projection()\n        gt   = aoi.geotransform()\n        writeArrayToRaster(\n            wetDelay,\n            wetFilename,\n            noDataValue=ndv,\n            fmt=outformat,\n            proj=proj,\n            gt=gt\n        )\n        writeArrayToRaster(\n            hydroDelay,\n            hydroFilename,\n            noDataValue=ndv,\n            fmt=outformat,\n            proj=proj,\n            gt=gt\n        )\n</code></pre>"},{"location":"reference/#RAiDER.utilFcns.writeWeatherVars2NETCDF4","title":"<code>writeWeatherVars2NETCDF4(self, lat, lon, h, q, p, t, outName=None, NoDataValue=None, chunk=(1, 91, 144), mapping_name='WGS84')</code>","text":"<p>By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file that can be accessed by external programs.</p> <p>The point of doing this is to alleviate some of the memory load of keeping the full model in memory and make it easier to scale up the program.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/utilFcns.py</code> <pre><code>def writeWeatherVars2NETCDF4(self, lat, lon, h, q, p, t, outName=None, NoDataValue=None, chunk=(1, 91, 144), mapping_name='WGS84'):\n'''\n    By calling the abstract/modular netcdf writer (RAiDER.utilFcns.write2NETCDF4core), write the OpenDAP/PyDAP-retrieved weather model data (GMAO and MERRA-2) to a NETCDF4 file\n    that can be accessed by external programs.\n\n    The point of doing this is to alleviate some of the memory load of keeping\n    the full model in memory and make it easier to scale up the program.\n    '''\n\n    import netCDF4\n\n    if outName is None:\n        outName = os.path.join(\n            os.getcwd() + '/weather_files',\n            self._Name + datetime.strftime(\n                self._time, '_%Y_%m_%d_T%H_%M_%S'\n            ) + '.nc'\n        )\n\n    if NoDataValue is None:\n        NoDataValue = -9999.\n\n    self._time = getTimeFromFile(outName)\n\n    dimidZ, dimidY, dimidX = t.shape\n    chunk_lines_Y = np.min([chunk[1], dimidY])\n    chunk_lines_X = np.min([chunk[2], dimidX])\n    ChunkSize = [1, chunk_lines_Y, chunk_lines_X]\n\n    nc_outfile = netCDF4.Dataset(outName, 'w', clobber=True, format='NETCDF4')\n    nc_outfile.setncattr('Conventions', 'CF-1.6')\n    nc_outfile.setncattr('datetime', datetime.strftime(self._time, \"%Y_%m_%dT%H_%M_%S\"))\n    nc_outfile.setncattr('date_created', datetime.now().strftime(\"%Y_%m_%dT%H_%M_%S\"))\n    title = self._Name + ' weather model data'\n    nc_outfile.setncattr('title', title)\n\n    tran = [lon[0], lon[1] - lon[0], 0.0, lat[0], 0.0, lat[1] - lat[0]]\n\n    dimension_dict = {\n        'x': {'varname': 'x',\n              'datatype': np.dtype('float64'),\n              'dimensions': ('x'),\n              'length': dimidX,\n              'FillValue': None,\n              'standard_name': 'longitude',\n              'description': 'longitude',\n              'dataset': lon,\n              'units': 'degrees_east'},\n        'y': {'varname': 'y',\n              'datatype': np.dtype('float64'),\n              'dimensions': ('y'),\n              'length': dimidY,\n              'FillValue': None,\n              'standard_name': 'latitude',\n              'description': 'latitude',\n              'dataset': lat,\n              'units': 'degrees_north'},\n        'z': {'varname': 'z',\n              'datatype': np.dtype('float32'),\n              'dimensions': ('z'),\n              'length': dimidZ,\n              'FillValue': None,\n              'standard_name': 'model_layers',\n              'description': 'model layers',\n              'dataset': np.arange(dimidZ),\n              'units': 'layer'}\n    }\n\n    dataset_dict = {\n        'h': {'varname': 'H',\n              'datatype': np.dtype('float32'),\n              'dimensions': ('z', 'y', 'x'),\n              'grid_mapping': mapping_name,\n              'FillValue': NoDataValue,\n              'ChunkSize': ChunkSize,\n              'standard_name': 'mid_layer_heights',\n              'description': 'mid layer heights',\n              'dataset': h,\n              'units': 'm'},\n        'q': {'varname': 'QV',\n              'datatype': np.dtype('float32'),\n              'dimensions': ('z', 'y', 'x'),\n              'grid_mapping': mapping_name,\n              'FillValue': NoDataValue,\n              'ChunkSize': ChunkSize,\n              'standard_name': 'specific_humidity',\n              'description': 'specific humidity',\n              'dataset': q,\n              'units': 'kg kg-1'},\n        'p': {'varname': 'PL',\n              'datatype': np.dtype('float32'),\n              'dimensions': ('z', 'y', 'x'),\n              'grid_mapping': mapping_name,\n              'FillValue': NoDataValue,\n              'ChunkSize': ChunkSize,\n              'standard_name': 'mid_level_pressure',\n              'description': 'mid level pressure',\n              'dataset': p,\n              'units': 'Pa'},\n        't': {'varname': 'T',\n              'datatype': np.dtype('float32'),\n              'dimensions': ('z', 'y', 'x'),\n              'grid_mapping': mapping_name,\n              'FillValue': NoDataValue,\n              'ChunkSize': ChunkSize,\n              'standard_name': 'air_temperature',\n              'description': 'air temperature',\n              'dataset': t,\n              'units': 'K'}\n    }\n\n    nc_outfile = write2NETCDF4core(nc_outfile, dimension_dict, dataset_dict, tran, mapping_name='WGS84')\n\n    nc_outfile.sync()  # flush data to disk\n    nc_outfile.close()\n</code></pre>"},{"location":"reference_td/","title":"Delay Calculation","text":"<p><code>tropo_delay</code></p> <p>RAiDER tropospheric delay calculation</p> <p>This module provides the main RAiDER functionality for calculating tropospheric wet and hydrostatic delays from a weather model. Weather models are accessed as NETCDF files and should have \"wet\" \"hydro\" \"wet_total\" and \"hydro_total\" fields specified.</p>"},{"location":"reference_td/#RAiDER.delay.transformPoints","title":"<code>transformPoints(lats, lons, hgts, old_proj, new_proj)</code>","text":"<p>Transform lat/lon/hgt data to an array of points in a new projection</p> <p>Parameters:</p> Name Type Description Default <code>lats</code> <code>ndarray</code> <p>ndarray   - WGS-84 latitude (EPSG: 4326)</p> required <code>lons</code> <code>ndarray</code> <p>ndarray   - ditto for longitude</p> required <code>hgts</code> <code>ndarray</code> <p>ndarray   - Ellipsoidal height in meters</p> required <code>old_proj</code> <code>CRS</code> <p>CRS   - the original projection of the points</p> required <code>new_proj</code> <code>CRS</code> <p>CRS   - the new projection in which to return the points</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <code>ndarray</code> <p>the array of query points in the weather model coordinate system (YX)</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def transformPoints(lats: np.ndarray, lons: np.ndarray, hgts: np.ndarray, old_proj: CRS, new_proj: CRS) -&gt; np.ndarray:\n'''\n    Transform lat/lon/hgt data to an array of points in a new\n    projection\n\n    Args:\n        lats: ndarray   - WGS-84 latitude (EPSG: 4326)\n        lons: ndarray   - ditto for longitude\n        hgts: ndarray   - Ellipsoidal height in meters\n        old_proj: CRS   - the original projection of the points\n        new_proj: CRS   - the new projection in which to return the points\n\n    Returns:\n        ndarray: the array of query points in the weather model coordinate system (YX)\n    '''\n    # Flags for flipping inputs or outputs\n    if not isinstance(new_proj, CRS):\n        new_proj = CRS.from_epsg(new_proj.lstrip('EPSG:'))\n    if not isinstance(old_proj, CRS):\n        old_proj = CRS.from_epsg(old_proj.lstrip('EPSG:'))\n\n    t = Transformer.from_crs(old_proj, new_proj, always_xy=True)\n\n    # in_flip = old_proj.axis_info[0].direction\n    # out_flip = new_proj.axis_info[0].direction\n\n    res  = t.transform(lons, lats, hgts)\n\n    # lat/lon/height\n    return  np.stack([res[1], res[0], res[2]], axis=-1)\n</code></pre>"},{"location":"reference_td/#RAiDER.delay.tropo_delay","title":"<code>tropo_delay(dt, weather_model_file, aoi, los, height_levels=None, out_proj=4326, zref=_ZREF)</code>","text":"<p>Calculate integrated delays on query points. Options are: 1. Zenith delays (ZTD) 2. Zenith delays projected to the line-of-sight (STD-projected) 3. Slant delays integrated along the raypath (STD-raytracing)</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <p>Datetime                - Datetime object for determining when to calculate delays</p> required <code>weather_model_File</code> <p>string  - Name of the NETCDF file containing a pre-processed weather model</p> required <code>aoi</code> <p>AOI object             - AOI object</p> required <code>los</code> <p>LOS object             - LOS object</p> required <code>height_levels</code> <code>List[float]</code> <p>list         - (optional) list of height levels on which to calculate delays. Only needed for cube generation.</p> <code>None</code> <code>out_proj</code> <code>Union[int, str]</code> <p>int,str           - (optional) EPSG code for output projection</p> <code>4326</code> <code>zref</code> <code>Union[int, float]</code> <p>int,float             - (optional) maximum height to integrate up to during raytracing</p> <code>_ZREF</code> <p>Returns:</p> Type Description <p>xarray Dataset or ndarrays: - wet and hydrostatic delays at the grid nodes / query points.</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def tropo_delay(\n        dt,\n        weather_model_file: str,\n        aoi,\n        los,\n        height_levels: List[float]=None,\n        out_proj: Union[int, str] =4326,\n        zref: Union[int, float]=_ZREF,\n    ):\n\"\"\"\n    Calculate integrated delays on query points. Options are:\n    1. Zenith delays (ZTD)\n    2. Zenith delays projected to the line-of-sight (STD-projected)\n    3. Slant delays integrated along the raypath (STD-raytracing)\n\n    Args:\n        dt: Datetime                - Datetime object for determining when to calculate delays\n        weather_model_File: string  - Name of the NETCDF file containing a pre-processed weather model\n        aoi: AOI object             - AOI object\n        los: LOS object             - LOS object\n        height_levels: list         - (optional) list of height levels on which to calculate delays. Only needed for cube generation.\n        out_proj: int,str           - (optional) EPSG code for output projection\n        zref: int,float             - (optional) maximum height to integrate up to during raytracing\n\n\n    Returns:\n        xarray Dataset *or* ndarrays: - wet and hydrostatic delays at the grid nodes / query points.\n    \"\"\"\n    crs = CRS(out_proj)\n\n    # Load CRS from weather model file\n    with xarray.load_dataset(weather_model_file) as ds:\n        try:\n            wm_proj = CRS.from_wkt(ds['proj'].attrs['crs_wkt'])\n        except KeyError:\n            logger.warning(\"WARNING: I can't find a CRS in the weather model file, so I will assume you are using WGS84\")\n            wm_proj = CRS.from_epsg(4326)\n\n    # get heights\n    with xarray.load_dataset(weather_model_file) as ds:\n        wm_levels = ds.z.values\n        toa       = wm_levels.max() - 1\n\n\n    if height_levels is None:\n        if aoi.type() == 'Geocube':\n            height_levels = aoi.readZ()\n        else:\n            height_levels = wm_levels\n\n    if not zref:\n        zref = toa\n\n    if zref &gt; toa:\n        zref = toa\n        logger.warning('Requested integration height (zref) is higher than top of weather model. Forcing to top ({toa}).')\n\n\n    #TODO: expose this as library function\n    ds = _get_delays_on_cube(dt, weather_model_file, wm_proj, aoi, height_levels,\n            los, crs, zref)\n\n    if (aoi.type() == 'bounding_box') or (aoi.type() == 'Geocube'):\n        return ds, None\n\n    else:\n        # CRS can be an int, str, or CRS object\n        try:\n            out_proj = CRS.from_epsg(out_proj)\n        except pyproj.exceptions.CRSError:\n            out_proj = out_proj\n\n        pnt_proj = CRS.from_epsg(4326)\n        lats, lons = aoi.readLL()\n        hgts = aoi.readZ()\n        pnts = transformPoints(lats, lons, hgts, pnt_proj, out_proj)\n\n        try:\n            ifWet, ifHydro = getInterpolators(ds, \"ztd\")\n        except RuntimeError:\n            logger.exception('Failed to get weather model %s interpolators.', weather_model_file)\n\n        wetDelay = ifWet(pnts)\n        hydroDelay = ifHydro(pnts)\n\n        # return the delays (ZTD or STD)\n        if los.is_Projected():\n            los.setTime(dt)\n            los.setPoints(lats, lons, hgts)\n            wetDelay   = los(wetDelay)\n            hydroDelay = los(hydroDelay)\n\n    return wetDelay, hydroDelay\n</code></pre>"},{"location":"reference_td/#RAiDER.delay.writeResultsToXarray","title":"<code>writeResultsToXarray(dt, xpts, ypts, zpts, crs, wetDelay, hydroDelay, weather_model_file, out_type)</code>","text":"<p>write a 1-D array to a NETCDF5 file</p> Source code in <code>/home/runner/micromamba-root/envs/RAiDER/lib/python3.10/site-packages/RAiDER/delay.py</code> <pre><code>def writeResultsToXarray(dt, xpts, ypts, zpts, crs, wetDelay, hydroDelay, weather_model_file, out_type):\n'''\n    write a 1-D array to a NETCDF5 file\n    '''\n       # Modify this as needed for NISAR / other projects\n    ds = xarray.Dataset(\n        data_vars=dict(\n            wet=([\"z\", \"y\", \"x\"],\n                 wetDelay,\n                 {\"units\" : \"m\",\n                  \"description\": f\"wet {out_type} delay\",\n                  # 'crs': crs.to_epsg(),\n                  \"grid_mapping\": \"crs\",\n\n                 }),\n            hydro=([\"z\", \"y\", \"x\"],\n                   hydroDelay,\n                   {\"units\": \"m\",\n                    # 'crs': crs.to_epsg(),\n                    \"description\": f\"hydrostatic {out_type} delay\",\n                    \"grid_mapping\": \"crs\",\n                   }),\n        ),\n        coords=dict(\n            x=([\"x\"], xpts),\n            y=([\"y\"], ypts),\n            z=([\"z\"], zpts),\n        ),\n        attrs=dict(\n            Conventions=\"CF-1.7\",\n            title=\"RAiDER geo cube\",\n            source=os.path.basename(weather_model_file),\n            history=str(datetime.utcnow()) + \" RAiDER\",\n            description=f\"RAiDER geo cube - {out_type}\",\n            reference_time=str(dt),\n        ),\n    )\n\n    # Write projection system mapping\n    ds[\"crs\"] = int(-2147483647) # dummy placeholder\n    for k, v in crs.to_cf().items():\n        ds.crs.attrs[k] = v\n\n    # Write z-axis information\n    ds.z.attrs[\"axis\"] = \"Z\"\n    ds.z.attrs[\"units\"] = \"m\"\n    ds.z.attrs[\"description\"] = \"height above ellipsoid\"\n\n    # If in degrees\n    if crs.axis_info[0].unit_name == \"degree\":\n        ds.y.attrs[\"units\"] = \"degrees_north\"\n        ds.y.attrs[\"standard_name\"] = \"latitude\"\n        ds.y.attrs[\"long_name\"] = \"latitude\"\n\n        ds.x.attrs[\"units\"] = \"degrees_east\"\n        ds.x.attrs[\"standard_name\"] = \"longitude\"\n        ds.x.attrs[\"long_name\"] = \"longitude\"\n\n    else:\n        ds.y.attrs[\"axis\"] = \"Y\"\n        ds.y.attrs[\"standard_name\"] = \"projection_y_coordinate\"\n        ds.y.attrs[\"long_name\"] = \"y-coordinate in projected coordinate system\"\n        ds.y.attrs[\"units\"] = \"m\"\n\n        ds.x.attrs[\"axis\"] = \"X\"\n        ds.x.attrs[\"standard_name\"] = \"projection_x_coordinate\"\n        ds.x.attrs[\"long_name\"] = \"x-coordinate in projected coordinate system\"\n        ds.x.attrs[\"units\"] = \"m\"\n\n    return ds\n</code></pre>"},{"location":"tutorials/","title":"Overview","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":""},{"location":"tutorials/#raytracing-atmospheric-delay-estimator-for-radar-raider","title":"Raytracing Atmospheric Delay Estimator for RADAR - RAiDER","text":"<p>RAiDER is a package in Python which contains tools to calculate tropospheric corrections for Radar using a raytracing implementation. Its development was funded under the NASA Sea-level Change Team (NSLCT) program, the Earth Surface and Interior (ESI) program, and the NISAR Science Team (NISAR-ST) (NTR-51433). U.S. Government sponsorship acknowledged.</p> <p>Copyright (c) 2019-2022, California Institute of Technology (\"Caltech\"). All rights reserved.</p> <p>THIS IS RESEARCH CODE PROVIDED TO YOU \"AS IS\" WITH NO WARRANTIES OF CORRECTNESS. USE AT YOUR OWN RISK.</p>"},{"location":"tutorials/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/#quick-start","title":"Quick Start","text":"<p>To get started, run the following lines in a terminal: </p> <pre><code>conda env create --name RAiDER  -c conda-forge raider jupyterlab\nconda activate RAiDER\n</code></pre> <p>Then download or clone this repository to your working directory, and run</p> <pre><code>jupyter lab\n</code></pre> <p>navigate to one of the tutorial notebooks, and open it. </p> <p>Other ways to install Defining Custom Weather Models </p>"},{"location":"tutorials/#tutorials_1","title":"Tutorials","text":"<p>Pandas tutorial for GNSS delay manipulation RAiDER tutorial RAiDER library access in Python Tutorial downloading GNSS tropospheric delays raiderStats tutorial </p>"}]}